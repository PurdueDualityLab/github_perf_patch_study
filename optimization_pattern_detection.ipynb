{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0486a43",
   "metadata": {},
   "source": [
    "## Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd880dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Environment ready!\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "!pip install pandas numpy matplotlib seaborn scipy wordcloud pyarrow datasets --quiet\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import re\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "# Plot settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Environment ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "588b1d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compatibility shim: some versions of fsspec don't expose url_to_fs at top-level.\n",
    "# This ensures code that expects fsspec.url_to_fs (used by some IO backends) continues to work.\n",
    "try:\n",
    "    import fsspec\n",
    "    if not hasattr(fsspec, \"url_to_fs\"):\n",
    "        try:\n",
    "            from fsspec.core import url_to_fs as _url_to_fs\n",
    "        except Exception:\n",
    "            try:\n",
    "                import fsspec.core as _core\n",
    "                _url_to_fs = _core.url_to_fs\n",
    "            except Exception:\n",
    "                # Fallback shim: create a minimal url_to_fs that returns a filesystem and the path.\n",
    "                def _url_to_fs(url, **kwargs):\n",
    "                    protocol = url.split(\"://\")[0] if \"://\" in url else \"file\"\n",
    "                    fs = fsspec.filesystem(protocol)\n",
    "                    return fs, url\n",
    "        fsspec.url_to_fs = _url_to_fs\n",
    "except Exception:\n",
    "    # If anything goes wrong, continue without failing here; subsequent IO calls will raise their own errors.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06bd5c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading AIDev datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/peng397/.pyenv/versions/3.10.0/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Performance PR IDs to process: 428\n",
      "\n",
      "Processing commit details (filtering to performance PRs only)...\n",
      "  Total commit records in dataset: 711,923\n",
      "  Filtered to performance PRs: 7,410 commit records\n",
      "  Unique performance PRs with commits: 340\n",
      "  ✓ Aggregated to 340 unique performance PRs\n",
      "  Avg commits per PR: 21.8\n",
      "  AI Agent PRs with commit data: 340 / 340 (100.0%)\n",
      "  Human PRs with commit data: 0 / 88 (0.0%)\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "✓ AI Agent Performance PRs: 340\n",
      "✓ Human Performance PRs: 88\n",
      "✓ Total Performance PRs: 428\n",
      "\n",
      "AI Agent Distribution:\n",
      "  OpenAI_Codex           207 ( 60.9%)\n",
      "  Devin                   62 ( 18.2%)\n",
      "  Copilot                 44 ( 12.9%)\n",
      "  Cursor                  24 (  7.1%)\n",
      "  Claude_Code              3 (  0.9%)\n",
      "\n",
      "================================================================================\n",
      "COMMIT STATISTICS\n",
      "================================================================================\n",
      "\n",
      "AI Agent:\n",
      "  PRs with commit data: 340 (100.0%)\n",
      "  Avg commits per PR: 21.8\n",
      "  Median commits per PR: 4.0\n",
      "  Avg additions: 646 lines\n",
      "  Median additions: 72 lines\n",
      "  Avg deletions: 646 lines\n",
      "  Median deletions: 26 lines\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "print(\"Loading AIDev datasets...\")\n",
    "\n",
    "# AI Agent PRs\n",
    "pr_df = pd.read_parquet(\"hf://datasets/hao-li/AIDev/pull_request.parquet\")\n",
    "pr_task_type_df = pd.read_parquet(\"hf://datasets/hao-li/AIDev/pr_task_type.parquet\")\n",
    "\n",
    "ai_perf_prs = (\n",
    "    pr_df\n",
    "    .merge(\n",
    "        pr_task_type_df[[\"id\", \"type\", \"reason\"]],\n",
    "        on=\"id\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    .query(\"type == 'perf'\")\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "ai_perf_prs['classification_reason'] = ai_perf_prs['reason']\n",
    "ai_perf_prs['author_type'] = 'AI Agent'\n",
    "\n",
    "\n",
    "# Human PRs\n",
    "human_pr_df = pd.read_parquet(\"hf://datasets/hao-li/AIDev/human_pull_request.parquet\")\n",
    "human_pr_task_type_df = pd.read_parquet(\"hf://datasets/hao-li/AIDev/human_pr_task_type.parquet\")\n",
    "human_perf_prs = (\n",
    "    human_pr_df\n",
    "    .merge(\n",
    "        human_pr_task_type_df[[\"id\", \"type\", \"reason\"]],\n",
    "        on=\"id\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    .query(\"type == 'perf'\")\n",
    "    .copy()\n",
    ")\n",
    "human_perf_prs['classification_reason'] = human_perf_prs['reason']\n",
    "human_perf_prs['author_type'] = 'Human'\n",
    "human_perf_prs['agent'] = 'Human'\n",
    "\n",
    "# Repository data for language info\n",
    "all_repo_df = pd.read_parquet(\"hf://datasets/hao-li/AIDev/all_repository.parquet\")\n",
    "\n",
    "# Get list of performance PR IDs we care about\n",
    "perf_pr_ids = set(ai_perf_prs['id'].tolist() + human_perf_prs['id'].tolist())\n",
    "print(f\"\\n✓ Performance PR IDs to process: {len(perf_pr_ids):,}\")\n",
    "\n",
    "# PR commits details - FILTER FIRST, then aggregate\n",
    "print(\"\\nProcessing commit details (filtering to performance PRs only)...\")\n",
    "pr_commits_details = pd.read_parquet(\"hf://datasets/hao-li/AIDev/pr_commit_details.parquet\")\n",
    "\n",
    "if 'pr_id' in pr_commits_details.columns:\n",
    "    print(f\"  Total commit records in dataset: {len(pr_commits_details):,}\")\n",
    "    \n",
    "    # FILTER: Keep only commits for performance PRs\n",
    "    pr_commits_filtered = pr_commits_details[pr_commits_details['pr_id'].isin(perf_pr_ids)].copy()\n",
    "    print(f\"  Filtered to performance PRs: {len(pr_commits_filtered):,} commit records\")\n",
    "    print(f\"  Unique performance PRs with commits: {pr_commits_filtered['pr_id'].nunique():,}\")\n",
    "    \n",
    "    if len(pr_commits_filtered) > 0:\n",
    "        # AGGREGATE: Now aggregate only the filtered commits\n",
    "        commit_aggregated = pr_commits_filtered.groupby('pr_id').agg({\n",
    "            'additions': 'sum',      # Total lines added across all commits\n",
    "            'deletions': 'sum',      # Total lines deleted across all commits\n",
    "            'patch': lambda x: '\\n\\n'.join([str(p) for p in x if pd.notna(p)])  # Concatenate all patches\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Add derived metrics\n",
    "        commit_aggregated['num_commits'] = pr_commits_filtered.groupby('pr_id').size().values\n",
    "        \n",
    "        # Calculate patch length (for analysis)\n",
    "        commit_aggregated['patch_length'] = commit_aggregated['patch'].str.len()\n",
    "        \n",
    "        print(f\"  ✓ Aggregated to {len(commit_aggregated):,} unique performance PRs\")\n",
    "        print(f\"  Avg commits per PR: {commit_aggregated['num_commits'].mean():.1f}\")\n",
    "        \n",
    "        # Merge commit stats into AI Agent PR table\n",
    "        ai_perf_prs = ai_perf_prs.merge(\n",
    "            commit_aggregated,\n",
    "            left_on='id',\n",
    "            right_on='pr_id',\n",
    "            how='left'\n",
    "        )\n",
    "        if 'pr_id' in ai_perf_prs.columns:\n",
    "            ai_perf_prs = ai_perf_prs.drop(columns=['pr_id'])\n",
    "        \n",
    "        ai_with_commits = ai_perf_prs['additions'].notna().sum()\n",
    "        print(f\"  AI Agent PRs with commit data: {ai_with_commits:,} / {len(ai_perf_prs):,} ({ai_with_commits/len(ai_perf_prs)*100:.1f}%)\")\n",
    "        \n",
    "        # Merge commit stats into Human PR table\n",
    "        human_perf_prs = human_perf_prs.merge(\n",
    "            commit_aggregated,\n",
    "            left_on='id',\n",
    "            right_on='pr_id',\n",
    "            how='left'\n",
    "        )\n",
    "        if 'pr_id' in human_perf_prs.columns:\n",
    "            human_perf_prs = human_perf_prs.drop(columns=['pr_id'])\n",
    "        \n",
    "        human_with_commits = human_perf_prs['additions'].notna().sum()\n",
    "        print(f\"  Human PRs with commit data: {human_with_commits:,} / {len(human_perf_prs):,} ({human_with_commits/len(human_perf_prs)*100:.1f}%)\")\n",
    "    else:\n",
    "        print(\"  ⚠ No commits found for performance PRs\")\n",
    "        # Add placeholder columns\n",
    "        for df in [ai_perf_prs, human_perf_prs]:\n",
    "            df['additions'] = None\n",
    "            df['deletions'] = None\n",
    "            df['num_commits'] = None\n",
    "            df['patch'] = None\n",
    "            df['patch_length'] = None\n",
    "    \n",
    "else:\n",
    "    print('⚠ pr_commit_details missing pr_id column; skipping commit merges.')\n",
    "    # Add placeholder columns\n",
    "    for df in [ai_perf_prs, human_perf_prs]:\n",
    "        df['additions'] = None\n",
    "        df['deletions'] = None\n",
    "        df['num_commits'] = None\n",
    "        df['patch'] = None\n",
    "        df['patch_length'] = None\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"✓ AI Agent Performance PRs: {len(ai_perf_prs):,}\")\n",
    "print(f\"✓ Human Performance PRs: {len(human_perf_prs):,}\")\n",
    "print(f\"✓ Total Performance PRs: {len(ai_perf_prs) + len(human_perf_prs):,}\")\n",
    "\n",
    "# Distribution by AI agent\n",
    "print(f\"\\nAI Agent Distribution:\")\n",
    "for agent, count in ai_perf_prs['agent'].value_counts().items():\n",
    "    pct = count / len(ai_perf_prs) * 100\n",
    "    print(f\"  {agent:20s} {count:5,d} ({pct:5.1f}%)\")\n",
    "\n",
    "# Commit statistics summary\n",
    "if 'num_commits' in ai_perf_prs.columns and ai_perf_prs['num_commits'].notna().any():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"COMMIT STATISTICS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    for author_type, df in [('AI Agent', ai_perf_prs), ('Human', human_perf_prs)]:\n",
    "        with_commits = df['num_commits'].notna()\n",
    "        if with_commits.sum() > 0:\n",
    "            print(f\"\\n{author_type}:\")\n",
    "            print(f\"  PRs with commit data: {with_commits.sum():,} ({with_commits.mean()*100:.1f}%)\")\n",
    "            print(f\"  Avg commits per PR: {df.loc[with_commits, 'num_commits'].mean():.1f}\")\n",
    "            print(f\"  Median commits per PR: {df.loc[with_commits, 'num_commits'].median():.1f}\")\n",
    "            print(f\"  Avg additions: {df.loc[with_commits, 'additions'].mean():.0f} lines\")\n",
    "            print(f\"  Median additions: {df.loc[with_commits, 'additions'].median():.0f} lines\")\n",
    "            print(f\"  Avg deletions: {df.loc[with_commits, 'deletions'].mean():.0f} lines\")\n",
    "            print(f\"  Median deletions: {df.loc[with_commits, 'deletions'].median():.0f} lines\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa5d5acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset: 428 performance PRs\n",
      "  AI Agents: 340\n",
      "  Humans: 88\n"
     ]
    }
   ],
   "source": [
    "# Combine AI and Human PRs\n",
    "perf_prs = pd.concat([ai_perf_prs, human_perf_prs], ignore_index=True)\n",
    "\n",
    "print(f\"Combined dataset: {len(perf_prs):,} performance PRs\")\n",
    "print(f\"  AI Agents: {(perf_prs['author_type'] == 'AI Agent').sum():,}\")\n",
    "print(f\"  Humans: {(perf_prs['author_type'] == 'Human').sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a247c4a",
   "metadata": {},
   "source": [
    "# Optimization Pattern Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b725c6f4",
   "metadata": {},
   "source": [
    "## LLM inference script\n",
    "Script to map optimization to the performance optimization pattern catalog using LLM, based on PR title, body, and patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56ed2759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Performance Optimization Pattern Detection with GPT\n",
    "# ============================================================================\n",
    "!pip install openai --quiet\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from pydantic import BaseModel \n",
    "from pathlib import Path\n",
    "\n",
    "def analyze_optimization_with_gpt(title, body, patch):\n",
    "    \"\"\"\n",
    "    Call GPT to analyze performance optimization patterns in a commit.\n",
    "    \n",
    "    Parameters:\n",
    "    - title: PR/commit title\n",
    "    - body: PR/commit description\n",
    "    - patch: Git diff/patch content\n",
    "    \n",
    "    Returns:\n",
    "    - dict with analysis results or error info\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prepare the context\n",
    "    context_parts = []\n",
    "    \n",
    "    if pd.notna(title) and str(title).strip():\n",
    "        context_parts.append(f\"**Title**: {title}\")\n",
    "    \n",
    "    if pd.notna(body) and str(body).strip():\n",
    "        context_parts.append(f\"**Description**: {body}\")\n",
    "    \n",
    "    if pd.notna(patch) and str(patch).strip():\n",
    "        # Truncate very long patches to avoid token limits\n",
    "        patch_str = str(patch)\n",
    "        if len(patch_str) > 15000:  # Rough character limit\n",
    "            patch_str = patch_str[:15000] + \"\\n\\n... [patch truncated for length] ...\"\n",
    "        context_parts.append(f\"**Code Changes (Patch)**:\\n```diff\\n{patch_str}\\n```\")\n",
    "    \n",
    "    if not context_parts:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": \"No content available\",\n",
    "            \"explanation\": None,\n",
    "            \"optimization_comparison\": None,\n",
    "            \"high_level_pattern\": None,\n",
    "            \"sub_pattern\": None,\n",
    "            \"tokens_used\": 0\n",
    "        }\n",
    "    \n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    client=OpenAI(api_key=\"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\")\n",
    "    \n",
    "    # Construct the prompt\n",
    "    prompt = f\"\"\"I have a performance optimization commit with the following information. Please analyze with the following goals:\n",
    "\n",
    "1. **Code Function Explanation**: Briefly explain what the code is doing—what problem it solves and how it works.\n",
    "\n",
    "2. **Optimization Comparison**: Compare the original and optimized versions to identify:\n",
    "   - **Algorithmic changes**: Any differences in logic, algorithm design, or problem-solving approach.\n",
    "   - **Performance improvements**: Enhancements related to time complexity, space efficiency, or runtime behavior.\n",
    "   - **Redundant code removal**: Elimination of unnecessary logic, method calls, or control structures.\n",
    "   - **Other noteworthy changes**: Any structural or stylistic differences that could impact performance or readability.\n",
    "   \n",
    "3. **Optimization Pattern Classification**:\n",
    "   Based on the overall nature of the optimized code, assign the following. Return \"No Meaningful Change\" if no meaningful change is made.\n",
    "   - **Exactly one high-level optimization pattern** from the list below  \n",
    "   - **One most representative sub-pattern** within that high-level category\n",
    "   \n",
    "   ### High-Level Optimization Patterns Taxonomy:\n",
    "   - **Algorithm-Level Optimizations**\n",
    "        - Select Computationally Efficient Algorithms\n",
    "        - Select Algorithm Based on Instruction Speed\n",
    "        - Structure Algorithm to Support instruction level parallelism (ILP)\n",
    "        - Select Space Efficient Algorithm\n",
    "        - Inheritance over Delegation for Energy Efficiency\n",
    "   - **Control-Flow and Branching Optimizations**\n",
    "        - Make Conditional Branches More Predictable\n",
    "        - Remove Branches with min/max Instructions\n",
    "        - Remove Branches by Doing Extra Work\n",
    "        - Remove Branching with Masking\n",
    "        - Rearranging Branches\n",
    "        - Combining Branches\n",
    "   - **Memory and Data Locality Optimizations**\n",
    "        - Access Data with Appropriate Type (Prevent Store Forwarding Issues)\n",
    "        - Increase Cache Efficiency via Locality\n",
    "        - Arrange Data for Optimal Hardware Prefetching\n",
    "        - Avoid cache capacity issues by segmenting work\n",
    "        - Increase Workload to Mitigate Memory Access Latency\n",
    "        - Use Smaller Data Types\n",
    "        - Caching\n",
    "        - Buffering\n",
    "        - Improve cache locality via data structure\n",
    "        - Optimize Object Use\n",
    "        - Reduce memory bloat from RTSJ Immortal Memory\n",
    "   - **Loop Transformations**\n",
    "        - Remove Conditional by Loop Unrolling\n",
    "        - Loop Distribution (Fission)\n",
    "        - Loop Fusion\n",
    "        - Loop Peeling\n",
    "        - Loop Interchanging\n",
    "        - Loop Invariant Branches\n",
    "        - Loop Strip-mining\n",
    "   - **I/O and Synchronization**\n",
    "        - Selection of I/O Size\n",
    "        - Polling\n",
    "        - Non-Blocking I/O\n",
    "   - **Data Structure Selection and Adaptation**\n",
    "        - Choose Structure for Energy Efficiency\n",
    "        - Darwinian Data Structure Selection\n",
    "        - Choose more energy-efficient data structure across Java Collections Framework, Apache Common Collections, and Eclipse Collections\n",
    "        - Choose energy-efficient data structure by method calls\n",
    "   - **Code Smells and Structural Simplification**\n",
    "        - Remove code bloat by removing optional features\n",
    "        - Remove Unnecessary Method Calls\n",
    "        - Remove long method by extracting new method\n",
    "        - Remove Duplicate code\n",
    "        - Minimize feature envy by moving methods\n",
    "        - Minimize occurrences of God Class\n",
    "        - Type Checking\n",
    "         \n",
    "Here are the info:\n",
    "            \n",
    "{context}\n",
    "\n",
    "**Output Structure**:  \n",
    "Please respond in JSON format with the following structure:\n",
    "{{\n",
    "  \"explanation\": \"Brief description of what the code is doing\",\n",
    "  \"optimization_comparison\": \"Detailed comparison highlighting specific optimizations\",\n",
    "  \"high_level_pattern\": \"Single most representative high-level optimization pattern (or 'No Meaningful Change')\",\n",
    "  \"sub_pattern\": \"Most representative sub-pattern within the category (or null if No Meaningful Change)\",\n",
    "}}\n",
    "\n",
    "Ensure your response is valid JSON that can be parsed.\n",
    "\"\"\"\n",
    "\n",
    "    class AnalysisResult(BaseModel):  \n",
    "        explanation: str\n",
    "        optimization_comparison: str\n",
    "        high_level_pattern: str\n",
    "        sub_pattern: str\n",
    "    \n",
    "    try:        \n",
    "        response = client.beta.chat.completions.parse(\n",
    "                    model = \"gpt-5.1\",\n",
    "                    messages = [\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": \"You are an expert software engineer specializing in performance optimization analysis. Analyze code changes and classify optimization patterns accurately.\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": prompt\n",
    "                        }\n",
    "                    ],\n",
    "                    response_format=AnalysisResult,\n",
    "                    temperature=0,\n",
    "                )\n",
    "\n",
    "        # Parse the response\n",
    "        content = response.choices[0].message.content\n",
    "        result = json.loads(content)\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"explanation\": result.get(\"explanation\", \"\"),\n",
    "            \"optimization_comparison\": result.get(\"optimization_comparison\", \"\"),\n",
    "            \"high_level_pattern\": result.get(\"high_level_pattern\", \"\"),\n",
    "            \"sub_pattern\": result.get(\"sub_pattern\", \"\"),\n",
    "            \"tokens_used\": response.usage.total_tokens,\n",
    "            \"error\": None\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e),\n",
    "            \"explanation\": None,\n",
    "            \"optimization_comparison\": None,\n",
    "            \"high_level_pattern\": None,\n",
    "            \"sub_pattern\": None,\n",
    "            \"tokens_used\": 0\n",
    "        }\n",
    "\n",
    "\n",
    "def batch_analyze_performance_prs(perf_prs, batch_size=10, delay=1.0,resume=False, checkpoint_prefix='perf_prs_checkpoint', output_file='perf_prs_with_gpt_analysis.csv'):\n",
    "    \"\"\"\n",
    "    Analyze all performance PRs in batches.\n",
    "\n",
    "    Parameters:\n",
    "    - perf_prs: DataFrame with performance PRs\n",
    "    - batch_size: Number of PRs to process before saving checkpoint\n",
    "    - delay: Delay between API calls in seconds\n",
    "    - resume: Continue from the last available checkpoint if True\n",
    "    - checkpoint_prefix: Filename prefix used for checkpoint files\n",
    "    - output_file: Final CSV filename for the aggregated results\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with analysis results added\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Starting GPT analysis of {len(perf_prs):,} performance PRs...\")\n",
    "\n",
    "    checkpoint_files = []\n",
    "    processed_count = 0\n",
    "\n",
    "    if resume:\n",
    "        checkpoint_files = sorted(Path('.').glob(f\"{checkpoint_prefix}_*.csv\"))\n",
    "        if checkpoint_files:\n",
    "            def _processed_from_path(path_obj):\n",
    "                suffix = path_obj.stem.rsplit('_', 1)[-1]\n",
    "                return int(suffix) if suffix.isdigit() else 0\n",
    "\n",
    "            latest_checkpoint = max(checkpoint_files, key=_processed_from_path)\n",
    "            checkpoint_progress = _processed_from_path(latest_checkpoint)\n",
    "            perf_prs = pd.read_csv(latest_checkpoint)\n",
    "            processed_count = min(checkpoint_progress, len(perf_prs))\n",
    "            print(f\"↻ Resuming from checkpoint {latest_checkpoint} ({processed_count} PRs processed)...\")\n",
    "        else:\n",
    "            print(\"↻ Resume requested but no checkpoint found. Starting from scratch.\")\n",
    "\n",
    "    result_defaults = {\n",
    "        'gpt_explanation': None,\n",
    "        'gpt_comparison': None,\n",
    "        'optimization_pattern': None,\n",
    "        'optimization_subpattern': None,\n",
    "        'gpt_success': False,\n",
    "        'gpt_error': None,\n",
    "        'gpt_tokens': 0\n",
    "    }\n",
    "\n",
    "    for column, default in result_defaults.items():\n",
    "        if resume and column in perf_prs.columns:\n",
    "            continue\n",
    "        perf_prs[column] = default\n",
    "\n",
    "    start_idx = processed_count if resume else 0\n",
    "    iterator = range(start_idx, len(perf_prs))\n",
    "    progress_bar = tqdm(iterator, total=len(perf_prs), desc=\"Analyzing PRs\", initial=start_idx)\n",
    "\n",
    "    for idx in progress_bar:\n",
    "        row = perf_prs.iloc[idx]\n",
    "        result = analyze_optimization_with_gpt(\n",
    "            title=row.get('title'),\n",
    "            body=row.get('body'),\n",
    "            patch=row.get('patch')\n",
    "        )\n",
    "\n",
    "        perf_prs.at[idx, 'gpt_success'] = result['success']\n",
    "        perf_prs.at[idx, 'gpt_tokens'] = result['tokens_used']\n",
    "\n",
    "        if result['success']:\n",
    "            perf_prs.at[idx, 'gpt_explanation'] = result['explanation']\n",
    "            perf_prs.at[idx, 'gpt_comparison'] = result['optimization_comparison']\n",
    "            perf_prs.at[idx, 'optimization_pattern'] = result['high_level_pattern']\n",
    "            perf_prs.at[idx, 'optimization_subpattern'] = result['sub_pattern']\n",
    "            perf_prs.at[idx, 'gpt_error'] = None\n",
    "        else:\n",
    "            perf_prs.at[idx, 'gpt_error'] = result['error']\n",
    "\n",
    "        time.sleep(delay)\n",
    "\n",
    "        if (idx + 1) % batch_size == 0:\n",
    "            checkpoint_file = f\"{checkpoint_prefix}_{idx+1}.csv\"\n",
    "            perf_prs.to_csv(checkpoint_file, index=False)\n",
    "            print(f\"✓ Checkpoint saved: {checkpoint_file}\")\n",
    "\n",
    "    perf_prs.to_csv(output_file, index=False)\n",
    "    print(f\"✓ Analysis complete! Saved to: {output_file}\")\n",
    "\n",
    "    success_series = perf_prs['gpt_success'].fillna(False)\n",
    "    success_count = success_series.sum()\n",
    "    success_rate = (success_count / len(perf_prs) * 100) if len(perf_prs) else 0\n",
    "    failure_count = success_series.eq(False).sum()\n",
    "    total_tokens = perf_prs['gpt_tokens'].sum()\n",
    "\n",
    "    print(f\"{'='*80}\")\n",
    "    print(\"ANALYSIS SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total PRs analyzed: {len(perf_prs):,}\")\n",
    "    print(f\"Successful: {success_count:,} ({success_rate:.1f}%)\")\n",
    "    print(f\"Failed: {failure_count:,}\")\n",
    "    print(f\"Total tokens used: {total_tokens:,}\")\n",
    "\n",
    "    if success_count > 0:\n",
    "        print(f\"{'='*80}\")\n",
    "        print(\"OPTIMIZATION PATTERN DISTRIBUTION\")\n",
    "        print(f\"{'='*80}\")\n",
    "        pattern_counts = perf_prs[perf_prs['gpt_success'] == True]['optimization_pattern'].value_counts()\n",
    "        for pattern, count in pattern_counts.items():\n",
    "            pct = count / success_count * 100\n",
    "            print(f\"  {pattern:50s} {count:4d} ({pct:5.1f}%)\")\n",
    "\n",
    "    return perf_prs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532b365f",
   "metadata": {},
   "source": [
    "## Usage scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107368e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lj/l0_qr8g54f3b60v0fzt7bf200000gs/T/ipykernel_6249/4063451614.py:239: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  perf_prs[column] = default\n",
      "/var/folders/lj/l0_qr8g54f3b60v0fzt7bf200000gs/T/ipykernel_6249/4063451614.py:239: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  perf_prs[column] = default\n",
      "/var/folders/lj/l0_qr8g54f3b60v0fzt7bf200000gs/T/ipykernel_6249/4063451614.py:239: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  perf_prs[column] = default\n",
      "/var/folders/lj/l0_qr8g54f3b60v0fzt7bf200000gs/T/ipykernel_6249/4063451614.py:239: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  perf_prs[column] = default\n",
      "/var/folders/lj/l0_qr8g54f3b60v0fzt7bf200000gs/T/ipykernel_6249/4063451614.py:239: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  perf_prs[column] = default\n",
      "/var/folders/lj/l0_qr8g54f3b60v0fzt7bf200000gs/T/ipykernel_6249/4063451614.py:239: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  perf_prs[column] = default\n",
      "/var/folders/lj/l0_qr8g54f3b60v0fzt7bf200000gs/T/ipykernel_6249/4063451614.py:239: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  perf_prs[column] = default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing GPT analysis on 340 AI PRs\n",
      "Starting GPT analysis of 340 performance PRs...\n",
      "↻ Resume requested but no checkpoint found. Starting from scratch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing PRs:   6%|▌         | 20/340 [09:48<2:00:37, 22.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint saved: new_pattern_analysis/ai_perf_prs_checkpoint_20.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing PRs:  12%|█▏        | 40/340 [13:32<56:35, 11.32s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint saved: new_pattern_analysis/ai_perf_prs_checkpoint_40.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing PRs:  18%|█▊        | 60/340 [17:54<1:10:20, 15.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint saved: new_pattern_analysis/ai_perf_prs_checkpoint_60.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing PRs:  24%|██▎       | 80/340 [24:16<1:38:49, 22.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint saved: new_pattern_analysis/ai_perf_prs_checkpoint_80.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing PRs:  29%|██▉       | 100/340 [30:16<1:04:03, 16.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint saved: new_pattern_analysis/ai_perf_prs_checkpoint_100.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing PRs:  35%|███▌      | 120/340 [35:24<56:29, 15.41s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint saved: new_pattern_analysis/ai_perf_prs_checkpoint_120.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing PRs:  41%|████      | 140/340 [41:31<1:04:52, 19.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint saved: new_pattern_analysis/ai_perf_prs_checkpoint_140.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing PRs:  47%|████▋     | 160/340 [47:26<1:01:24, 20.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint saved: new_pattern_analysis/ai_perf_prs_checkpoint_160.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing PRs:  53%|█████▎    | 180/340 [53:27<1:00:01, 22.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint saved: new_pattern_analysis/ai_perf_prs_checkpoint_180.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing PRs:  59%|█████▉    | 200/340 [59:12<37:20, 16.01s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint saved: new_pattern_analysis/ai_perf_prs_checkpoint_200.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing PRs:  65%|██████▍   | 220/340 [1:04:35<33:06, 16.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint saved: new_pattern_analysis/ai_perf_prs_checkpoint_220.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing PRs:  71%|███████   | 240/340 [1:09:49<25:27, 15.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint saved: new_pattern_analysis/ai_perf_prs_checkpoint_240.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing PRs:  76%|███████▋  | 260/340 [1:15:08<16:51, 12.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint saved: new_pattern_analysis/ai_perf_prs_checkpoint_260.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing PRs:  82%|████████▏ | 280/340 [1:21:02<16:38, 16.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint saved: new_pattern_analysis/ai_perf_prs_checkpoint_280.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing PRs:  84%|████████▍ | 285/340 [1:22:31<18:17, 19.96s/it]"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Usage\n",
    "# ============================================================================\n",
    "\n",
    "# run ai and human pr analysis separately\n",
    "\n",
    "# ai pr analysis\n",
    "ai_sample = perf_prs[perf_prs['author_type'] == 'AI Agent']\n",
    "print(f\"Testing GPT analysis on {len(ai_sample)} AI PRs\")\n",
    "\n",
    "# Run the analysis\n",
    "perf_prs_analyzed = batch_analyze_performance_prs(\n",
    "    ai_sample,\n",
    "    batch_size=20,    # Save checkpoint every 10 PRs\n",
    "    delay=0.5,        # 0.5 second delay between API calls\n",
    "    resume=True,      # Continue from the last saved checkpoint if available\n",
    "    checkpoint_prefix='new_pattern_analysis/ai_perf_prs_checkpoint',\n",
    "    output_file='new_pattern_analysis/ai_perf_prs_with_gpt_analysis.csv'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ba3fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# human pr analysis\n",
    "human_sample = perf_prs[perf_prs['author_type'] == 'Human']\n",
    "print(f\"Testing GPT analysis on {len(human_sample)} Human PRs\")\n",
    "\n",
    "# Run the analysis\n",
    "perf_prs_analyzed = batch_analyze_performance_prs(\n",
    "    human_sample,\n",
    "    batch_size=10,    # Save checkpoint every 10 PRs\n",
    "    delay=0.5,        # 0.5 second delay between API calls\n",
    "    resume=True,      # Continue from the last saved checkpoint if available\n",
    "    checkpoint_prefix='human_perf_prs_checkpoint',\n",
    "    output_file='human_perf_prs_with_gpt_analysis.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291d666d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze results by author type\n",
    "print(\"=\"*80)\n",
    "print(\"PATTERN COMPARISON: AI AGENTS VS HUMANS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for author_type in ['AI Agent', 'Human']:\n",
    "    subset = perf_prs_analyzed[\n",
    "        (perf_prs_analyzed['author_type'] == author_type) & \n",
    "        (perf_prs_analyzed['gpt_success'] == True)\n",
    "    ]\n",
    "    \n",
    "    if len(subset) > 0:\n",
    "        print(f\"{author_type} (n={len(subset):,}):\")\n",
    "        pattern_dist = subset['optimization_pattern'].value_counts().head(5)\n",
    "        for pattern, count in pattern_dist.items():\n",
    "            pct = count / len(subset) * 100\n",
    "            print(f\"  {pattern:50s} {count:4d} ({pct:5.1f}%)\")\n",
    "\n",
    "# Compare sub-patterns\n",
    "print(\"=\"*80)\n",
    "print(\"TOP SUB-PATTERNS BY AUTHOR TYPE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for author_type in ['AI Agent', 'Human']:\n",
    "    subset = perf_prs_analyzed[\n",
    "        (perf_prs_analyzed['author_type'] == author_type) & \n",
    "        (perf_prs_analyzed['gpt_success'] == True)\n",
    "    ]\n",
    "    \n",
    "    if len(subset) > 0:\n",
    "        print(f\"{author_type}:\")\n",
    "        subpattern_dist = subset['optimization_subpattern'].value_counts().head(5)\n",
    "        for subpattern, count in subpattern_dist.items():\n",
    "            pct = count / len(subset) * 100\n",
    "            print(f\"  {subpattern:50s} {count:4d} ({pct:5.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
