{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0e094d3",
   "metadata": {},
   "source": [
    "\n",
    "# Combine OpenAI and Gemini validation labels\n",
    "\n",
    "Align OpenAI and Gemini RQ3 validation outputs, flag disagreements, and fill those rows with manual labels from the human/agent spreadsheets. The notebook saves a single resolved parquet file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9290bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BASE_DIR: /Users/antoniozhong/Documents/dev/purdue/MSR2026/github_perf_patch_study/RQ4_test_and_validation\n",
      "OpenAI file: /Users/antoniozhong/Documents/dev/purdue/MSR2026/github_perf_patch_study/RQ4_test_and_validation/llm_data/rq4_validation_evidence_openai.parquet\n",
      "Gemini file: /Users/antoniozhong/Documents/dev/purdue/MSR2026/github_perf_patch_study/RQ4_test_and_validation/llm_data/rq4_validation_evidence_gemini.parquet\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "def normalize_validation_type(series):\n",
    "    return (\n",
    "        series.fillna('none')\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "        .replace({'static analysis': 'static-analysis'})\n",
    "    )\n",
    "\n",
    "# Use absolute paths to avoid notebook CWD issues\n",
    "BASE_DIR = Path('/Users/antoniozhong/Documents/dev/purdue/MSR2026/github_perf_patch_study/RQ4_test_and_validation')\n",
    "LLM_DIR = BASE_DIR / 'llm_data'\n",
    "MANUAL_DIR = BASE_DIR / 'manual_label'\n",
    "OUTPUT_DIR = BASE_DIR / 'final_data'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "openai_path = LLM_DIR / 'rq4_validation_evidence_openai.parquet'\n",
    "gemini_path = LLM_DIR / 'rq4_validation_evidence_gemini.parquet'\n",
    "manual_human_path = MANUAL_DIR / 'Final - Human PRs.csv'\n",
    "manual_agent_path = MANUAL_DIR / 'Final-Agent PRs.csv'\n",
    "print('Using BASE_DIR:', BASE_DIR)\n",
    "print('OpenAI file:', openai_path)\n",
    "print('Gemini file:', gemini_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5834b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI rows: 407\n",
      "Gemini rows: 407\n",
      "Columns: ['pr_id', 'author_type', 'repo', 'pr_number', 'pr_title', 'pipeline_names', 'validation_present', 'evidence_sources', 'validation_type', 'validation_description', 'pipeline_signal', 'description_signal', 'comment_signal']\n"
     ]
    }
   ],
   "source": [
    "openai_df = pd.read_parquet(openai_path)\n",
    "gemini_df = pd.read_parquet(gemini_path)\n",
    "\n",
    "print(f'OpenAI rows: {len(openai_df)}')\n",
    "print(f'Gemini rows: {len(gemini_df)}')\n",
    "print(f'Columns: {list(openai_df.columns)}')\n",
    "\n",
    "\n",
    "openai_df['validation_type'] = normalize_validation_type(openai_df['validation_type'])\n",
    "gemini_df['validation_type'] = normalize_validation_type(gemini_df['validation_type'])\n",
    "assert openai_df['pr_id'].is_unique and gemini_df['pr_id'].is_unique, 'pr_id must be unique'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d70070e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows compared: 407\n",
      "Agreements: 344 | Mismatches: 63\n",
      "Mismatches by author_type (OpenAI side):\n",
      "author_type_openai\n",
      "ai_agent    53\n",
      "human       10\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "comparison_columns = {\n",
    "    'validation_present': lambda s: s.fillna(False).astype(bool),\n",
    "    'validation_type': normalize_validation_type,\n",
    "}\n",
    "\n",
    "merged = openai_df.merge(\n",
    "    gemini_df,\n",
    "    on=['pr_id', 'repo', 'pr_number'],\n",
    "    suffixes=('_openai', '_gemini'),\n",
    "    how='inner',\n",
    ")\n",
    "assert len(merged) == len(openai_df) == len(gemini_df)\n",
    "\n",
    "mismatch_mask = pd.Series(False, index=merged.index)\n",
    "for col, normalizer in comparison_columns.items():\n",
    "    left = normalizer(merged[f\"{col}_openai\"])\n",
    "    right = normalizer(merged[f\"{col}_gemini\"])\n",
    "    merged[f\"{col}_mismatch\"] = left != right\n",
    "    mismatch_mask = mismatch_mask | merged[f\"{col}_mismatch\"]\n",
    "\n",
    "merged['has_mismatch'] = mismatch_mask\n",
    "agreement_ids = set(merged.loc[~mismatch_mask, 'pr_id'])\n",
    "mismatch_ids = set(merged.loc[mismatch_mask, 'pr_id'])\n",
    "\n",
    "print(f'Rows compared: {len(merged)}')\n",
    "print(f'Agreements: {len(agreement_ids)} | Mismatches: {len(mismatch_ids)}')\n",
    "print('Mismatches by author_type (OpenAI side):')\n",
    "print(merged.loc[mismatch_mask, 'author_type_openai'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d5b0215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual labels loaded: 63 rows\n"
     ]
    }
   ],
   "source": [
    "\n",
    "manual_human_df = pd.read_csv(manual_human_path)\n",
    "manual_agent_df = pd.read_csv(manual_agent_path)\n",
    "manual_df = pd.concat([manual_human_df, manual_agent_df], ignore_index=True)\n",
    "\n",
    "manual_df = manual_df.rename(columns={'ID': 'pr_id', 'URL': 'html_url'})\n",
    "manual_df['pr_id'] = manual_df['pr_id'].astype(int)\n",
    "manual_df['validation_type'] = normalize_validation_type(manual_df['validation_type'])\n",
    "manual_df['evidence_sources'] = manual_df['evidence_sources'].fillna('')\n",
    "manual_df['validation_present'] = manual_df['validation_present'].astype(bool)\n",
    "\n",
    "manual_lookup = manual_df.set_index('pr_id')\n",
    "assert manual_lookup.index.is_unique\n",
    "missing_manual = mismatch_ids - set(manual_lookup.index)\n",
    "if missing_manual:\n",
    "    raise ValueError(f'Missing manual labels for mismatch ids: {sorted(missing_manual)}')\n",
    "\n",
    "print(f'Manual labels loaded: {len(manual_lookup)} rows')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80a580ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_source\n",
      "openai_gemini_agree    344\n",
      "manual_override         63\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "final_df = openai_df.copy()\n",
    "final_df['label_source'] = 'openai_gemini_agree'\n",
    "\n",
    "for pr_id in mismatch_ids:\n",
    "    manual_row = manual_lookup.loc[pr_id]\n",
    "    mask = final_df['pr_id'] == pr_id\n",
    "    final_df.loc[mask, 'validation_present'] = bool(manual_row['validation_present'])\n",
    "    final_df.loc[mask, 'validation_type'] = manual_row['validation_type']\n",
    "    final_df.loc[mask, 'evidence_sources'] = manual_row['evidence_sources']\n",
    "    final_df.loc[mask, 'label_source'] = 'manual_override'\n",
    "\n",
    "final_df = final_df.merge(manual_df[['pr_id', 'html_url']], on='pr_id', how='left')\n",
    "final_df = final_df.drop(columns=['html_url'], errors='ignore')\n",
    "\n",
    "def _normalize_evidence_sources(val):\n",
    "    if val is None:\n",
    "        return ''\n",
    "    if isinstance(val, float) and pd.isna(val):\n",
    "        return ''\n",
    "    if isinstance(val, str):\n",
    "        return val\n",
    "    if isinstance(val, (list, tuple, set)):\n",
    "        return '; '.join(map(str, val))\n",
    "    if isinstance(val, dict):\n",
    "        try:\n",
    "            return json.dumps(val, ensure_ascii=True)\n",
    "        except Exception:\n",
    "            return str(val)\n",
    "    return str(val)\n",
    "\n",
    "final_df['evidence_sources'] = final_df['evidence_sources'].apply(_normalize_evidence_sources)\n",
    "\n",
    "print(final_df['label_source'].value_counts())\n",
    "\n",
    "final_df = final_df.drop(columns=['html_url', 'label_source'], errors='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "113d1ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved resolved data to: /Users/antoniozhong/Documents/dev/purdue/MSR2026/github_perf_patch_study/RQ4_test_and_validation/final_data/rq4_validation_evidence_final.parquet\n"
     ]
    }
   ],
   "source": [
    "\n",
    "resolved_parquet = OUTPUT_DIR / 'rq4_validation_evidence_final.parquet'\n",
    "final_df_to_save = final_df.drop(columns=['html_url', 'label_source'], errors='ignore')\n",
    "final_df_to_save.to_parquet(resolved_parquet, index=False)\n",
    "print(f'Saved resolved data to: {resolved_parquet}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48ea4e31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "407"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d8c8d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['pr_id', 'author_type', 'repo', 'pr_number', 'pr_title',\n",
       "       'pipeline_names', 'validation_present', 'evidence_sources',\n",
       "       'validation_type', 'validation_description', 'pipeline_signal',\n",
       "       'description_signal', 'comment_signal'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
