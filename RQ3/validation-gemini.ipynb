{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf4ad7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1063.51s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.11/site-packages (2.3.3)\n",
      "Requirement already satisfied: python-dotenv in /opt/anaconda3/lib/python3.11/site-packages (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "1069.25s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "Requirement already satisfied: google-genai==1.53.0 in /opt/anaconda3/lib/python3.11/site-packages (1.53.0)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /opt/anaconda3/lib/python3.11/site-packages (from google-genai==1.53.0) (4.11.0)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /opt/anaconda3/lib/python3.11/site-packages (from google-auth[requests]<3.0.0,>=2.14.1->google-genai==1.53.0) (2.30.0)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /opt/anaconda3/lib/python3.11/site-packages (from google-genai==1.53.0) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in /opt/anaconda3/lib/python3.11/site-packages (from google-genai==1.53.0) (2.12.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /opt/anaconda3/lib/python3.11/site-packages (from google-genai==1.53.0) (2.32.5)\n",
      "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /opt/anaconda3/lib/python3.11/site-packages (from google-genai==1.53.0) (9.1.2)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from google-genai==1.53.0) (15.0.1)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /opt/anaconda3/lib/python3.11/site-packages (from google-genai==1.53.0) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.11/site-packages (from anyio<5.0.0,>=4.8.0->google-genai==1.53.0) (3.11)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/lib/python3.11/site-packages (from anyio<5.0.0,>=4.8.0->google-genai==1.53.0) (1.3.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai==1.53.0) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda3/lib/python3.11/site-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai==1.53.0) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/anaconda3/lib/python3.11/site-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai==1.53.0) (4.9)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.11/site-packages (from httpx<1.0.0,>=0.28.1->google-genai==1.53.0) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.11/site-packages (from httpx<1.0.0,>=0.28.1->google-genai==1.53.0) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/lib/python3.11/site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai==1.53.0) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.9.0->google-genai==1.53.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.9.0->google-genai==1.53.0) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.9.0->google-genai==1.53.0) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.28.1->google-genai==1.53.0) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.28.1->google-genai==1.53.0) (2.5.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /opt/anaconda3/lib/python3.11/site-packages (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai==1.53.0) (0.4.8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install pandas python-dotenv\n",
    "!python -m pip install google-genai==1.53.0\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List, Optional, Tuple\n",
    "import pandas as pd\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import time\n",
    "load_dotenv(find_dotenv()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1cd09b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"validation_present\": {\n",
    "            \"type\": \"boolean\"\n",
    "        },\n",
    "        \"evidence_sources\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": [\"pipeline\", \"description\", \"comments\"]\n",
    "            }\n",
    "        },\n",
    "        \"validation_type\": {\n",
    "            \"type\": \"string\",\n",
    "            \"enum\": [\n",
    "                \"benchmark\",\n",
    "                \"profiling\",\n",
    "                \"static-analysis\",\n",
    "                \"anecdotal\",\n",
    "            ]\n",
    "        },\n",
    "        \"validation_description\": {\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        \"pipeline_signal\": {\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        \"description_signal\": {\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        \"comment_signal\": {\n",
    "            \"type\": \"string\"\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\n",
    "        \"validation_present\",\n",
    "        \"evidence_sources\",\n",
    "        \"validation_type\",\n",
    "        \"validation_description\",\n",
    "        \"pipeline_signal\",\n",
    "        \"description_signal\",\n",
    "        \"comment_signal\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfa3c233",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0  \n",
    "\n",
    "def run_llm(prompt: str,user_prompt: str) -> str:\n",
    "    model = \"gemini-3-pro-preview\"\n",
    "    print(\"Running GEMINI, model:\", model)\n",
    "        \n",
    "    GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "    GEMINI_API_KEY_2 = os.getenv(\"GEMINI_API_KEY_2\")\n",
    "    client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "    client_2 = genai.Client(api_key=GEMINI_API_KEY_2)\n",
    "\n",
    "        \n",
    "    config = types.GenerateContentConfig(\n",
    "            temperature=0.0,\n",
    "            response_mime_type=\"application/json\",\n",
    "            response_schema=schema,\n",
    "            thinking_config=types.ThinkingConfig(\n",
    "                thinking_level=types.ThinkingLevel.HIGH\n",
    "            ),\n",
    "            system_instruction=prompt\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        response = None    \n",
    "        if counter >= 205:   \n",
    "            response = client_2.models.generate_content(\n",
    "                model=model,\n",
    "                contents=user_prompt,\n",
    "                config=config,\n",
    "        )\n",
    "        else:\n",
    "            response = client.models.generate_content(\n",
    "                model=model,\n",
    "                contents=user_prompt,\n",
    "                config=config,\n",
    "        )  \n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfa3b352",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_datasets_dir(start: Optional[Path] = None) -> Path:\n",
    "    start = start or Path.cwd()\n",
    "    for path in (start, *start.parents):\n",
    "        candidate = path / \"datasets\"\n",
    "        if candidate.exists():\n",
    "            return candidate\n",
    "    raise FileNotFoundError(f\"Could not find 'datasets' directory from {start}\")\n",
    "\n",
    "\n",
    "DATASETS_DIR = find_datasets_dir()\n",
    "PROJECT_ROOT = DATASETS_DIR.parent\n",
    "\n",
    "def extract_json(text: str) -> Dict:\n",
    "    \"\"\"Best-effort JSON extraction from model output.\"\"\"\n",
    "    start = text.find(\"{\")\n",
    "    end = text.rfind(\"}\")\n",
    "    if start != -1 and end != -1 and end > start:\n",
    "        snippet = text[start : end + 1]\n",
    "        try:\n",
    "            return json.loads(snippet)\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    return {}\n",
    "\n",
    "def truncate(text: str, limit: int = 10000) -> str:\n",
    "    return text if len(text) <= limit else text[:limit] + \"...[truncated]\"\n",
    "\n",
    "def load_pr_core(prefix: str) -> pd.DataFrame:\n",
    "    commits = pd.read_parquet(\n",
    "        DATASETS_DIR / f\"{prefix}_pr\" / f\"{prefix}_pr_commits.parquet\"\n",
    "    )\n",
    "    return commits.drop_duplicates(\"pr_id\").set_index(\"pr_id\")\n",
    "\n",
    "\n",
    "def collect_comments(prefix: str, pr_id: int) -> List[str]:\n",
    "    issue = pd.read_parquet(\n",
    "        DATASETS_DIR / f\"{prefix}_pr\" / f\"{prefix}_pr_issue_comments.parquet\"\n",
    "    )\n",
    "    review = pd.read_parquet(\n",
    "        DATASETS_DIR / f\"{prefix}_pr\" / f\"{prefix}_pr_review_comments.parquet\"\n",
    "    )\n",
    "    texts = []\n",
    "    for df in (issue, review):\n",
    "        subset = df[df[\"pr_id\"] == pr_id]\n",
    "        texts.extend(subset[\"body\"].dropna().tolist())\n",
    "    return texts\n",
    "\n",
    "\n",
    "def collect_pipeline_names(prefix: str, pr_id: int) -> List[str]:\n",
    "    workflows = pd.read_parquet(\n",
    "        DATASETS_DIR / f\"{prefix}_pr\" / f\"{prefix}_pr_workflow_runs.parquet\"\n",
    "    )\n",
    "    subset = workflows[workflows[\"pr_id\"] == pr_id]\n",
    "    return sorted(subset[\"workflow_name\"].dropna().unique().tolist())\n",
    "\n",
    "def pr_ids_from_commits(prefix: str, limit: Optional[int] = None) -> Iterable[int]:\n",
    "    commits = pd.read_parquet(\n",
    "        DATASETS_DIR / f\"{prefix}_pr\" / f\"{prefix}_pr_commits.parquet\"\n",
    "    )\n",
    "    pr_ids = sorted(commits[\"pr_id\"].dropna().astype(int).unique().tolist())\n",
    "    return pr_ids if limit is None else pr_ids[:limit]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144154fb",
   "metadata": {},
   "source": [
    "Generate by begining of file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c80e7dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_pr(\n",
    "    prefix: str,\n",
    "    pr_id: int,\n",
    "    author_type: str,\n",
    "    pr_core: pd.DataFrame,\n",
    ") -> Tuple[Dict, Dict]:\n",
    "    row = pr_core.loc[pr_id]\n",
    "    pipeline_names = collect_pipeline_names(prefix, pr_id)\n",
    "    comments = collect_comments(prefix, pr_id)\n",
    "    description = (row.get(\"pr_description\") or \"\").strip()\n",
    "    code_diff = (row.get(\"patch\") or \"\").strip()\n",
    "\n",
    "    DEVELOPER_PROMPT_TEMPLATE = \"\"\"\n",
    "    You are a classifier for performance validation evidence in GitHub Pull Requests (PRs).\n",
    "\n",
    "    Your job is to decide whether there is explicit performance validation, and if so,\n",
    "    to classify it into EXACTLY ONE validation_type category based on the definitions below.\n",
    "\n",
    "    First, carefully read and internalize these validation_type categories:\n",
    "\n",
    "    1. Benchmark-Based Validation (Unit Tests or Microbenchmarks)\n",
    "    Definition:\n",
    "    The PR validates performance by running benchmark tests—either existing unit tests or newly added benchmark/microbenchmark tests. It includes explicit before-and-after comparisons such as runtime, throughput, memory, CPU usage, or any quantitative metric collected from tests.\n",
    "\n",
    "    2. Profiling-Based Validation (Application- or Function-Level Profiling)\n",
    "    Definition:\n",
    "    The PR uses profiling tools to validate performance, typically capturing stack samples, CPU hotspots, flamegraphs, or function-level timings. Evidence includes profiling outputs before and after the change.\n",
    "\n",
    "    3. Static-Analysis-Based Validation (Reasoning Without Runtime Evidence)\n",
    "    Definition:\n",
    "    The PR argues for performance improvement through static reasoning about the code—algorithmic complexity, data-structure changes, loop bounds, allocation count reduction, etc., without providing runtime/profiling data.\n",
    "\n",
    "    4. Anecdotal or Informal Local Testing (No Evidence Provided)\n",
    "    Definition:\n",
    "    The PR claims that performance is improved based on local testing, intuition, or manual observation, but provides no quantitative metrics, no profiling output, and no static-analysis justification.\n",
    "\n",
    "    Decision Rules:\n",
    "\n",
    "    1) First decide if performance validation is explicitly present (validation_present).\n",
    "    - Set validation_present = TRUE only when the PR explicitly shows some form of validation evidence:\n",
    "        benchmarks/microbenchmarks, profiling traces, static performance reasoning,\n",
    "        or explicit local testing statements.\n",
    "    - Do NOT infer validation from performance intent alone.\n",
    "\n",
    "    2) If validation_present = FALSE:\n",
    "    - Set validation_type = \"none\".\n",
    "    - Set evidence_sources = [].\n",
    "    - Set validation_description to explain the absence of validation.\n",
    "    - Never assign any other validation_type.\n",
    "    \n",
    "    3) If validation_present = TRUE:\n",
    "    - Choose exactly ONE non-\"none\" validation_type from the following: Benchmark-Based Validation; Profiling-Based Validation; Static-Analysis-Based Validation; Anecdotal or Informal Local Testing\n",
    "\n",
    "    4) evidence_sources must list where the validation is explicitly mentioned:\n",
    "    - \"pipeline\", \"description\", \"comments\", \"code_diff\"\n",
    "\n",
    "    5) When validation_type is \"benchmark\", \"profiling\":\n",
    "    mention the metrics used (latency, throughput, memory, CPU, etc.).\n",
    "\n",
    "    You must ALWAYS return STRICT JSON with exactly these keys:\n",
    "\n",
    "    validation_present, evidence_sources, validation_type, metrics,\n",
    "    validation_description, pipeline_signal,\n",
    "    description_signal, comment_signal.\n",
    "\n",
    "    No extra commentary. No markdown.\n",
    "    No explanations.\n",
    "    \"\"\"\n",
    "\n",
    "    USER_PROMPT_TEMPLATE = \"\"\"\n",
    "    Classify the following PR strictly using the rules and definitions from the system.\n",
    "\n",
    "    PIPELINES:\n",
    "    {pipeline_names}\n",
    "\n",
    "    DESCRIPTION:\n",
    "    {description}\n",
    "\n",
    "    COMMENTS:\n",
    "    {comments}\n",
    "    \n",
    "    CODE DIFF:\n",
    "    {code_diff}\n",
    "    \"\"\"\n",
    "\n",
    "    empty_record = {\n",
    "        \"pr_id\": pr_id,\n",
    "        \"author_type\": author_type,\n",
    "        \"repo\": f\"{row.get('repo_owner')}/{row.get('repo_name')}\",\n",
    "        \"pr_number\": row.get(\"pr_number\"),\n",
    "        \"pr_title\": row.get(\"pr_title\"),\n",
    "        \"pipeline_names\": pipeline_names,\n",
    "        \"validation_present\": False,\n",
    "        \"evidence_sources\": [],\n",
    "        \"validation_type\": \"none\",\n",
    "        \"validation_description\": \"No validation evidence\",\n",
    "        \"pipeline_signal\": \"\",\n",
    "        \"description_signal\": \"\",\n",
    "        \"comment_signal\": \"\",\n",
    "    }\n",
    "\n",
    "    if not pipeline_names and not description and not comments and not code_diff:\n",
    "        print(f\"Short-circuiting PR {pr_id} with no signals\")\n",
    "        return empty_record\n",
    "\n",
    "    prompt = USER_PROMPT_TEMPLATE.format(\n",
    "        pipeline_names=\"- \" + \"- \".join(pipeline_names) if pipeline_names else \"None\",\n",
    "        description=truncate(description) if description else \"None\",\n",
    "        comments=\"- \" + \"- \".join(truncate(\" | \".join(comments)).split(\" | \")) if comments else \"None\",\n",
    "        code_diff=truncate(code_diff) if code_diff else \"None\",\n",
    "    )\n",
    "\n",
    "    developer_prompt = DEVELOPER_PROMPT_TEMPLATE\n",
    "\n",
    "    raw = \"\"\n",
    "    try:\n",
    "        raw = run_llm(\n",
    "            prompt=developer_prompt,\n",
    "            user_prompt=prompt,\n",
    "        )\n",
    "    except Exception as exc:\n",
    "        raw = \"\"\n",
    "        print(f\"gemini model call failed for PR {pr_id}: {exc}\")\n",
    "        raise exc\n",
    "        \n",
    "\n",
    "    parsed_llm = extract_json(raw) or {}\n",
    "\n",
    "    evidence_sources = parsed_llm.get(\"evidence_sources\") or []\n",
    "    if isinstance(evidence_sources, (tuple, list)):\n",
    "        evidence_sources = list(evidence_sources)\n",
    "    else:\n",
    "        evidence_sources = []\n",
    "\n",
    "    result = {\n",
    "        \"pr_id\": pr_id,\n",
    "        \"author_type\": author_type,\n",
    "        \"repo\": f\"{row.get('repo_owner')}/{row.get('repo_name')}\",\n",
    "        \"pr_number\": row.get(\"pr_number\"),\n",
    "        \"pr_title\": row.get(\"pr_title\"),\n",
    "        \"pipeline_names\": pipeline_names,\n",
    "        \"validation_present\": parsed_llm.get(\"validation_present\", False),\n",
    "        \"evidence_sources\": evidence_sources,\n",
    "        \"validation_type\": parsed_llm.get(\"validation_type\", \"none\"),\n",
    "        \"validation_description\": parsed_llm.get(\"validation_description\", \"No validation evidence\"),\n",
    "        \"pipeline_signal\": parsed_llm.get(\"pipeline_signal\", \"\"),\n",
    "        \"description_signal\": parsed_llm.get(\"description_signal\", \"\"),\n",
    "        \"comment_signal\": parsed_llm.get(\"comment_signal\", \"\"),\n",
    "    }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3640b8e9",
   "metadata": {},
   "source": [
    "Resume from start point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de14159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "out_dir = PROJECT_ROOT / \"RQ3\"\n",
    "out_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "out_path_gemini = out_dir / \"rq3_validation_evidence_gemini.parquet\"\n",
    "error_csv_path = out_dir / \"rq3_validation_errors_gemini.csv\"\n",
    "\n",
    "records_gemini = []\n",
    "processed = set()\n",
    "\n",
    "ai_core = load_pr_core(\"ai\")\n",
    "human_core = load_pr_core(\"human\")\n",
    "\n",
    "limit = None\n",
    "\n",
    "# ============================\n",
    "# Resume positions (1-based)\n",
    "# ============================\n",
    "start_ai_pos = 171    \n",
    "start_human_pos = 1    \n",
    "\n",
    "ai_start_idx = max(start_ai_pos - 1, 0)\n",
    "human_start_idx = max(start_human_pos - 1, 0)\n",
    "\n",
    "# ============================\n",
    "# Helpers\n",
    "# ============================\n",
    "def save_partial(records, out_path):\n",
    "    \"\"\"Save a partial parquet snapshot of current records.\"\"\"\n",
    "    if not records:\n",
    "        return\n",
    "    df_tmp = pd.DataFrame(records)\n",
    "    df_tmp.to_parquet(out_path, index=False)\n",
    "    print(f\"[partial save] Saved {len(df_tmp)} rows to {out_path}\")\n",
    "\n",
    "def log_error(pr_id, prefix, author_type, exc):\n",
    "    \"\"\"Append an error row to the CSV log.\"\"\"\n",
    "    row = {\n",
    "        \"prefix\": prefix,\n",
    "        \"pr_id\": pr_id,\n",
    "        \"author_type\": author_type,\n",
    "        \"error\": str(exc),\n",
    "    }\n",
    "    df_err = pd.DataFrame([row])\n",
    "    header = not error_csv_path.exists()\n",
    "    # Append mode: this does NOT delete existing CSV content\n",
    "    df_err.to_csv(error_csv_path, mode=\"a\", header=header, index=False)\n",
    "    print(f\"[error] Logged PR {pr_id} ({prefix}/{author_type}) to {error_csv_path}: {exc}\")\n",
    "\n",
    "def _merge_record(prefix, pr_id, author_type, res_dict):\n",
    "    \"\"\"\n",
    "    Merge metadata with the model result without overwriting metadata keys.\n",
    "    Ensures prefix/pr_id/author_type are present in the saved parquet.\n",
    "    \"\"\"\n",
    "    base = {\"prefix\": prefix, \"pr_id\": pr_id, \"author_type\": author_type}\n",
    "    if isinstance(res_dict, dict):\n",
    "        for k, v in res_dict.items():\n",
    "            if k not in base:\n",
    "                base[k] = v\n",
    "    return base\n",
    "\n",
    "def is_gemini_overloaded_error(exc: Exception) -> bool:\n",
    "    \"\"\"Detect transient Gemini overload errors that are retryable once.\"\"\"\n",
    "    msg = str(exc).lower()\n",
    "    return (\"503\" in msg) or (\"unavailable\" in msg) or (\"overloaded\" in msg)\n",
    "\n",
    "def analyze_pr_with_one_retry(prefix: str, pr_id: int, author_type: str, core):\n",
    "    \"\"\"\n",
    "    Try analyze_pr once.\n",
    "    If it fails due to 503/UNAVAILABLE/overloaded, wait 5s and retry exactly one more time.\n",
    "    If it fails again, re-raise so the caller logs it to CSV.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return analyze_pr(prefix, pr_id, author_type, core)\n",
    "    except Exception as exc:\n",
    "        if is_gemini_overloaded_error(exc):\n",
    "            print(f\"[retry] Gemini overloaded for PR {pr_id}. Waiting 5s before retry...\")\n",
    "            time.sleep(5)\n",
    "            print(f\"[retry] Retrying now for PR {pr_id}...\")\n",
    "            return analyze_pr(prefix, pr_id, author_type, core)\n",
    "        raise\n",
    "\n",
    "# ============================\n",
    "# Load existing partial (recommended)\n",
    "# ============================\n",
    "if out_path_gemini.exists():\n",
    "    try:\n",
    "        df_prev = pd.read_parquet(out_path_gemini)\n",
    "\n",
    "        # Keep existing records in memory so new saves include old + new\n",
    "        records_gemini = df_prev.to_dict(\"records\")\n",
    "\n",
    "        # Build processed as (prefix, pr_id) tuples if possible\n",
    "        if {\"prefix\", \"pr_id\"}.issubset(df_prev.columns):\n",
    "            processed = set(\n",
    "                zip(\n",
    "                    df_prev[\"prefix\"].astype(str),\n",
    "                    df_prev[\"pr_id\"].astype(int)\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            # If old parquet doesn't have prefix, we do NOT build a generic int-based\n",
    "            # processed set to avoid accidental skipping.\n",
    "            processed = set()\n",
    "\n",
    "        print(f\"[resume] Loaded {len(records_gemini)} existing records from {out_path_gemini}\")\n",
    "        print(f\"[resume] Processed keys loaded: {len(processed)}\")\n",
    "\n",
    "    except Exception as exc:\n",
    "        # Safer behavior: refuse to continue if we cannot read the parquet\n",
    "        raise RuntimeError(f\"Parquet read failed; refusing to overwrite. Reason: {exc}\")\n",
    "\n",
    "# ============================\n",
    "# Build ID lists\n",
    "# ============================\n",
    "ai_ids = list(pr_ids_from_commits(\"ai\", limit=limit))\n",
    "human_ids = list(pr_ids_from_commits(\"human\", limit=limit))\n",
    "\n",
    "print(f\"Processing {len(ai_ids)} AI PRs and {len(human_ids)} human PRs (first {limit} each).\")\n",
    "print(f\"[resume] AI starting at position {start_ai_pos} (slice index {ai_start_idx})\")\n",
    "print(f\"[resume] Human starting at position {start_human_pos} (slice index {human_start_idx})\")\n",
    "\n",
    "# Slice the lists to start from the requested positions\n",
    "ai_ids_to_process = ai_ids[ai_start_idx:]\n",
    "human_ids_to_process = human_ids[human_start_idx:]\n",
    "\n",
    "# ============================\n",
    "# Process AI PRs\n",
    "# ============================\n",
    "for idx, pr_id in enumerate(ai_ids_to_process, start=start_ai_pos):\n",
    "\n",
    "    # Skip if already processed\n",
    "    if (\"ai\", pr_id) in processed:\n",
    "        # Uncomment if you want explicit skip logs\n",
    "        # print(f\"[skip] AI PR already processed: {pr_id}\")\n",
    "        continue\n",
    "\n",
    "    # Print only when we are actually going to process it\n",
    "    print(f\"Processing AI PR {idx}/{len(ai_ids)}: {pr_id}\")\n",
    "\n",
    "    try:\n",
    "        gemini_res = analyze_pr_with_one_retry(\"ai\", pr_id, \"ai_agent\", ai_core)\n",
    "        record = _merge_record(\"ai\", pr_id, \"ai_agent\", gemini_res)\n",
    "        records_gemini.append(record)\n",
    "        processed.add((\"ai\", pr_id))\n",
    "    except Exception as exc:\n",
    "        log_error(pr_id, prefix=\"ai\", author_type=\"ai_agent\", exc=exc)\n",
    "\n",
    "    # Save every 10 total records currently in memory\n",
    "    if idx % 10 == 0:\n",
    "        save_partial(records_gemini, out_path_gemini)\n",
    "\n",
    "# ============================\n",
    "# Process Human PRs\n",
    "# ============================\n",
    "for idx, pr_id in enumerate(human_ids_to_process, start=start_human_pos):\n",
    "\n",
    "    # Skip if already processed\n",
    "    if (\"human\", pr_id) in processed:\n",
    "        # Uncomment if you want explicit skip logs\n",
    "        # print(f\"[skip] Human PR already processed: {pr_id}\")\n",
    "        continue\n",
    "\n",
    "    # Print only when we are actually going to process it\n",
    "    print(f\"Processing human PR {idx}/{len(human_ids)}: {pr_id}\")\n",
    "\n",
    "    try:\n",
    "        gemini_res = analyze_pr_with_one_retry(\"human\", pr_id, \"human\", human_core)\n",
    "        record = _merge_record(\"human\", pr_id, \"human\", gemini_res)\n",
    "        records_gemini.append(record)\n",
    "        processed.add((\"human\", pr_id))\n",
    "    except Exception as exc:\n",
    "        log_error(pr_id, prefix=\"human\", author_type=\"human\", exc=exc)\n",
    "\n",
    "    # Save every 10 total records currently in memory\n",
    "    if idx % 10 == 0:\n",
    "        save_partial(records_gemini, out_path_gemini)\n",
    "\n",
    "# ============================\n",
    "# Final save\n",
    "# ============================\n",
    "df_gemini = pd.DataFrame(records_gemini)\n",
    "df_gemini.to_parquet(out_path_gemini, index=False)\n",
    "print(f\"Saved FINAL GEMINI {len(df_gemini)} rows to {out_path_gemini}\")\n",
    "print(f\"Errored PRs (if any) logged to {error_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fcae45",
   "metadata": {},
   "source": [
    "Generate by id erros log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce1e5a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-processing 274 PRs (Gemini only).\n",
      "Processing [1/274] ai PR: 2843312341\n",
      "Running GEMINI, model: gemini-3-pro-preview\n",
      "gemini model call failed for PR 2843312341: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'Deadline expired before operation could complete.', 'status': 'UNAVAILABLE'}}\n",
      "[error] Logged PR 2843312341 (ai/ai_agent) to /Users/antoniozhong/Documents/dev/purdue/MSR2026/github_perf_patch_study/RQ3/rq3_validation_errors.csv: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'Deadline expired before operation could complete.', 'status': 'UNAVAILABLE'}}\n",
      "Processing [2/274] ai PR: 2843334531\n",
      "Running GEMINI, model: gemini-3-pro-preview\n",
      "gemini model call failed for PR 2843334531: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}\n",
      "[error] Logged PR 2843334531 (ai/ai_agent) to /Users/antoniozhong/Documents/dev/purdue/MSR2026/github_perf_patch_study/RQ3/rq3_validation_errors.csv: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}\n",
      "Processing [3/274] ai PR: 2926188053\n",
      "Running GEMINI, model: gemini-3-pro-preview\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 63\u001b[0m\n\u001b[1;32m     60\u001b[0m prefix, author_type, core \u001b[38;5;241m=\u001b[39m resolve_target(pr_id)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(ids)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m PR: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpr_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 63\u001b[0m gemini_res \u001b[38;5;241m=\u001b[39m analyze_pr(prefix, pr_id, author_type, core)\n\u001b[1;32m     65\u001b[0m records_gemini\u001b[38;5;241m.\u001b[39mappend(gemini_res)\n\u001b[1;32m     66\u001b[0m processed_ok \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[11], line 117\u001b[0m, in \u001b[0;36manalyze_pr\u001b[0;34m(prefix, pr_id, author_type, pr_core)\u001b[0m\n\u001b[1;32m    115\u001b[0m raw \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     raw \u001b[38;5;241m=\u001b[39m run_llm(\n\u001b[1;32m    118\u001b[0m         prompt\u001b[38;5;241m=\u001b[39mdeveloper_prompt,\n\u001b[1;32m    119\u001b[0m         user_prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m    120\u001b[0m     )\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    122\u001b[0m     raw \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[9], line 32\u001b[0m, in \u001b[0;36mrun_llm\u001b[0;34m(prompt, user_prompt)\u001b[0m\n\u001b[1;32m     26\u001b[0m         response \u001b[38;5;241m=\u001b[39m client_2\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mgenerate_content(\n\u001b[1;32m     27\u001b[0m             model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     28\u001b[0m             contents\u001b[38;5;241m=\u001b[39muser_prompt,\n\u001b[1;32m     29\u001b[0m             config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m     30\u001b[0m     )\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 32\u001b[0m         response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mgenerate_content(\n\u001b[1;32m     33\u001b[0m             model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     34\u001b[0m             contents\u001b[38;5;241m=\u001b[39muser_prompt,\n\u001b[1;32m     35\u001b[0m             config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m     36\u001b[0m     )  \n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/google/genai/models.py:5218\u001b[0m, in \u001b[0;36mModels.generate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   5216\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m remaining_remote_calls_afc \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   5217\u001b[0m   i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 5218\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_content(\n\u001b[1;32m   5219\u001b[0m       model\u001b[38;5;241m=\u001b[39mmodel, contents\u001b[38;5;241m=\u001b[39mcontents, config\u001b[38;5;241m=\u001b[39mparsed_config\n\u001b[1;32m   5220\u001b[0m   )\n\u001b[1;32m   5222\u001b[0m   function_map \u001b[38;5;241m=\u001b[39m _extra_utils\u001b[38;5;241m.\u001b[39mget_function_map(parsed_config)\n\u001b[1;32m   5223\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m function_map:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/google/genai/models.py:4000\u001b[0m, in \u001b[0;36mModels._generate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   3997\u001b[0m request_dict \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mconvert_to_dict(request_dict)\n\u001b[1;32m   3998\u001b[0m request_dict \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mencode_unserializable_types(request_dict)\n\u001b[0;32m-> 4000\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_client\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m   4001\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m'\u001b[39m, path, request_dict, http_options\n\u001b[1;32m   4002\u001b[0m )\n\u001b[1;32m   4004\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[1;32m   4005\u001b[0m     config, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshould_return_http_response\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4006\u001b[0m ):\n\u001b[1;32m   4007\u001b[0m   return_value \u001b[38;5;241m=\u001b[39m types\u001b[38;5;241m.\u001b[39mGenerateContentResponse(sdk_http_response\u001b[38;5;241m=\u001b[39mresponse)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/google/genai/_api_client.py:1388\u001b[0m, in \u001b[0;36mBaseApiClient.request\u001b[0;34m(self, http_method, path, request_dict, http_options)\u001b[0m\n\u001b[1;32m   1378\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m   1379\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1380\u001b[0m     http_method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1383\u001b[0m     http_options: Optional[HttpOptionsOrDict] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1384\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SdkHttpResponse:\n\u001b[1;32m   1385\u001b[0m   http_request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request(\n\u001b[1;32m   1386\u001b[0m       http_method, path, request_dict, http_options\n\u001b[1;32m   1387\u001b[0m   )\n\u001b[0;32m-> 1388\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(http_request, http_options, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1389\u001b[0m   response_body \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1390\u001b[0m       response\u001b[38;5;241m.\u001b[39mresponse_stream[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mresponse_stream \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1391\u001b[0m   )\n\u001b[1;32m   1392\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m SdkHttpResponse(headers\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders, body\u001b[38;5;241m=\u001b[39mresponse_body)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/google/genai/_api_client.py:1224\u001b[0m, in \u001b[0;36mBaseApiClient._request\u001b[0;34m(self, http_request, http_options, stream)\u001b[0m\n\u001b[1;32m   1221\u001b[0m     retry \u001b[38;5;241m=\u001b[39m tenacity\u001b[38;5;241m.\u001b[39mRetrying(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mretry_kwargs)\n\u001b[1;32m   1222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retry(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request_once, http_request, stream)  \u001b[38;5;66;03m# type: ignore[no-any-return]\u001b[39;00m\n\u001b[0;32m-> 1224\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request_once, http_request, stream)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/tenacity/__init__.py:477\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 477\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter(retry_state\u001b[38;5;241m=\u001b[39mretry_state)\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    479\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/tenacity/__init__.py:378\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    376\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mactions:\n\u001b[0;32m--> 378\u001b[0m     result \u001b[38;5;241m=\u001b[39m action(retry_state)\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/tenacity/__init__.py:400\u001b[0m, in \u001b[0;36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[0;34m(rs)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetryCallState\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    399\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mis_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mretry_run_result):\n\u001b[0;32m--> 400\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: rs\u001b[38;5;241m.\u001b[39moutcome\u001b[38;5;241m.\u001b[39mresult())\n\u001b[1;32m    401\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/tenacity/__init__.py:480\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 480\u001b[0m         result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[1;32m    482\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/google/genai/_api_client.py:1194\u001b[0m, in \u001b[0;36mBaseApiClient._request_once\u001b[0;34m(self, http_request, stream)\u001b[0m\n\u001b[1;32m   1190\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[1;32m   1191\u001b[0m       response\u001b[38;5;241m.\u001b[39mheaders, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response\u001b[38;5;241m.\u001b[39mtext]\n\u001b[1;32m   1192\u001b[0m   )\n\u001b[1;32m   1193\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1194\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_httpx_client\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m   1195\u001b[0m       method\u001b[38;5;241m=\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m   1196\u001b[0m       url\u001b[38;5;241m=\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39murl,\n\u001b[1;32m   1197\u001b[0m       headers\u001b[38;5;241m=\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m   1198\u001b[0m       content\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m   1199\u001b[0m       timeout\u001b[38;5;241m=\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[1;32m   1200\u001b[0m   )\n\u001b[1;32m   1201\u001b[0m   errors\u001b[38;5;241m.\u001b[39mAPIError\u001b[38;5;241m.\u001b[39mraise_for_response(response)\n\u001b[1;32m   1202\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[1;32m   1203\u001b[0m       response\u001b[38;5;241m.\u001b[39mheaders, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response\u001b[38;5;241m.\u001b[39mtext]\n\u001b[1;32m   1204\u001b[0m   )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpx/_client.py:825\u001b[0m, in \u001b[0;36mClient.request\u001b[0;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m    810\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    812\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_request(\n\u001b[1;32m    813\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    814\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    823\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mextensions,\n\u001b[1;32m    824\u001b[0m )\n\u001b[0;32m--> 825\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(request, auth\u001b[38;5;241m=\u001b[39mauth, follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[1;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_auth(\n\u001b[1;32m    915\u001b[0m     request,\n\u001b[1;32m    916\u001b[0m     auth\u001b[38;5;241m=\u001b[39mauth,\n\u001b[1;32m    917\u001b[0m     follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[1;32m    918\u001b[0m     history\u001b[38;5;241m=\u001b[39m[],\n\u001b[1;32m    919\u001b[0m )\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_redirects(\n\u001b[1;32m    943\u001b[0m         request,\n\u001b[1;32m    944\u001b[0m         follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[1;32m    945\u001b[0m         history\u001b[38;5;241m=\u001b[39mhistory,\n\u001b[1;32m    946\u001b[0m     )\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_single_request(request)\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpx/_client.py:1014\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1010\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1011\u001b[0m     )\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1014\u001b[0m     response \u001b[38;5;241m=\u001b[39m transport\u001b[38;5;241m.\u001b[39mhandle_request(request)\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1018\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpx/_transports/default.py:250\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    237\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    238\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    239\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    247\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    248\u001b[0m )\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 250\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mhandle_request(req)\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    255\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    256\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    257\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    258\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    259\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    253\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    232\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mhandle_request(\n\u001b[1;32m    237\u001b[0m         pool_request\u001b[38;5;241m.\u001b[39mrequest\n\u001b[1;32m    238\u001b[0m     )\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpcore/_sync/connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mhandle_request(request)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpcore/_sync/http11.py:136\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpcore/_sync/http11.py:106\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m     99\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    100\u001b[0m     (\n\u001b[1;32m    101\u001b[0m         http_version,\n\u001b[1;32m    102\u001b[0m         status,\n\u001b[1;32m    103\u001b[0m         reason_phrase,\n\u001b[1;32m    104\u001b[0m         headers,\n\u001b[1;32m    105\u001b[0m         trailing_data,\n\u001b[0;32m--> 106\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_response_headers(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    107\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    114\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpcore/_sync/http11.py:177\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    174\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_event(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpcore/_sync/http11.py:217\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\u001b[38;5;241m.\u001b[39mread(\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mREAD_NUM_BYTES, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    219\u001b[0m     )\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpcore/_backends/sync.py:128\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv(max_bytes)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/ssl.py:1296\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1293\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1294\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1295\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(buflen)\n\u001b[1;32m   1297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/ssl.py:1169\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1168\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n\u001b[1;32m   1170\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[1;32m   1171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "out_dir = PROJECT_ROOT / \"RQ3\"\n",
    "out_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "out_path_gemini = out_dir / \"rq3_validation_evidence_gemini.parquet\"\n",
    "out_path_gemini_partial = out_dir / \"rq3_validation_evidence_gemini_reprocess.partial.parquet\"\n",
    "\n",
    "error_csv_path = out_dir / \"rq3_validation_errors.csv\"\n",
    "\n",
    "df = pd.read_csv(\"/Users/antoniozhong/Documents/dev/purdue/MSR2026/github_perf_patch_study/RQ3/rq3_validation_errors_gemini.csv\")\n",
    "ids = df[\"pr_id\"].tolist()\n",
    "\n",
    "records_gemini = []\n",
    "ai_core = load_pr_core(\"ai\")\n",
    "human_core = load_pr_core(\"human\")\n",
    "\n",
    "def save_partial(records, out_path):\n",
    "    if not records:\n",
    "        return\n",
    "    df_tmp = pd.DataFrame(records)\n",
    "    df_tmp.to_parquet(out_path, index=False)\n",
    "    print(f\"[partial save] Saved {len(df_tmp)} rows to {out_path}\")\n",
    "\n",
    "def log_error(pr_id, prefix, author_type, exc):\n",
    "    row = {\n",
    "        \"prefix\": prefix,\n",
    "        \"pr_id\": pr_id,\n",
    "        \"author_type\": author_type,\n",
    "        \"error\": str(exc),\n",
    "    }\n",
    "    df_err = pd.DataFrame([row])\n",
    "    header = not error_csv_path.exists()\n",
    "    df_err.to_csv(error_csv_path, mode=\"a\", header=header, index=False)\n",
    "    print(f\"[error] Logged PR {pr_id} ({prefix}/{author_type}) to {error_csv_path}: {exc}\")\n",
    "\n",
    "def resolve_target(pr_id: int):\n",
    "    if pr_id in ai_core.index:\n",
    "        return \"ai\", \"ai_agent\", ai_core\n",
    "    if pr_id in human_core.index:\n",
    "        return \"human\", \"human\", human_core\n",
    "\n",
    "    try:\n",
    "        pid = int(pr_id)\n",
    "        if pid in ai_core.index:\n",
    "            return \"ai\", \"ai_agent\", ai_core\n",
    "        if pid in human_core.index:\n",
    "            return \"human\", \"human\", human_core\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    raise KeyError(f\"PR {pr_id} not found in ai_core or human_core\")\n",
    "\n",
    "print(f\"Re-processing {len(ids)} PRs (Gemini only).\")\n",
    "\n",
    "processed_ok = 0\n",
    "\n",
    "for idx, pr_id in enumerate(ids, 1):\n",
    "    try:\n",
    "        prefix, author_type, core = resolve_target(pr_id)\n",
    "        print(f\"Processing [{idx}/{len(ids)}] {prefix} PR: {pr_id}\")\n",
    "\n",
    "        gemini_res = analyze_pr(prefix, pr_id, author_type, core)\n",
    "\n",
    "        records_gemini.append(gemini_res)\n",
    "        processed_ok += 1\n",
    "\n",
    "        # GUARDA CADA ÉXITO\n",
    "        save_partial(records_gemini, out_path_gemini_partial)\n",
    "\n",
    "    except Exception as exc:\n",
    "        try:\n",
    "            prefix, author_type, _ = resolve_target(pr_id)\n",
    "        except Exception:\n",
    "            prefix, author_type = \"unknown\", \"unknown\"\n",
    "        log_error(pr_id, prefix=prefix, author_type=author_type, exc=exc)\n",
    "\n",
    "# Construye new_df desde lo que tienes en memoria (o del archivo partial si prefieres)\n",
    "new_df = pd.DataFrame(records_gemini)\n",
    "\n",
    "if new_df.empty:\n",
    "    print(\"[final] No new Gemini records to merge.\")\n",
    "    print(f\"Errored PRs (if any) logged to {error_csv_path}\")\n",
    "else:\n",
    "    if \"pr_id\" in new_df.columns:\n",
    "        try:\n",
    "            new_df[\"pr_id\"] = new_df[\"pr_id\"].astype(\"int64\")\n",
    "        except Exception:\n",
    "            new_df[\"pr_id\"] = pd.to_numeric(new_df[\"pr_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    ids_set = set(int(x) for x in ids)\n",
    "\n",
    "    if out_path_gemini.exists():\n",
    "        old_df = pd.read_parquet(out_path_gemini)\n",
    "\n",
    "        if \"pr_id\" in old_df.columns and \"pr_id\" in new_df.columns:\n",
    "            try:\n",
    "                old_df[\"pr_id\"] = old_df[\"pr_id\"].astype(\"int64\")\n",
    "            except Exception:\n",
    "                old_df[\"pr_id\"] = pd.to_numeric(old_df[\"pr_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "            old_df = old_df[~old_df[\"pr_id\"].isin(ids_set)]\n",
    "            final_df = pd.concat([old_df, new_df], ignore_index=True)\n",
    "        else:\n",
    "            final_df = pd.concat([old_df, new_df], ignore_index=True)\n",
    "    else:\n",
    "        final_df = new_df\n",
    "\n",
    "    final_df.to_parquet(out_path_gemini, index=False)\n",
    "    print(f\"Saved MERGED GEMINI {len(final_df)} rows to {out_path_gemini}\")\n",
    "    print(f\"Errored PRs (if any) logged to {error_csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
