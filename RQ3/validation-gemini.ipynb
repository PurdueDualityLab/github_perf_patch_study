{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf4ad7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.11/site-packages (2.3.3)\n",
      "Requirement already satisfied: python-dotenv in /opt/anaconda3/lib/python3.11/site-packages (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: google-genai==1.53.0 in /opt/anaconda3/lib/python3.11/site-packages (1.53.0)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /opt/anaconda3/lib/python3.11/site-packages (from google-genai==1.53.0) (4.11.0)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /opt/anaconda3/lib/python3.11/site-packages (from google-auth[requests]<3.0.0,>=2.14.1->google-genai==1.53.0) (2.30.0)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /opt/anaconda3/lib/python3.11/site-packages (from google-genai==1.53.0) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in /opt/anaconda3/lib/python3.11/site-packages (from google-genai==1.53.0) (2.12.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /opt/anaconda3/lib/python3.11/site-packages (from google-genai==1.53.0) (2.32.5)\n",
      "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /opt/anaconda3/lib/python3.11/site-packages (from google-genai==1.53.0) (9.1.2)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from google-genai==1.53.0) (15.0.1)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /opt/anaconda3/lib/python3.11/site-packages (from google-genai==1.53.0) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.11/site-packages (from anyio<5.0.0,>=4.8.0->google-genai==1.53.0) (3.11)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/lib/python3.11/site-packages (from anyio<5.0.0,>=4.8.0->google-genai==1.53.0) (1.3.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai==1.53.0) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda3/lib/python3.11/site-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai==1.53.0) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/anaconda3/lib/python3.11/site-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai==1.53.0) (4.9)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.11/site-packages (from httpx<1.0.0,>=0.28.1->google-genai==1.53.0) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.11/site-packages (from httpx<1.0.0,>=0.28.1->google-genai==1.53.0) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/lib/python3.11/site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai==1.53.0) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.9.0->google-genai==1.53.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.9.0->google-genai==1.53.0) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.9.0->google-genai==1.53.0) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.28.1->google-genai==1.53.0) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.28.1->google-genai==1.53.0) (2.5.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /opt/anaconda3/lib/python3.11/site-packages (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai==1.53.0) (0.4.8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install pandas python-dotenv\n",
    "!python -m pip install google-genai==1.53.0\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List, Optional, Tuple\n",
    "import pandas as pd\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1cd09b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"validation_present\": {\n",
    "            \"type\": \"boolean\"\n",
    "        },\n",
    "        \"evidence_sources\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": [\"pipeline\", \"description\", \"comments\"]\n",
    "            }\n",
    "        },\n",
    "        \"validation_type\": {\n",
    "            \"type\": \"string\",\n",
    "            \"enum\": [\n",
    "                \"benchmark\",\n",
    "                \"profiling\",\n",
    "                \"static-analysis\",\n",
    "                \"anecdotal\",\n",
    "            ]\n",
    "        },\n",
    "        \"validation_description\": {\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        \"pipeline_signal\": {\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        \"description_signal\": {\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        \"comment_signal\": {\n",
    "            \"type\": \"string\"\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\n",
    "        \"validation_present\",\n",
    "        \"evidence_sources\",\n",
    "        \"validation_type\",\n",
    "        \"validation_description\",\n",
    "        \"pipeline_signal\",\n",
    "        \"description_signal\",\n",
    "        \"comment_signal\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfa3c233",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0  \n",
    "\n",
    "def run_llm(prompt: str,user_prompt: str) -> str:\n",
    "    model = \"gemini-3-pro-preview\"\n",
    "    print(\"Running GEMINI, model:\", model)\n",
    "        \n",
    "    GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "    GEMINI_API_KEY_2 = os.getenv(\"GEMINI_API_KEY_2\")\n",
    "    client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "    client_2 = genai.Client(api_key=GEMINI_API_KEY_2)\n",
    "\n",
    "        \n",
    "    config = types.GenerateContentConfig(\n",
    "            temperature=0.0,\n",
    "            response_mime_type=\"application/json\",\n",
    "            response_schema=schema,\n",
    "            thinking_config=types.ThinkingConfig(\n",
    "                thinking_level=types.ThinkingLevel.HIGH\n",
    "            ),\n",
    "            system_instruction=prompt\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        response = None    \n",
    "        if counter >= 210:   \n",
    "            response = client_2.models.generate_content(\n",
    "                model=model,\n",
    "                contents=user_prompt,\n",
    "                config=config,\n",
    "        )\n",
    "        else:\n",
    "            response = client.models.generate_content(\n",
    "                model=model,\n",
    "                contents=user_prompt,\n",
    "                config=config,\n",
    "        )    \n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(\"GEMINI API Error:\", e)\n",
    "        raise e\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfa3b352",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_datasets_dir(start: Optional[Path] = None) -> Path:\n",
    "    start = start or Path.cwd()\n",
    "    for path in (start, *start.parents):\n",
    "        candidate = path / \"datasets\"\n",
    "        if candidate.exists():\n",
    "            return candidate\n",
    "    raise FileNotFoundError(f\"Could not find 'datasets' directory from {start}\")\n",
    "\n",
    "\n",
    "DATASETS_DIR = find_datasets_dir()\n",
    "PROJECT_ROOT = DATASETS_DIR.parent\n",
    "\n",
    "def extract_json(text: str) -> Dict:\n",
    "    \"\"\"Best-effort JSON extraction from model output.\"\"\"\n",
    "    start = text.find(\"{\")\n",
    "    end = text.rfind(\"}\")\n",
    "    if start != -1 and end != -1 and end > start:\n",
    "        snippet = text[start : end + 1]\n",
    "        try:\n",
    "            return json.loads(snippet)\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    return {}\n",
    "\n",
    "def truncate(text: str, limit: int = 10000) -> str:\n",
    "    return text if len(text) <= limit else text[:limit] + \"...[truncated]\"\n",
    "\n",
    "def load_pr_core(prefix: str) -> pd.DataFrame:\n",
    "    commits = pd.read_parquet(\n",
    "        DATASETS_DIR / f\"{prefix}_pr\" / f\"{prefix}_pr_commits.parquet\"\n",
    "    )\n",
    "    return commits.drop_duplicates(\"pr_id\").set_index(\"pr_id\")\n",
    "\n",
    "\n",
    "def collect_comments(prefix: str, pr_id: int) -> List[str]:\n",
    "    issue = pd.read_parquet(\n",
    "        DATASETS_DIR / f\"{prefix}_pr\" / f\"{prefix}_pr_issue_comments.parquet\"\n",
    "    )\n",
    "    review = pd.read_parquet(\n",
    "        DATASETS_DIR / f\"{prefix}_pr\" / f\"{prefix}_pr_review_comments.parquet\"\n",
    "    )\n",
    "    texts = []\n",
    "    for df in (issue, review):\n",
    "        subset = df[df[\"pr_id\"] == pr_id]\n",
    "        texts.extend(subset[\"body\"].dropna().tolist())\n",
    "    return texts\n",
    "\n",
    "\n",
    "def collect_pipeline_names(prefix: str, pr_id: int) -> List[str]:\n",
    "    workflows = pd.read_parquet(\n",
    "        DATASETS_DIR / f\"{prefix}_pr\" / f\"{prefix}_pr_workflow_runs.parquet\"\n",
    "    )\n",
    "    subset = workflows[workflows[\"pr_id\"] == pr_id]\n",
    "    return sorted(subset[\"workflow_name\"].dropna().unique().tolist())\n",
    "\n",
    "def pr_ids_from_commits(prefix: str, limit: Optional[int] = None) -> Iterable[int]:\n",
    "    commits = pd.read_parquet(\n",
    "        DATASETS_DIR / f\"{prefix}_pr\" / f\"{prefix}_pr_commits.parquet\"\n",
    "    )\n",
    "    pr_ids = sorted(commits[\"pr_id\"].dropna().astype(int).unique().tolist())\n",
    "    return pr_ids if limit is None else pr_ids[:limit]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c80e7dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_pr(\n",
    "    prefix: str,\n",
    "    pr_id: int,\n",
    "    author_type: str,\n",
    "    pr_core: pd.DataFrame,\n",
    ") -> Tuple[Dict, Dict]:\n",
    "    row = pr_core.loc[pr_id]\n",
    "    pipeline_names = collect_pipeline_names(prefix, pr_id)\n",
    "    comments = collect_comments(prefix, pr_id)\n",
    "    description = (row.get(\"pr_description\") or \"\").strip()\n",
    "    code_diff = (row.get(\"patch\") or \"\").strip()\n",
    "\n",
    "    DEVELOPER_PROMPT_TEMPLATE = \"\"\"\n",
    "    You are a classifier for performance validation evidence in GitHub Pull Requests (PRs).\n",
    "\n",
    "    Your job is to decide whether there is explicit performance validation, and if so,\n",
    "    to classify it into EXACTLY ONE validation_type category based on the definitions below.\n",
    "\n",
    "    First, carefully read and internalize these validation_type categories:\n",
    "\n",
    "    1. Benchmark-Based Validation (Unit Tests or Microbenchmarks)\n",
    "    Definition:\n",
    "    The PR validates performance by running benchmark tests—either existing unit tests or newly added benchmark/microbenchmark tests. It includes explicit before-and-after comparisons such as runtime, throughput, memory, CPU usage, or any quantitative metric collected from tests.\n",
    "\n",
    "    2. Profiling-Based Validation (Application- or Function-Level Profiling)\n",
    "    Definition:\n",
    "    The PR uses profiling tools to validate performance, typically capturing stack samples, CPU hotspots, flamegraphs, or function-level timings. Evidence includes profiling outputs before and after the change.\n",
    "\n",
    "    3. Static-Analysis-Based Validation (Reasoning Without Runtime Evidence)\n",
    "    Definition:\n",
    "    The PR argues for performance improvement through static reasoning about the code—algorithmic complexity, data-structure changes, loop bounds, allocation count reduction, etc., without providing runtime/profiling data.\n",
    "\n",
    "    4. Anecdotal or Informal Local Testing (No Evidence Provided)\n",
    "    Definition:\n",
    "    The PR claims that performance is improved based on local testing, intuition, or manual observation, but provides no quantitative metrics, no profiling output, and no static-analysis justification.\n",
    "\n",
    "    Decision Rules:\n",
    "\n",
    "    1) First decide if performance validation is explicitly present (validation_present).\n",
    "    - Set validation_present = TRUE only when the PR explicitly shows some form of validation evidence:\n",
    "        benchmarks/microbenchmarks, profiling traces, static performance reasoning,\n",
    "        or explicit local testing statements.\n",
    "    - Do NOT infer validation from performance intent alone.\n",
    "\n",
    "    2) If validation_present = FALSE:\n",
    "    - Set validation_type = \"none\".\n",
    "    - Set evidence_sources = [].\n",
    "    - Set validation_description to explain the absence of validation.\n",
    "    - Never assign any other validation_type.\n",
    "    \n",
    "    3) If validation_present = TRUE:\n",
    "    - Choose exactly ONE non-\"none\" validation_type from the following: Benchmark-Based Validation; Profiling-Based Validation; Static-Analysis-Based Validation; Anecdotal or Informal Local Testing\n",
    "\n",
    "    4) evidence_sources must list where the validation is explicitly mentioned:\n",
    "    - \"pipeline\", \"description\", \"comments\", \"code_diff\"\n",
    "\n",
    "    5) When validation_type is \"benchmark\", \"profiling\":\n",
    "    mention the metrics used (latency, throughput, memory, CPU, etc.).\n",
    "\n",
    "    You must ALWAYS return STRICT JSON with exactly these keys:\n",
    "\n",
    "    validation_present, evidence_sources, validation_type, metrics,\n",
    "    validation_description, pipeline_signal,\n",
    "    description_signal, comment_signal.\n",
    "\n",
    "    No extra commentary. No markdown.\n",
    "    No explanations.\n",
    "    \"\"\"\n",
    "\n",
    "    USER_PROMPT_TEMPLATE = \"\"\"\n",
    "    Classify the following PR strictly using the rules and definitions from the system.\n",
    "\n",
    "    PIPELINES:\n",
    "    {pipeline_names}\n",
    "\n",
    "    DESCRIPTION:\n",
    "    {description}\n",
    "\n",
    "    COMMENTS:\n",
    "    {comments}\n",
    "    \n",
    "    CODE DIFF:\n",
    "    {code_diff}\n",
    "    \"\"\"\n",
    "\n",
    "    empty_record = {\n",
    "        \"pr_id\": pr_id,\n",
    "        \"author_type\": author_type,\n",
    "        \"repo\": f\"{row.get('repo_owner')}/{row.get('repo_name')}\",\n",
    "        \"pr_number\": row.get(\"pr_number\"),\n",
    "        \"pr_title\": row.get(\"pr_title\"),\n",
    "        \"pipeline_names\": pipeline_names,\n",
    "        \"validation_present\": False,\n",
    "        \"evidence_sources\": [],\n",
    "        \"validation_type\": \"none\",\n",
    "        \"validation_description\": \"No validation evidence\",\n",
    "        \"pipeline_signal\": \"\",\n",
    "        \"description_signal\": \"\",\n",
    "        \"comment_signal\": \"\",\n",
    "    }\n",
    "\n",
    "    if not pipeline_names and not description and not comments and not code_diff:\n",
    "        print(f\"Short-circuiting PR {pr_id} with no signals\")\n",
    "        return empty_record\n",
    "\n",
    "    prompt = USER_PROMPT_TEMPLATE.format(\n",
    "        pipeline_names=\"- \" + \"- \".join(pipeline_names) if pipeline_names else \"None\",\n",
    "        description=truncate(description) if description else \"None\",\n",
    "        comments=\"- \" + \"- \".join(truncate(\" | \".join(comments)).split(\" | \")) if comments else \"None\",\n",
    "        code_diff=truncate(code_diff) if code_diff else \"None\",\n",
    "    )\n",
    "\n",
    "    developer_prompt = DEVELOPER_PROMPT_TEMPLATE\n",
    "\n",
    "    raw = \"\"\n",
    "    try:\n",
    "        raw = run_llm(\n",
    "            prompt=developer_prompt,\n",
    "            user_prompt=prompt,\n",
    "        )\n",
    "    except Exception as exc:\n",
    "        raw = \"\"\n",
    "        print(f\"gemini model call failed for PR {pr_id}: {exc}\")\n",
    "\n",
    "    parsed_llm = extract_json(raw) or {}\n",
    "\n",
    "    evidence_sources = parsed_llm.get(\"evidence_sources\") or []\n",
    "    if isinstance(evidence_sources, (tuple, list)):\n",
    "        evidence_sources = list(evidence_sources)\n",
    "    else:\n",
    "        evidence_sources = []\n",
    "\n",
    "    result = {\n",
    "        \"pr_id\": pr_id,\n",
    "        \"author_type\": author_type,\n",
    "        \"repo\": f\"{row.get('repo_owner')}/{row.get('repo_name')}\",\n",
    "        \"pr_number\": row.get(\"pr_number\"),\n",
    "        \"pr_title\": row.get(\"pr_title\"),\n",
    "        \"pipeline_names\": pipeline_names,\n",
    "        \"validation_present\": parsed_llm.get(\"validation_present\", False),\n",
    "        \"evidence_sources\": evidence_sources,\n",
    "        \"validation_type\": parsed_llm.get(\"validation_type\", \"none\"),\n",
    "        \"validation_description\": parsed_llm.get(\"validation_description\", \"No validation evidence\"),\n",
    "        \"pipeline_signal\": parsed_llm.get(\"pipeline_signal\", \"\"),\n",
    "        \"description_signal\": parsed_llm.get(\"description_signal\", \"\"),\n",
    "        \"comment_signal\": parsed_llm.get(\"comment_signal\", \"\"),\n",
    "    }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de14159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "out_dir = PROJECT_ROOT / \"RQ3\"\n",
    "out_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "out_path_gemini = out_dir / \"rq3_validation_evidence_gemini.parquet\"\n",
    "error_csv_path = out_dir / \"rq3_validation_errors_gemini.csv\"\n",
    "\n",
    "records_gemini = []\n",
    "processed = set()\n",
    "\n",
    "ai_core = load_pr_core(\"ai\")\n",
    "human_core = load_pr_core(\"human\")\n",
    "\n",
    "limit = None\n",
    "\n",
    "# ============================\n",
    "# Resume positions (1-based)\n",
    "# ============================\n",
    "start_ai_pos = 171\n",
    "start_human_pos = 1\n",
    "\n",
    "# Convert to 0-based slice indices safely\n",
    "ai_start_idx = max(start_ai_pos - 1, 0)\n",
    "human_start_idx = max(start_human_pos - 1, 0)\n",
    "\n",
    "# ============================\n",
    "# Helpers\n",
    "# ============================\n",
    "def save_partial(records, out_path):\n",
    "    \"\"\"Save a partial parquet snapshot of current records.\"\"\"\n",
    "    if not records:\n",
    "        return\n",
    "    df_tmp = pd.DataFrame(records)\n",
    "    df_tmp.to_parquet(out_path, index=False)\n",
    "    print(f\"[partial save] Saved {len(df_tmp)} rows to {out_path}\")\n",
    "\n",
    "def log_error(pr_id, prefix, author_type, exc):\n",
    "    \"\"\"Append an error row to the CSV log.\"\"\"\n",
    "    row = {\n",
    "        \"prefix\": prefix,\n",
    "        \"pr_id\": pr_id,\n",
    "        \"author_type\": author_type,\n",
    "        \"error\": str(exc),\n",
    "    }\n",
    "    df_err = pd.DataFrame([row])\n",
    "    header = not error_csv_path.exists()\n",
    "    df_err.to_csv(error_csv_path, mode=\"a\", header=header, index=False)\n",
    "    print(f\"[error] Logged PR {pr_id} ({prefix}/{author_type}) to {error_csv_path}: {exc}\")\n",
    "\n",
    "def _merge_record(prefix, pr_id, author_type, res_dict):\n",
    "    \"\"\"\n",
    "    Merge metadata with the model result without overwriting metadata keys.\n",
    "    Ensures prefix/pr_id/author_type are present in the saved parquet.\n",
    "    \"\"\"\n",
    "    base = {\"prefix\": prefix, \"pr_id\": pr_id, \"author_type\": author_type}\n",
    "    if isinstance(res_dict, dict):\n",
    "        for k, v in res_dict.items():\n",
    "            if k not in base:\n",
    "                base[k] = v\n",
    "    return base\n",
    "\n",
    "def is_gemini_overloaded_error(exc: Exception) -> bool:\n",
    "    \"\"\"\n",
    "    Detect transient Gemini overload errors.\n",
    "    We treat these as retryable once.\n",
    "    \"\"\"\n",
    "    msg = str(exc).lower()\n",
    "    return (\"503\" in msg) or (\"unavailable\" in msg) or (\"overloaded\" in msg)\n",
    "\n",
    "def analyze_pr_with_one_retry(prefix: str, pr_id: int, author_type: str, core):\n",
    "    \"\"\"\n",
    "    Try analyze_pr once.\n",
    "    If it fails due to 503/UNAVAILABLE/overloaded, retry exactly one more time.\n",
    "    If it fails again, re-raise so the caller logs it to CSV.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return analyze_pr(prefix, pr_id, author_type, core)\n",
    "    except Exception as exc:\n",
    "        if is_gemini_overloaded_error(exc):\n",
    "            print(f\"[retry] Gemini overloaded for PR {pr_id}. Retrying once...\")\n",
    "            return analyze_pr(prefix, pr_id, author_type, core)\n",
    "        raise\n",
    "\n",
    "# ============================\n",
    "# Load existing partial (recommended)\n",
    "# ============================\n",
    "if out_path_gemini.exists():\n",
    "    try:\n",
    "        df_prev = pd.read_parquet(out_path_gemini)\n",
    "        records_gemini = df_prev.to_dict(\"records\")\n",
    "\n",
    "        # Build a set to skip already processed PRs\n",
    "        if {\"prefix\", \"pr_id\"}.issubset(df_prev.columns):\n",
    "            processed = set(zip(df_prev[\"prefix\"].astype(str), df_prev[\"pr_id\"].astype(int)))\n",
    "        elif \"pr_id\" in df_prev.columns:\n",
    "            processed = set(df_prev[\"pr_id\"].astype(int).tolist())\n",
    "\n",
    "        print(f\"[resume] Loaded {len(records_gemini)} existing records from {out_path_gemini}\")\n",
    "    except Exception as exc:\n",
    "        print(f\"[resume] Could not read existing parquet, starting fresh. Reason: {exc}\")\n",
    "\n",
    "# ============================\n",
    "# Build ID lists\n",
    "# ============================\n",
    "ai_ids = list(pr_ids_from_commits(\"ai\", limit=limit))\n",
    "human_ids = list(pr_ids_from_commits(\"human\", limit=limit))\n",
    "\n",
    "print(f\"Processing {len(ai_ids)} AI PRs and {len(human_ids)} human PRs (first {limit} each).\")\n",
    "print(f\"[resume] AI starting at position {start_ai_pos} (slice index {ai_start_idx})\")\n",
    "print(f\"[resume] Human starting at position {start_human_pos} (slice index {human_start_idx})\")\n",
    "\n",
    "# Slice the lists to start from the requested positions\n",
    "ai_ids_to_process = ai_ids[ai_start_idx:]\n",
    "human_ids_to_process = human_ids[human_start_idx:]\n",
    "\n",
    "# ============================\n",
    "# Process AI PRs\n",
    "# ============================\n",
    "for idx, pr_id in enumerate(ai_ids_to_process, start=start_ai_pos):\n",
    "    print(f\"Processing AI PR {idx}/{len(ai_ids)}: {pr_id}\")\n",
    "\n",
    "    # Skip if already processed (based on loaded parquet)\n",
    "    if (\"ai\", pr_id) in processed or pr_id in processed:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        gemini_res = analyze_pr_with_one_retry(\"ai\", pr_id, \"ai_agent\", ai_core)\n",
    "        record = _merge_record(\"ai\", pr_id, \"ai_agent\", gemini_res)\n",
    "        records_gemini.append(record)\n",
    "        processed.add((\"ai\", pr_id))\n",
    "    except Exception as exc:\n",
    "        log_error(pr_id, prefix=\"ai\", author_type=\"ai_agent\", exc=exc)\n",
    "\n",
    "    # Save every 10 total records currently in memory\n",
    "    if len(records_gemini) % 10 == 0:\n",
    "        save_partial(records_gemini, out_path_gemini)\n",
    "\n",
    "# ============================\n",
    "# Process Human PRs\n",
    "# ============================\n",
    "for idx, pr_id in enumerate(human_ids_to_process, start=start_human_pos):\n",
    "    print(f\"Processing human PR {idx}/{len(human_ids)}: {pr_id}\")\n",
    "\n",
    "    # Skip if already processed (based on loaded parquet)\n",
    "    if (\"human\", pr_id) in processed or pr_id in processed:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        gemini_res = analyze_pr_with_one_retry(\"human\", pr_id, \"human\", human_core)\n",
    "        record = _merge_record(\"human\", pr_id, \"human\", gemini_res)\n",
    "        records_gemini.append(record)\n",
    "        processed.add((\"human\", pr_id))\n",
    "    except Exception as exc:\n",
    "        log_error(pr_id, prefix=\"human\", author_type=\"human\", exc=exc)\n",
    "\n",
    "    # Save every 10 total records currently in memory\n",
    "    if len(records_gemini) % 10 == 0:\n",
    "        save_partial(records_gemini, out_path_gemini)\n",
    "\n",
    "# ============================\n",
    "# Final save\n",
    "# ============================\n",
    "df_gemini = pd.DataFrame(records_gemini)\n",
    "df_gemini.to_parquet(out_path_gemini, index=False)\n",
    "print(f\"Saved FINAL GEMINI {len(df_gemini)} rows to {out_path_gemini}\")\n",
    "print(f\"Errored PRs (if any) logged to {error_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1e5a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "out_dir = PROJECT_ROOT / \"RQ3\"\n",
    "out_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "out_path_gemini = out_dir / \"rq3_validation_evidence_gemini.parquet\"\n",
    "error_csv_path = out_dir / \"rq3_validation_errors.csv\"\n",
    "\n",
    "records_gemini = []\n",
    "ai_core = load_pr_core(\"ai\")\n",
    "human_core = load_pr_core(\"human\")\n",
    "\n",
    "def save_partial(records, out_path):\n",
    "    if not records:\n",
    "        return\n",
    "    df_tmp = pd.DataFrame(records)\n",
    "    df_tmp.to_parquet(out_path, index=False)\n",
    "    print(f\"[partial save] Saved {len(df_tmp)} rows to {out_path}\")\n",
    "\n",
    "def log_error(pr_id, prefix, author_type, exc):\n",
    "    row = {\n",
    "        \"prefix\": prefix,\n",
    "        \"pr_id\": pr_id,\n",
    "        \"author_type\": author_type,\n",
    "        \"error\": str(exc),\n",
    "    }\n",
    "    df_err = pd.DataFrame([row])\n",
    "    header = not error_csv_path.exists()\n",
    "    df_err.to_csv(error_csv_path, mode=\"a\", header=header, index=False)\n",
    "    print(f\"[error] Logged PR {pr_id} ({prefix}/{author_type}) to {error_csv_path}: {exc}\")\n",
    "\n",
    "\n",
    "\n",
    "def resolve_target(pr_id: int):\n",
    "    if pr_id in ai_core.index:\n",
    "        return \"ai\", \"ai_agent\", ai_core\n",
    "    if pr_id in human_core.index:\n",
    "        return \"human\", \"human\", human_core\n",
    "\n",
    "    try:\n",
    "        pid = int(pr_id)\n",
    "        if pid in ai_core.index:\n",
    "            return \"ai\", \"ai_agent\", ai_core\n",
    "        if pid in human_core.index:\n",
    "            return \"human\", \"human\", human_core\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    raise KeyError(f\"PR {pr_id} not found in ai_core or human_core\")\n",
    "\n",
    "print(f\"Re-processing {len(ids)} PRs (Gemini only).\")\n",
    "\n",
    "processed_ok = 0\n",
    "\n",
    "for idx, pr_id in enumerate(ids, 1):\n",
    "    try:\n",
    "        prefix, author_type, core = resolve_target(pr_id)\n",
    "        print(f\"Processing [{idx}/{len(ids)}] {prefix} PR: {pr_id}\")\n",
    "\n",
    "        gemini_res = analyze_pr(prefix, pr_id, author_type, core)\n",
    "\n",
    "        records_gemini.append(gemini_res)\n",
    "        processed_ok += 1\n",
    "\n",
    "    except Exception as exc:\n",
    "        try:\n",
    "            prefix, author_type, _ = resolve_target(pr_id)\n",
    "        except Exception:\n",
    "            prefix, author_type = \"unknown\", \"unknown\"\n",
    "        log_error(pr_id, prefix=prefix, author_type=author_type, exc=exc)\n",
    "\n",
    "    if processed_ok > 0 and processed_ok % 10 == 0:\n",
    "        save_partial(records_gemini, out_path_gemini)\n",
    "\n",
    "new_df = pd.DataFrame(records_gemini)\n",
    "\n",
    "if new_df.empty:\n",
    "    print(\"[final] No new Gemini records to merge.\")\n",
    "    print(f\"Errored PRs (if any) logged to {error_csv_path}\")\n",
    "else:\n",
    "    if \"pr_id\" not in new_df.columns:\n",
    "\n",
    "        pass\n",
    "\n",
    "    if \"pr_id\" in new_df.columns:\n",
    "        try:\n",
    "            new_df[\"pr_id\"] = new_df[\"pr_id\"].astype(\"int64\")\n",
    "        except Exception:\n",
    "            new_df[\"pr_id\"] = pd.to_numeric(new_df[\"pr_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    ids_set = set(int(x) for x in ids)\n",
    "\n",
    "    if out_path_gemini.exists():\n",
    "        old_df = pd.read_parquet(out_path_gemini)\n",
    "\n",
    "        if \"pr_id\" in old_df.columns and \"pr_id\" in new_df.columns:\n",
    "\n",
    "            try:\n",
    "                old_df[\"pr_id\"] = old_df[\"pr_id\"].astype(\"int64\")\n",
    "            except Exception:\n",
    "                old_df[\"pr_id\"] = pd.to_numeric(old_df[\"pr_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "            old_df = old_df[~old_df[\"pr_id\"].isin(ids_set)]\n",
    "\n",
    "            final_df = pd.concat([old_df, new_df], ignore_index=True)\n",
    "        else:\n",
    "            final_df = pd.concat([old_df, new_df], ignore_index=True)\n",
    "    else:\n",
    "        final_df = new_df\n",
    "\n",
    "    final_df.to_parquet(out_path_gemini, index=False)\n",
    "    print(f\"Saved MERGED GEMINI {len(final_df)} rows to {out_path_gemini}\")\n",
    "    print(f\"Errored PRs (if any) logged to {error_csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
