{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e779bfd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List, Optional\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "64e600d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_llm(system_pront: str, prompt: str, type: str = \"ollama\", model: str = \"gemma3:27b\") -> str:\n",
    "    \"\"\"Call local OLLAMA.\"\"\"\n",
    "    LLM_BASE_URL = \"http://localhost:11434/v1\"\n",
    "    LLM_API_KEY = \"\"\n",
    "    llm_client = OpenAI(base_url=LLM_BASE_URL, api_key=LLM_API_KEY)\n",
    "    \n",
    "  \n",
    "    \n",
    "    if(type == \"ollama\"):\n",
    "        print(\"Running OLLAMA, model:\", model)\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_pront},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "        \n",
    "        r = llm_client.chat.completions.create(\n",
    "            model= model,\n",
    "            messages=messages,\n",
    "        )\n",
    "        result = r.choices[0].message.content.strip()\n",
    "        print(\"OLLAMA response:\", result)\n",
    "        return result.strip()\n",
    "    elif(type == \"gemini\"):\n",
    "        \n",
    "        model = \"gemini-3-pro-preview\"\n",
    "        print(\"Running GEMINI, model:\", model)\n",
    "        \n",
    "        GEMINI_API_KEY = os.environ.get(\"GEMINI_API_KEY\")\n",
    "        client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "\n",
    "        schema={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"validation_present\": {\n",
    "                        \"type\": \"boolean\"\n",
    "                    },\n",
    "                    \"evidence_sources\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"enum\": [\"pipeline\", \"description\", \"comments\"]\n",
    "                        }\n",
    "                    },\n",
    "                    \"validation_type\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\n",
    "                            \"benchmark\",\n",
    "                            \"profiling\",\n",
    "                            \"load/canary\",\n",
    "                            \"unit-only\",\n",
    "                            \"unspecified\",\n",
    "                            \"none\"\n",
    "                        ]\n",
    "                    },\n",
    "                    \"validation_description\": {\n",
    "                        \"type\": \"string\"\n",
    "                    },\n",
    "                    \"pipeline_signal\": {\n",
    "                        \"type\": \"string\"\n",
    "                    },\n",
    "                    \"description_signal\": {\n",
    "                        \"type\": \"string\"\n",
    "                    },\n",
    "                    \"comment_signal\": {\n",
    "                        \"type\": \"string\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\n",
    "                    \"validation_present\",\n",
    "                    \"evidence_sources\",\n",
    "                    \"validation_type\",\n",
    "                    \"validation_description\",\n",
    "                    \"pipeline_signal\",\n",
    "                    \"description_signal\",\n",
    "                    \"comment_signal\"\n",
    "                ]\n",
    "            }\n",
    "        \n",
    "        config = types.GenerateContentConfig(\n",
    "            temperature=0.0,\n",
    "            response_mime_type=\"application/json\",\n",
    "            response_schema=schema,\n",
    "            thinking_config=types.ThinkingConfig(\n",
    "                thinking_level=types.ThinkingLevel.HIGH\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        messages = [\n",
    "            system_pront,\n",
    "            prompt\n",
    "        ]\n",
    "        \n",
    "        response = client.models.generate_content(\n",
    "            model=model,\n",
    "            contents=messages,\n",
    "            config=config,\n",
    "        )\n",
    "        return response.text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "56c2b2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_datasets_dir(start: Optional[Path] = None) -> Path:\n",
    "    start = start or Path.cwd()\n",
    "    for path in (start, *start.parents):\n",
    "        candidate = path / \"datasets\"\n",
    "        if candidate.exists():\n",
    "            return candidate\n",
    "    raise FileNotFoundError(f\"Could not find 'datasets' directory from {start}\")\n",
    "\n",
    "\n",
    "DATASETS_DIR = find_datasets_dir()\n",
    "PROJECT_ROOT = DATASETS_DIR.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "10afe7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json(text: str) -> Dict:\n",
    "    \"\"\"Best-effort JSON extraction from model output.\"\"\"\n",
    "    start = text.find(\"{\")\n",
    "    end = text.rfind(\"}\")\n",
    "    if start != -1 and end != -1 and end > start:\n",
    "        snippet = text[start : end + 1]\n",
    "        try:\n",
    "            return json.loads(snippet)\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    return {}\n",
    "\n",
    "def truncate(text: str, limit: int = 10000) -> str:\n",
    "    return text if len(text) <= limit else text[:limit] + \"...[truncated]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "29784b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pr_core(prefix: str) -> pd.DataFrame:\n",
    "    commits = pd.read_parquet(\n",
    "        DATASETS_DIR / f\"{prefix}_pr\" / f\"{prefix}_pr_commits.parquet\"\n",
    "    )\n",
    "    return commits.drop_duplicates(\"pr_id\").set_index(\"pr_id\")\n",
    "\n",
    "\n",
    "def collect_comments(prefix: str, pr_id: int) -> List[str]:\n",
    "    issue = pd.read_parquet(\n",
    "        DATASETS_DIR / f\"{prefix}_pr\" / f\"{prefix}_pr_issue_comments.parquet\"\n",
    "    )\n",
    "    review = pd.read_parquet(\n",
    "        DATASETS_DIR / f\"{prefix}_pr\" / f\"{prefix}_pr_review_comments.parquet\"\n",
    "    )\n",
    "    texts = []\n",
    "    for df in (issue, review):\n",
    "        subset = df[df[\"pr_id\"] == pr_id]\n",
    "        texts.extend(subset[\"body\"].dropna().tolist())\n",
    "    return texts\n",
    "\n",
    "\n",
    "def collect_pipeline_names(prefix: str, pr_id: int) -> List[str]:\n",
    "    workflows = pd.read_parquet(\n",
    "        DATASETS_DIR / f\"{prefix}_pr\" / f\"{prefix}_pr_workflow_runs.parquet\"\n",
    "    )\n",
    "    subset = workflows[workflows[\"pr_id\"] == pr_id]\n",
    "    return sorted(subset[\"workflow_name\"].dropna().unique().tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "63d63ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_pr(\n",
    "    prefix: str, pr_id: int, author_type: str, pr_core: pd.DataFrame\n",
    ") -> Dict:\n",
    "    row = pr_core.loc[pr_id]\n",
    "    pipeline_names = collect_pipeline_names(prefix, pr_id)\n",
    "    comments = collect_comments(prefix, pr_id)\n",
    "    description = (row.get(\"pr_description\") or \"\").strip()\n",
    "    \n",
    "    SYSTEM_PROMPT_TEMPLATE = \"\"\"You classify evidence of performance validation for a PR.\n",
    "    Return compact JSON only with keys:\n",
    "    validation_present (bool), evidence_sources (list of \"pipeline\",\"description\",\"comments\"),\n",
    "    validation_type (benchmark,profiling,load/canary,unit-only,unspecified,none),\n",
    "    validation_description (short text),\n",
    "    pipeline_signal (short), description_signal (short), comment_signal (short).\n",
    "\n",
    "    Rules:\n",
    "    - Pipelines count only if workflow names imply perf/benchmark/load/canary; note when they are unit/lint-only.\n",
    "    - Description/comments count if they mention perf benchmarks, profiling, latency/throughput numbers,\n",
    "    load/canary rollout, A/B tests, perf tools, or explicit \"no perf validation\".\n",
    "    - If nothing indicates perf validation, set validation_present=false,\n",
    "    validation_type=\"none\", evidence_sources=[],\n",
    "    validation_description=\"No validation evidence\".\n",
    "    \"\"\"\n",
    "    \n",
    "    PROMPT_TEMPLATE = \"\"\"\n",
    "    You are given information about a GitHub Pull Request (PR).\n",
    "    Using the provided PIPELINES, DESCRIPTION, and COMMENTS, determine if there is evidence of performance validation for the PR.\n",
    "    Input (TOONS format):\n",
    "\n",
    "    PIPELINES:\n",
    "    {pipeline_names}\n",
    "\n",
    "    DESCRIPTION:\n",
    "    {description}\n",
    "\n",
    "    COMMENTS:\n",
    "    {comments}\n",
    "\n",
    "    JSON:\n",
    "    \"\"\"\n",
    "\n",
    "    if not pipeline_names and not description and not comments:\n",
    "        return {\n",
    "            \"pr_id\": pr_id,\n",
    "            \"author_type\": author_type,\n",
    "            \"repo\": f\"{row.get('repo_owner')}/{row.get('repo_name')}\",\n",
    "            \"pr_number\": row.get(\"pr_number\"),\n",
    "            \"pr_title\": row.get(\"pr_title\"),\n",
    "            \"pipeline_names\": pipeline_names,\n",
    "            \"validation_present\": False,\n",
    "            \"evidence_sources\": [],\n",
    "            \"validation_type\": \"none\",\n",
    "            \"validation_description\": \"No validation evidence\",\n",
    "            \"pipeline_signal\": \"\",\n",
    "            \"description_signal\": \"\",\n",
    "            \"comment_signal\": \"\",\n",
    "        }\n",
    "\n",
    "    prompt = PROMPT_TEMPLATE.format(\n",
    "    pipeline_names=\"\\n  - \" + \"\\n  - \".join(pipeline_names) if pipeline_names else \"  None\",\n",
    "    description=\"  \" + truncate(description).replace(\"\\n\", \"\\n  \") if description else \"  None\",\n",
    "    comments=\"  - \" + \"\\n  - \".join(truncate(\" | \".join(comments)).split(\" | \")) if comments else \"  None\",\n",
    "    )\n",
    "    \n",
    "    raw = run_llm(SYSTEM_PROMPT_TEMPLATE, prompt, type=\"gemini\")\n",
    "    parsed = extract_json(raw)\n",
    "\n",
    "    evidence_sources = parsed.get(\"evidence_sources\") or []\n",
    "    if isinstance(evidence_sources, (tuple, list)):\n",
    "        evidence_sources = list(evidence_sources)\n",
    "\n",
    "    return {\n",
    "        \"pr_id\": pr_id,\n",
    "        \"author_type\": author_type,\n",
    "        \"repo\": f\"{row.get('repo_owner')}/{row.get('repo_name')}\",\n",
    "        \"pr_number\": row.get(\"pr_number\"),\n",
    "        \"pr_title\": row.get(\"pr_title\"),\n",
    "        \"pipeline_names\": pipeline_names,\n",
    "        \"validation_present\": parsed.get(\"validation_present\"),\n",
    "        \"evidence_sources\": evidence_sources,\n",
    "        \"validation_type\": parsed.get(\"validation_type\"),\n",
    "        \"validation_description\": parsed.get(\"validation_description\"),\n",
    "        \"pipeline_signal\": parsed.get(\"pipeline_signal\"),\n",
    "        \"description_signal\": parsed.get(\"description_signal\"),\n",
    "        \"comment_signal\": parsed.get(\"comment_signal\"),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "178deb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr_ids_from_commits(prefix: str, limit: Optional[int] = None) -> Iterable[int]:\n",
    "    commits = pd.read_parquet(\n",
    "        DATASETS_DIR / f\"{prefix}_pr\" / f\"{prefix}_pr_commits.parquet\"\n",
    "    )\n",
    "    pr_ids = sorted(commits[\"pr_id\"].dropna().astype(int).unique().tolist())\n",
    "    return pr_ids if limit is None else pr_ids[:limit]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e873eead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 5 AI PRs and 5 human PRs (first 5 each).\n",
      "Processing AI PR 1/5: 2766896431\n",
      "Running GEMINI, model: gemini-3-pro-preview\n",
      "Processing AI PR 2/5: 2843312341\n",
      "Running GEMINI, model: gemini-3-pro-preview\n",
      "Processing AI PR 3/5: 2843334531\n",
      "Running GEMINI, model: gemini-3-pro-preview\n",
      "Processing AI PR 4/5: 2855302194\n",
      "Running GEMINI, model: gemini-3-pro-preview\n",
      "Processing AI PR 5/5: 2859989652\n",
      "Running GEMINI, model: gemini-3-pro-preview\n",
      "Processing human PR 1/5: 2260441374\n",
      "Running GEMINI, model: gemini-3-pro-preview\n",
      "Processing human PR 2/5: 2260678480\n",
      "Running GEMINI, model: gemini-3-pro-preview\n",
      "Processing human PR 3/5: 2269202548\n",
      "Running GEMINI, model: gemini-3-pro-preview\n",
      "Processing human PR 4/5: 2269709704\n",
      "Running GEMINI, model: gemini-3-pro-preview\n",
      "Processing human PR 5/5: 2277950711\n",
      "Running GEMINI, model: gemini-3-pro-preview\n",
      "Saved 10 rows to /Users/antoniozhong/Documents/dev/purdue/MSR2026/github_perf_patch_study/RQ3/rq3_validation_evidence.parquet\n"
     ]
    }
   ],
   "source": [
    "out_dir = PROJECT_ROOT / \"RQ3\"\n",
    "out_dir.mkdir(exist_ok=True, parents=True)\n",
    "out_path = out_dir / \"rq3_validation_evidence.parquet\"\n",
    "records = []\n",
    "ai_core = load_pr_core(\"ai\")\n",
    "human_core = load_pr_core(\"human\")\n",
    "\n",
    "ai_ids = list(pr_ids_from_commits(\"ai\", limit=5))\n",
    "human_ids = list(pr_ids_from_commits(\"human\", limit=5))\n",
    "print(f\"Processing {len(ai_ids)} AI PRs and {len(human_ids)} human PRs (first 5 each).\")\n",
    "\n",
    "for idx, pr_id in enumerate(ai_ids, 1):\n",
    "    print(f\"Processing AI PR {idx}/{len(ai_ids)}: {pr_id}\")\n",
    "    try:\n",
    "        records.append(analyze_pr(\"ai\", pr_id, \"ai_agent\", ai_core))\n",
    "    except Exception as exc:\n",
    "        records.append(\n",
    "                {\n",
    "                    \"pr_id\": pr_id,\n",
    "                    \"author_type\": \"ai_agent\",\n",
    "                    \"repo\": \"\",\n",
    "                    \"pr_number\": None,\n",
    "                    \"pr_title\": \"\",\n",
    "                    \"pipeline_names\": [],\n",
    "                    \"validation_present\": None,\n",
    "                    \"evidence_sources\": [],\n",
    "                    \"validation_type\": \"error\",\n",
    "                    \"validation_description\": f\"error: {exc}\",\n",
    "                    \"pipeline_signal\": \"\",\n",
    "                    \"description_signal\": \"\",\n",
    "                    \"comment_signal\": \"\",\n",
    "                }\n",
    "            )\n",
    "for idx, pr_id in enumerate(human_ids, 1):\n",
    "    print(f\"Processing human PR {idx}/{len(human_ids)}: {pr_id}\")\n",
    "    try:\n",
    "        records.append(analyze_pr(\"human\", pr_id, \"human\", human_core))\n",
    "    except Exception as exc:\n",
    "        records.append(\n",
    "                {\n",
    "                    \"pr_id\": pr_id,\n",
    "                    \"author_type\": \"human\",\n",
    "                    \"repo\": \"\",\n",
    "                    \"pr_number\": None,\n",
    "                    \"pr_title\": \"\",\n",
    "                    \"pipeline_names\": [],\n",
    "                    \"validation_present\": None,\n",
    "                    \"evidence_sources\": [],\n",
    "                    \"validation_type\": \"error\",\n",
    "                    \"validation_description\": f\"error: {exc}\",\n",
    "                    \"pipeline_signal\": \"\",\n",
    "                    \"description_signal\": \"\",\n",
    "                    \"comment_signal\": \"\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "df.to_parquet(out_path, index=False)\n",
    "print(f\"Saved {len(df)} rows to {out_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e12e20c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pr_id</th>\n",
       "      <th>author_type</th>\n",
       "      <th>repo</th>\n",
       "      <th>pr_number</th>\n",
       "      <th>pr_title</th>\n",
       "      <th>pipeline_names</th>\n",
       "      <th>validation_present</th>\n",
       "      <th>evidence_sources</th>\n",
       "      <th>validation_type</th>\n",
       "      <th>validation_description</th>\n",
       "      <th>pipeline_signal</th>\n",
       "      <th>description_signal</th>\n",
       "      <th>comment_signal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2766896431</td>\n",
       "      <td>ai_agent</td>\n",
       "      <td>onlook-dev/onlook</td>\n",
       "      <td>982</td>\n",
       "      <td>Replace motion library with Tailwind transitions in EditPanel</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>none</td>\n",
       "      <td>The PR description claims performance improvements by removing a dependency, but the testing section only confirms compilation, linting, and functional correctness of transitions without providing performance metrics or profiling.</td>\n",
       "      <td>None</td>\n",
       "      <td>Mentions performance improvement as a goal but testing is limited to build/lint and visual verification.</td>\n",
       "      <td>Bot comment only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2843312341</td>\n",
       "      <td>ai_agent</td>\n",
       "      <td>promptfoo/promptfoo</td>\n",
       "      <td>3046</td>\n",
       "      <td>perf: optimize cache and token handling</td>\n",
       "      <td>[CI, Validate PR Title]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>none</td>\n",
       "      <td>The PR describes performance optimizations and high-load issues but only provides evidence of unit, integration, and manual functional testing. No benchmarks or load tests are reported.</td>\n",
       "      <td>CI, Validate PR Title (generic)</td>\n",
       "      <td>Mentions performance optimizations but testing is limited to unit, integration, and manual functional checks</td>\n",
       "      <td>Bot interactions and unit test generation status</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2843334531</td>\n",
       "      <td>ai_agent</td>\n",
       "      <td>promptfoo/promptfoo</td>\n",
       "      <td>3047</td>\n",
       "      <td>perf: optimize cache and token handling</td>\n",
       "      <td>[CI, Validate PR Title]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>none</td>\n",
       "      <td>The PR description claims performance optimizations (cache, token counting) but only lists unit, integration, and manual functional testing; no benchmarks or load tests are provided to verify the performance improvements.</td>\n",
       "      <td>CI, Validate PR Title (no perf workflows)</td>\n",
       "      <td>Mentions performance optimizations but lists only unit/integration/manual functional tests</td>\n",
       "      <td>Bot interactions only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2855302194</td>\n",
       "      <td>ai_agent</td>\n",
       "      <td>pdfme/pdfme</td>\n",
       "      <td>711</td>\n",
       "      <td>Optimize Font Loading Performance in Tests</td>\n",
       "      <td>[Unit Testing]</td>\n",
       "      <td>True</td>\n",
       "      <td>[description]</td>\n",
       "      <td>benchmark</td>\n",
       "      <td>Description provides explicit 'Before' and 'After' timing comparisons for specific tests and the full suite (e.g., 10.7s to 4.2s).</td>\n",
       "      <td>Unit Testing</td>\n",
       "      <td>Performance Improvements section lists specific timings (Before/After) for tests.</td>\n",
       "      <td>Automated bot comments only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2859989652</td>\n",
       "      <td>ai_agent</td>\n",
       "      <td>wolfSSL/wolfssh</td>\n",
       "      <td>779</td>\n",
       "      <td>Update SFTP status callback to output once per second</td>\n",
       "      <td>[Cppcheck Test, Kyber Tests, OS Check Test, Single-thread Check Test, Windows Build Test, Zephyr tests, wolfSSH SCP Test, wolfSSHd Test]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>none</td>\n",
       "      <td>No validation evidence</td>\n",
       "      <td>Static analysis and functional tests (Cppcheck, wolfSSH)</td>\n",
       "      <td>Verified using cppcheck; mentions reducing status update frequency</td>\n",
       "      <td>User mentions fixing a bottleneck and requests performance difference, but no results are provided</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2260441374</td>\n",
       "      <td>human</td>\n",
       "      <td>OpenHFT/Chronicle-Core</td>\n",
       "      <td>684</td>\n",
       "      <td>StringUtils.equalsCaseIgnore optimisation fixes #683</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "      <td>[description, comments]</td>\n",
       "      <td>benchmark</td>\n",
       "      <td>The description mentions optimization and includes a before/after screenshot. Comments explicitly confirm that benchmarks were set up for best and worst cases (referencing JLBH) and results were good.</td>\n",
       "      <td>None</td>\n",
       "      <td>Mentions optimization and includes a before/after screenshot implying performance comparison.</td>\n",
       "      <td>Author states 'I have set up benchmarks for best and worst cases; both are looking good' and provides a screenshot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2260678480</td>\n",
       "      <td>human</td>\n",
       "      <td>OpenHFT/Chronicle-Wire</td>\n",
       "      <td>984</td>\n",
       "      <td>Tweak JsonWire benchmark</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>none</td>\n",
       "      <td>No validation evidence</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>SonarCloud quality gate only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2269202548</td>\n",
       "      <td>human</td>\n",
       "      <td>seasonedcc/remix-forms</td>\n",
       "      <td>272</td>\n",
       "      <td>Improve performMutation and formAction</td>\n",
       "      <td>[CI]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>none</td>\n",
       "      <td>No validation evidence</td>\n",
       "      <td>Generic CI pipeline</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2269709704</td>\n",
       "      <td>human</td>\n",
       "      <td>microsoft/typescript-go</td>\n",
       "      <td>218</td>\n",
       "      <td>Speed up, reduce memory usage of file reading</td>\n",
       "      <td>[CI, Code Scanning - Action]</td>\n",
       "      <td>True</td>\n",
       "      <td>[comments]</td>\n",
       "      <td>benchmark</td>\n",
       "      <td>Detailed Go benchmark comparisons (benchstat) provided in comments showing latency and allocation improvements for file reading operations after optimization adjustments.</td>\n",
       "      <td>CI, Code Scanning - Action</td>\n",
       "      <td>claims optimization for file reading and BOM checking</td>\n",
       "      <td>benchstat output comparing old vs new performance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2277950711</td>\n",
       "      <td>human</td>\n",
       "      <td>OpenHFT/Chronicle-Wire</td>\n",
       "      <td>985</td>\n",
       "      <td>Optimisation around append sep [WIP]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>none</td>\n",
       "      <td>No validation evidence</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>SonarCloud quality gate only</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        pr_id author_type                     repo  pr_number  \\\n",
       "0  2766896431    ai_agent        onlook-dev/onlook        982   \n",
       "1  2843312341    ai_agent      promptfoo/promptfoo       3046   \n",
       "2  2843334531    ai_agent      promptfoo/promptfoo       3047   \n",
       "3  2855302194    ai_agent              pdfme/pdfme        711   \n",
       "4  2859989652    ai_agent          wolfSSL/wolfssh        779   \n",
       "5  2260441374       human   OpenHFT/Chronicle-Core        684   \n",
       "6  2260678480       human   OpenHFT/Chronicle-Wire        984   \n",
       "7  2269202548       human   seasonedcc/remix-forms        272   \n",
       "8  2269709704       human  microsoft/typescript-go        218   \n",
       "9  2277950711       human   OpenHFT/Chronicle-Wire        985   \n",
       "\n",
       "                                                        pr_title  \\\n",
       "0  Replace motion library with Tailwind transitions in EditPanel   \n",
       "1                        perf: optimize cache and token handling   \n",
       "2                        perf: optimize cache and token handling   \n",
       "3                     Optimize Font Loading Performance in Tests   \n",
       "4          Update SFTP status callback to output once per second   \n",
       "5           StringUtils.equalsCaseIgnore optimisation fixes #683   \n",
       "6                                       Tweak JsonWire benchmark   \n",
       "7                         Improve performMutation and formAction   \n",
       "8                  Speed up, reduce memory usage of file reading   \n",
       "9                           Optimisation around append sep [WIP]   \n",
       "\n",
       "                                                                                                                             pipeline_names  \\\n",
       "0                                                                                                                                        []   \n",
       "1                                                                                                                   [CI, Validate PR Title]   \n",
       "2                                                                                                                   [CI, Validate PR Title]   \n",
       "3                                                                                                                            [Unit Testing]   \n",
       "4  [Cppcheck Test, Kyber Tests, OS Check Test, Single-thread Check Test, Windows Build Test, Zephyr tests, wolfSSH SCP Test, wolfSSHd Test]   \n",
       "5                                                                                                                                        []   \n",
       "6                                                                                                                                        []   \n",
       "7                                                                                                                                      [CI]   \n",
       "8                                                                                                              [CI, Code Scanning - Action]   \n",
       "9                                                                                                                                        []   \n",
       "\n",
       "   validation_present         evidence_sources validation_type  \\\n",
       "0               False                       []            none   \n",
       "1               False                       []            none   \n",
       "2               False                       []            none   \n",
       "3                True            [description]       benchmark   \n",
       "4               False                       []            none   \n",
       "5                True  [description, comments]       benchmark   \n",
       "6               False                       []            none   \n",
       "7               False                       []            none   \n",
       "8                True               [comments]       benchmark   \n",
       "9               False                       []            none   \n",
       "\n",
       "                                                                                                                                                                                                                   validation_description  \\\n",
       "0  The PR description claims performance improvements by removing a dependency, but the testing section only confirms compilation, linting, and functional correctness of transitions without providing performance metrics or profiling.   \n",
       "1                                               The PR describes performance optimizations and high-load issues but only provides evidence of unit, integration, and manual functional testing. No benchmarks or load tests are reported.   \n",
       "2           The PR description claims performance optimizations (cache, token counting) but only lists unit, integration, and manual functional testing; no benchmarks or load tests are provided to verify the performance improvements.   \n",
       "3                                                                                                      Description provides explicit 'Before' and 'After' timing comparisons for specific tests and the full suite (e.g., 10.7s to 4.2s).   \n",
       "4                                                                                                                                                                                                                  No validation evidence   \n",
       "5                                The description mentions optimization and includes a before/after screenshot. Comments explicitly confirm that benchmarks were set up for best and worst cases (referencing JLBH) and results were good.   \n",
       "6                                                                                                                                                                                                                  No validation evidence   \n",
       "7                                                                                                                                                                                                                  No validation evidence   \n",
       "8                                                              Detailed Go benchmark comparisons (benchstat) provided in comments showing latency and allocation improvements for file reading operations after optimization adjustments.   \n",
       "9                                                                                                                                                                                                                  No validation evidence   \n",
       "\n",
       "                                            pipeline_signal  \\\n",
       "0                                                      None   \n",
       "1                           CI, Validate PR Title (generic)   \n",
       "2                 CI, Validate PR Title (no perf workflows)   \n",
       "3                                              Unit Testing   \n",
       "4  Static analysis and functional tests (Cppcheck, wolfSSH)   \n",
       "5                                                      None   \n",
       "6                                                      None   \n",
       "7                                       Generic CI pipeline   \n",
       "8                                CI, Code Scanning - Action   \n",
       "9                                                      None   \n",
       "\n",
       "                                                                                             description_signal  \\\n",
       "0      Mentions performance improvement as a goal but testing is limited to build/lint and visual verification.   \n",
       "1  Mentions performance optimizations but testing is limited to unit, integration, and manual functional checks   \n",
       "2                    Mentions performance optimizations but lists only unit/integration/manual functional tests   \n",
       "3                             Performance Improvements section lists specific timings (Before/After) for tests.   \n",
       "4                                            Verified using cppcheck; mentions reducing status update frequency   \n",
       "5                 Mentions optimization and includes a before/after screenshot implying performance comparison.   \n",
       "6                                                                                                          None   \n",
       "7                                                                                                          None   \n",
       "8                                                         claims optimization for file reading and BOM checking   \n",
       "9                                                                                                          None   \n",
       "\n",
       "                                                                                                        comment_signal  \n",
       "0                                                                                                     Bot comment only  \n",
       "1                                                                     Bot interactions and unit test generation status  \n",
       "2                                                                                                Bot interactions only  \n",
       "3                                                                                          Automated bot comments only  \n",
       "4                   User mentions fixing a bottleneck and requests performance difference, but no results are provided  \n",
       "5  Author states 'I have set up benchmarks for best and worst cases; both are looking good' and provides a screenshot.  \n",
       "6                                                                                         SonarCloud quality gate only  \n",
       "7                                                                                                                 None  \n",
       "8                                                                    benchstat output comparing old vs new performance  \n",
       "9                                                                                         SonarCloud quality gate only  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.expand_frame_repr\", True)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "data_temp = pd.read_parquet(out_path)\n",
    "data_temp.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
