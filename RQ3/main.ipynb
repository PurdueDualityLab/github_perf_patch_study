{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e779bfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai google-genai pandas python-dotenv\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List, Optional\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600ce435",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"validation_present\": {\n",
    "            \"type\": \"boolean\"\n",
    "        },\n",
    "        \"evidence_sources\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": [\"pipeline\", \"description\", \"comments\"]\n",
    "            }\n",
    "        },\n",
    "        \"validation_type\": {\n",
    "            \"type\": \"string\",\n",
    "            \"enum\": [\n",
    "                \"benchmark\",\n",
    "                \"profiling\",\n",
    "                \"static-analysis\",\n",
    "                \"anecdotal\",\n",
    "                \"load/canary\",\n",
    "                \"none\"\n",
    "            ]\n",
    "        },\n",
    "        \"validation_description\": {\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        \"pipeline_signal\": {\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        \"description_signal\": {\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        \"comment_signal\": {\n",
    "            \"type\": \"string\"\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\n",
    "        \"validation_present\",\n",
    "        \"evidence_sources\",\n",
    "        \"validation_type\",\n",
    "        \"validation_description\",\n",
    "        \"pipeline_signal\",\n",
    "        \"description_signal\",\n",
    "        \"comment_signal\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "schema_openai = {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"validation_present\": {\"type\": \"boolean\"},\n",
    "            \"evidence_sources\": {\n",
    "                \"type\": \"array\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"pipeline\", \"description\", \"comments\"]\n",
    "                }\n",
    "            },\n",
    "            \"validation_type\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": [\n",
    "                    \"benchmark\",\n",
    "                    \"profiling\",\n",
    "                    \"static-analysis\",\n",
    "                    \"anecdotal\",\n",
    "                    \"load/canary\",\n",
    "                    \"none\"\n",
    "                ]\n",
    "            },\n",
    "            \"validation_description\": {\"type\": \"string\"},\n",
    "            \"pipeline_signal\": {\"type\": \"string\"},\n",
    "            \"description_signal\": {\"type\": \"string\"},\n",
    "            \"comment_signal\": {\"type\": \"string\"}\n",
    "        },\n",
    "        \"required\": [\n",
    "            \"validation_present\",\n",
    "            \"evidence_sources\",\n",
    "            \"validation_type\",\n",
    "            \"validation_description\",\n",
    "            \"pipeline_signal\",\n",
    "            \"description_signal\",\n",
    "            \"comment_signal\"\n",
    "        ],\n",
    "        \"additionalProperties\": False,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e600d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_llm(prompt: str,user_prompt: str, type: str = \"openai\", model: str = \"gpt-5.1-2025-11-13\") -> str:\n",
    "    \"\"\"Call local OLLAMA.\"\"\"\n",
    "    LLM_BASE_URL = None\n",
    "    LLM_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "    llm_client = OpenAI(base_url=LLM_BASE_URL, api_key=LLM_API_KEY)\n",
    "    \n",
    "    if(type == \"openai\"):\n",
    "        model = \"gpt-5.1-2025-11-13\"\n",
    "        print(\"Running OPENAI, model:\", model)\n",
    "  \n",
    "        messages = [\n",
    "            {\"role\": \"developer\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ]\n",
    "        \n",
    "\n",
    "        r = llm_client.chat.completions.create(\n",
    "            model=model,\n",
    "            temperature=0.0,\n",
    "            messages=messages,\n",
    "            response_format={\n",
    "                \"type\": \"json_schema\",\n",
    "                \"json_schema\": {\n",
    "                    \"name\": \"validation_schema\",\n",
    "                    \"schema\": schema_openai,\n",
    "                    \"strict\": True,\n",
    "                },\n",
    "            },\n",
    "        )\n",
    "\n",
    "        result = r.choices[0].message.content.strip()\n",
    "        return result\n",
    "    elif(type == \"gemini\"):\n",
    "        \n",
    "        model = \"models/gemini-pro-latest\"\n",
    "        print(\"Running GEMINI, model:\", model)\n",
    "        \n",
    "        GEMINI_API_KEY = os.environ.get(\"GEMINI_API_KEY\")\n",
    "        client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "\n",
    "        \n",
    "        config = types.GenerateContentConfig(\n",
    "            temperature=0.0,\n",
    "            response_mime_type=\"application/json\",\n",
    "            response_schema=schema,\n",
    "            # thinking_config=types.ThinkingConfig(\n",
    "            #     thinking_level=types.ThinkingLevel.HIGH\n",
    "            # )\n",
    "        )\n",
    "        \n",
    "        messages = [\n",
    "            prompt,\n",
    "            user_prompt,\n",
    "        ]\n",
    "        \n",
    "        response = client.models.generate_content(\n",
    "            model=model,\n",
    "            contents=messages,\n",
    "            config=config,\n",
    "        )\n",
    "        return response.text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c2b2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_datasets_dir(start: Optional[Path] = None) -> Path:\n",
    "    start = start or Path.cwd()\n",
    "    for path in (start, *start.parents):\n",
    "        candidate = path / \"datasets\"\n",
    "        if candidate.exists():\n",
    "            return candidate\n",
    "    raise FileNotFoundError(f\"Could not find 'datasets' directory from {start}\")\n",
    "\n",
    "\n",
    "DATASETS_DIR = find_datasets_dir()\n",
    "PROJECT_ROOT = DATASETS_DIR.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10afe7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json(text: str) -> Dict:\n",
    "    \"\"\"Best-effort JSON extraction from model output.\"\"\"\n",
    "    start = text.find(\"{\")\n",
    "    end = text.rfind(\"}\")\n",
    "    if start != -1 and end != -1 and end > start:\n",
    "        snippet = text[start : end + 1]\n",
    "        try:\n",
    "            return json.loads(snippet)\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    return {}\n",
    "\n",
    "def truncate(text: str, limit: int = 10000) -> str:\n",
    "    return text if len(text) <= limit else text[:limit] + \"...[truncated]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29784b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pr_core(prefix: str) -> pd.DataFrame:\n",
    "    commits = pd.read_parquet(\n",
    "        DATASETS_DIR / f\"{prefix}_pr\" / f\"{prefix}_pr_commits.parquet\"\n",
    "    )\n",
    "    return commits.drop_duplicates(\"pr_id\").set_index(\"pr_id\")\n",
    "\n",
    "\n",
    "def collect_comments(prefix: str, pr_id: int) -> List[str]:\n",
    "    issue = pd.read_parquet(\n",
    "        DATASETS_DIR / f\"{prefix}_pr\" / f\"{prefix}_pr_issue_comments.parquet\"\n",
    "    )\n",
    "    review = pd.read_parquet(\n",
    "        DATASETS_DIR / f\"{prefix}_pr\" / f\"{prefix}_pr_review_comments.parquet\"\n",
    "    )\n",
    "    texts = []\n",
    "    for df in (issue, review):\n",
    "        subset = df[df[\"pr_id\"] == pr_id]\n",
    "        texts.extend(subset[\"body\"].dropna().tolist())\n",
    "    return texts\n",
    "\n",
    "\n",
    "def collect_pipeline_names(prefix: str, pr_id: int) -> List[str]:\n",
    "    workflows = pd.read_parquet(\n",
    "        DATASETS_DIR / f\"{prefix}_pr\" / f\"{prefix}_pr_workflow_runs.parquet\"\n",
    "    )\n",
    "    subset = workflows[workflows[\"pr_id\"] == pr_id]\n",
    "    return sorted(subset[\"workflow_name\"].dropna().unique().tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d63ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_pr(\n",
    "    prefix: str, pr_id: int, author_type: str, pr_core: pd.DataFrame\n",
    ") -> tuple[Dict, Dict]:\n",
    "    row = pr_core.loc[pr_id]\n",
    "    pipeline_names = collect_pipeline_names(prefix, pr_id)\n",
    "    comments = collect_comments(prefix, pr_id)\n",
    "    description = (row.get(\"pr_description\") or \"\").strip()\n",
    "    code_diff = (row.get(\"patch\") or \"\").strip()\n",
    "    \n",
    "    DEVELOPER_PROMPT_TEMPLATE = \"\"\"\n",
    "    You are a classifier for performance validation evidence in GitHub Pull Requests (PRs).\n",
    "\n",
    "Your job is to decide whether there is explicit performance validation, and if so,\n",
    "to classify it into EXACTLY ONE validation_type category based on the definitions below.\n",
    "\n",
    "First, carefully read and internalize these validation_type categories:\n",
    "\n",
    "1. Benchmark-Based Validation (validation_type=\"benchmark\")\n",
    "Definition:\n",
    "The PR validates performance by running benchmark tests—either existing unit tests or newly\n",
    "added benchmark/microbenchmark tests. It includes explicit before-and-after comparisons such as\n",
    "runtime, throughput, memory, CPU usage, or any quantitative metric collected from tests.\n",
    "\n",
    "2. Profiling-Based Validation (validation_type=\"profiling\")\n",
    "Definition:\n",
    "The PR uses profiling tools to validate performance, typically capturing stack samples,\n",
    "CPU hotspots, flamegraphs, or function-level timings. Evidence includes profiling outputs\n",
    "before and after the change.\n",
    "\n",
    "3. Static-Analysis-Based Validation (validation_type=\"static-analysis\")\n",
    "Definition:\n",
    "The PR argues for performance improvement through static reasoning about the code—algorithmic\n",
    "complexity, data-structure changes, loop bounds, allocation count reduction, etc.—without\n",
    "providing runtime or profiling data.\n",
    "\n",
    "4. Anecdotal or Informal Local Testing (validation_type=\"anecdotal\")\n",
    "Definition:\n",
    "The PR claims that performance is improved based on local testing, intuition, or manual\n",
    "observation (for example, “this feels faster on my machine”, “latency looks better now”),\n",
    "but provides no quantitative metrics, no profiling output, and no detailed static-analysis\n",
    "justification.\n",
    "\n",
    "5. Load/Canary-Based Validation (validation_type=\"load/canary\")\n",
    "Definition:\n",
    "The PR validates performance by running load tests, stress tests, or canary/phased rollouts\n",
    "under real or synthetic traffic. Evidence includes references to load-testing tools,\n",
    "canary deployments, A/B tests, gradual rollout with monitoring, or production metrics under load.\n",
    "\n",
    "6. No Validation Evidence (validation_type=\"none\")\n",
    "Definition:\n",
    "The PR provides no explicit evidence of performance validation. It may mention optimization intent,\n",
    "but there are no benchmarks, no profiling, no static reasoning, no local-testing claims, and no\n",
    "load/canary validation described.\n",
    "\n",
    "Decision Rules:\n",
    "\n",
    "1) First decide if performance validation is explicitly present (validation_present).\n",
    "   - Set validation_present = TRUE only when the PR explicitly shows some form of validation evidence:\n",
    "     benchmarks/microbenchmarks, profiling traces, static performance reasoning, load/canary rollout,\n",
    "     or explicit local testing statements.\n",
    "   - Do NOT infer validation from performance intent alone.\n",
    "\n",
    "2) If validation_present = FALSE:\n",
    "   - Set validation_type = \"none\".\n",
    "   - Set evidence_sources = [].\n",
    "   - Set validation_description to explain the absence of validation.\n",
    "   - Never assign any other validation_type.\n",
    "\n",
    "3) If validation_present = TRUE:\n",
    "   - Choose exactly ONE non-\"none\" validation_type with this priority:\n",
    "     benchmark > profiling > load/canary > static-analysis > anecdotal\n",
    "\n",
    "4) evidence_sources must list where the validation is explicitly mentioned:\n",
    "   - \"pipeline\", \"description\", \"comments\"\n",
    "\n",
    "5) When validation_type is \"benchmark\", \"profiling\", or \"load/canary\",\n",
    "   mention the metrics used (latency, throughput, memory, CPU, etc.).\n",
    "\n",
    "You must ALWAYS return STRICT JSON with exactly these keys:\n",
    "\n",
    "validation_present, evidence_sources, validation_type,\n",
    "validation_description, pipeline_signal,\n",
    "description_signal, comment_signal.\n",
    "\n",
    "No extra commentary. No markdown.\n",
    "No explanations.\n",
    "    \"\"\"\n",
    "    \n",
    "    USER_PROMPT_TEMPLATE = \"\"\"\n",
    "    Classify the following PR strictly using the rules and definitions from the system.\n",
    "\n",
    "    PIPELINES:\n",
    "    {pipeline_names}\n",
    "\n",
    "    DESCRIPTION:\n",
    "    {description}\n",
    "\n",
    "    COMMENTS:\n",
    "    {comments}\n",
    "    \n",
    "    CODE DIFF:\n",
    "    {code_diff}\n",
    "    \"\"\"\n",
    "    \n",
    "   \n",
    "    empty_record = {\n",
    "        \"pr_id\": pr_id,\n",
    "        \"author_type\": author_type,\n",
    "        \"repo\": f\"{row.get('repo_owner')}/{row.get('repo_name')}\",\n",
    "        \"pr_number\": row.get(\"pr_number\"),\n",
    "        \"pr_title\": row.get(\"pr_title\"),\n",
    "        \"pipeline_names\": pipeline_names,\n",
    "        \"validation_present\": False,\n",
    "        \"evidence_sources\": [],\n",
    "        \"validation_type\": \"none\",\n",
    "        \"validation_description\": \"No validation evidence\",\n",
    "        \"pipeline_signal\": \"\",\n",
    "        \"description_signal\": \"\",\n",
    "        \"comment_signal\": \"\",\n",
    "    }\n",
    "\n",
    "    if not pipeline_names and not description and not comments:\n",
    "        return empty_record, empty_record\n",
    "\n",
    "    prompt = USER_PROMPT_TEMPLATE.format(\n",
    "        pipeline_names=\"- \" + \"- \".join(pipeline_names) if pipeline_names else \"None\",\n",
    "        description=truncate(description) if description else \"None\",\n",
    "        comments=\"- \" + \"- \".join(truncate(\" | \".join(comments)).split(\" | \")) if comments else \"None\",\n",
    "        code_diff=truncate(code_diff) if code_diff else \"None\",\n",
    "    )\n",
    "    \n",
    "    developer_prompt = DEVELOPER_PROMPT_TEMPLATE\n",
    "\n",
    "    raw_openai = \"\"\n",
    "    raw_gemini = \"\"\n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        futures = {\n",
    "            \"openai\": executor.submit(\n",
    "                run_llm, prompt=developer_prompt, user_prompt=prompt, type=\"openai\"\n",
    "            ),\n",
    "            \"gemini\": executor.submit(\n",
    "                run_llm, prompt=developer_prompt, user_prompt=prompt, type=\"gemini\"\n",
    "            ),\n",
    "        }\n",
    "        for name, fut in futures.items():\n",
    "            try:\n",
    "                result = fut.result()\n",
    "            except Exception as exc:\n",
    "                result = \"\"\n",
    "                print(f\"{name} model call failed for PR {pr_id}: {exc}\")\n",
    "            if name == \"openai\":\n",
    "                raw_openai = result\n",
    "            else:\n",
    "                raw_gemini = result\n",
    "\n",
    "    parsed_openai = extract_json(raw_openai) or {}\n",
    "    parsed_gemini = extract_json(raw_gemini) or {}\n",
    "\n",
    "    def build_result(parsed: Dict):\n",
    "        evidence_sources = parsed.get(\"evidence_sources\") or []\n",
    "        if isinstance(evidence_sources, (tuple, list)):\n",
    "            evidence_sources = list(evidence_sources)\n",
    "        \n",
    "        return {\n",
    "            \"pr_id\": pr_id,\n",
    "            \"author_type\": author_type,\n",
    "            \"repo\": f\"{row.get('repo_owner')}/{row.get('repo_name')}\",\n",
    "            \"pr_number\": row.get(\"pr_number\"),\n",
    "            \"pr_title\": row.get(\"pr_title\"),\n",
    "            \"pipeline_names\": pipeline_names,\n",
    "            \"validation_present\": parsed.get(\"validation_present\"),\n",
    "            \"evidence_sources\": evidence_sources,\n",
    "            \"validation_type\": parsed.get(\"validation_type\"),\n",
    "            \"validation_description\": parsed.get(\"validation_description\"),\n",
    "            \"pipeline_signal\": parsed.get(\"pipeline_signal\"),\n",
    "            \"description_signal\": parsed.get(\"description_signal\"),\n",
    "            \"comment_signal\": parsed.get(\"comment_signal\"),\n",
    "        }\n",
    "\n",
    "    return build_result(parsed_openai), build_result(parsed_gemini)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178deb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr_ids_from_commits(prefix: str, limit: Optional[int] = None) -> Iterable[int]:\n",
    "    commits = pd.read_parquet(\n",
    "        DATASETS_DIR / f\"{prefix}_pr\" / f\"{prefix}_pr_commits.parquet\"\n",
    "    )\n",
    "    pr_ids = sorted(commits[\"pr_id\"].dropna().astype(int).unique().tolist())\n",
    "    return pr_ids if limit is None else pr_ids[:limit]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8772b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = PROJECT_ROOT / \"RQ3\"\n",
    "out_dir.mkdir(exist_ok=True, parents=True)\n",
    "out_path_openai = out_dir / \"rq3_validation_evidence_openai.parquet\"\n",
    "out_path_gemini = out_dir / \"rq3_validation_evidence_gemini.parquet\"\n",
    "\n",
    "records_openai = []\n",
    "records_gemini = []\n",
    "ai_core = load_pr_core(\"ai\")\n",
    "human_core = load_pr_core(\"human\")\n",
    "\n",
    "limit = None\n",
    "\n",
    "def save_partial(records, out_path):\n",
    "    df_tmp = pd.DataFrame(records)\n",
    "    df_tmp.to_parquet(out_path, index=False)\n",
    "    print(f\"[partial save] Saved {len(df_tmp)} rows to {out_path}\")\n",
    "\n",
    "ai_ids = list(pr_ids_from_commits(\"ai\", limit=limit))\n",
    "human_ids = list(pr_ids_from_commits(\"human\", limit=limit))\n",
    "print(f\"Processing {len(ai_ids)} AI PRs and {len(human_ids)} human PRs (first {limit} each).\")\n",
    "\n",
    "# ============================\n",
    "# Process AI PRs\n",
    "# ============================\n",
    "for idx, pr_id in enumerate(ai_ids, 1):\n",
    "    print(f\"Processing AI PR {idx}/{len(ai_ids)}: {pr_id}\")\n",
    "    try:\n",
    "        openai, gemini = analyze_pr(\"ai\", pr_id, \"ai_agent\", ai_core)\n",
    "        records_openai.append(openai)\n",
    "        records_gemini.append(gemini)\n",
    "    except Exception as exc:\n",
    "        empty = {\n",
    "            \"pr_id\": pr_id,\n",
    "            \"author_type\": \"human\",\n",
    "            \"repo\": \"\",\n",
    "            \"pr_number\": None,\n",
    "            \"pr_title\": \"\",\n",
    "            \"pipeline_names\": [],\n",
    "            \"validation_present\": None,\n",
    "            \"evidence_sources\": [],\n",
    "            \"validation_type\": \"error\",\n",
    "            \"validation_description\": f\"error: {exc}\",\n",
    "            \"pipeline_signal\": \"\",\n",
    "            \"description_signal\": \"\",\n",
    "            \"comment_signal\": \"\",\n",
    "        }\n",
    "        records_openai.append(empty)\n",
    "        records_gemini.append(empty)\n",
    "\n",
    "    # ---- SAVE EVERY 10 ----\n",
    "    if len(records_openai) % 10 == 0:\n",
    "        save_partial(records_openai, out_path_openai)\n",
    "        save_partial(records_gemini, out_path_gemini)\n",
    "\n",
    "# ============================\n",
    "# Process Human PRs\n",
    "# ============================\n",
    "for idx, pr_id in enumerate(human_ids, 1):\n",
    "    print(f\"Processing human PR {idx}/{len(human_ids)}: {pr_id}\")\n",
    "    try:\n",
    "        openai, gemini = analyze_pr(\"human\", pr_id, \"human\", human_core)\n",
    "        records_openai.append(openai)\n",
    "        records_gemini.append(gemini)\n",
    "    except Exception as exc:\n",
    "        empty = {\n",
    "            \"pr_id\": pr_id,\n",
    "            \"author_type\": \"human\",\n",
    "            \"repo\": \"\",\n",
    "            \"pr_number\": None,\n",
    "            \"pr_title\": \"\",\n",
    "            \"pipeline_names\": [],\n",
    "            \"validation_present\": None,\n",
    "            \"evidence_sources\": [],\n",
    "            \"validation_type\": \"error\",\n",
    "            \"validation_description\": f\"error: {exc}\",\n",
    "            \"pipeline_signal\": \"\",\n",
    "            \"description_signal\": \"\",\n",
    "            \"comment_signal\": \"\",\n",
    "        }\n",
    "        records_openai.append(empty)\n",
    "        records_gemini.append(empty)\n",
    "\n",
    "    # ---- SAVE EVERY 20 ----\n",
    "    if len(records_openai) % 10 == 0:\n",
    "        save_partial(records_openai, out_path_openai)\n",
    "        save_partial(records_gemini, out_path_gemini)\n",
    "\n",
    "# ============================\n",
    "# Final save\n",
    "# ============================\n",
    "df_open_ai = pd.DataFrame(records_openai)\n",
    "df_gemini = pd.DataFrame(records_gemini)\n",
    "df_open_ai.to_parquet(out_path_openai, index=False)\n",
    "df_gemini.to_parquet(out_path_gemini, index=False)\n",
    "print(f\"Saved FINAL OPENAI {len(df_open_ai)} rows to {out_path_openai}\")\n",
    "print(f\"Saved FINAL GEMINI {len(df_gemini)} rows to {out_path_gemini}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12e20c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.expand_frame_repr\", True)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "data_temp = pd.read_parquet(out_path_openai)\n",
    "data_temp.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37f71bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.expand_frame_repr\", True)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "data_temp = pd.read_parquet(out_path_gemini)\n",
    "data_temp.tail(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
