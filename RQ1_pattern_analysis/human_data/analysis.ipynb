{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b488495a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's Kappa Agreement (Huiyun vs Ricardo final human labels)\n",
      "Identifier columns used: ['PR ID']\n",
      "Rows compared: 40\n",
      "  pattern: kappa=0.6581, match_rate=70.00%\n",
      "Rows compared: 26\n",
      "  sub_pattern: kappa=0.5759, match_rate=61.54%\n",
      "Cohen's Kappa Agreement (Huiyun vs Ricardo final agent labels)\n",
      "Identifier columns used: ['PR ID']\n",
      "Rows compared: 155\n",
      "  pattern: kappa=0.6010, match_rate=65.81%\n",
      "Rows compared: 101\n",
      "  sub_pattern: kappa=0.5988, match_rate=62.38%\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "FIELD_NAMES = ['Pattern', 'Sub Pattern']\n",
    "COLUMN_MAP = {\n",
    "    'Pattern': {'Huiyun': 'Huiyun_Pattern', 'Ricardo': 'Ricardo_Pattern'},\n",
    "    'Sub Pattern': {'Huiyun': 'Huiyun_Sub_Pattern', 'Ricardo': 'Ricardo_Sub_Pattern'},\n",
    "}\n",
    "DATASETS = [\n",
    "    {\n",
    "        'label': 'final human labels',\n",
    "        'file': Path('./Final_Human_PRs.csv'),\n",
    "    },\n",
    "    {\n",
    "        'label': 'final agent labels',\n",
    "        'file': Path('./Final_Agent_PRs.csv'),\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "def load_table(path):\n",
    "    with path.open(newline='', encoding='utf-8') as f:\n",
    "        return {row['PR ID']: row for row in csv.DictReader(f)}\n",
    "\n",
    "\n",
    "def collect_labels(table, field):\n",
    "    labels_h, labels_r = [], []\n",
    "    columns = COLUMN_MAP[field]\n",
    "    for row in table.values():\n",
    "        a = row.get(columns['Huiyun'], '').strip()\n",
    "        b = row.get(columns['Ricardo'], '').strip()\n",
    "        if a and b:\n",
    "            labels_h.append(a)\n",
    "            labels_r.append(b)\n",
    "    return labels_h, labels_r\n",
    "\n",
    "\n",
    "def compute_metrics(table, field):\n",
    "    labels_h, labels_r = collect_labels(table, field)\n",
    "    total = len(labels_h)\n",
    "    if not total:\n",
    "        return 0, 0.0, float('nan')\n",
    "    matches = sum(1 for a, b in zip(labels_h, labels_r) if a == b)\n",
    "    match_rate = matches / total\n",
    "    kappa = cohen_kappa_score(labels_h, labels_r)\n",
    "    return total, match_rate, kappa\n",
    "\n",
    "\n",
    "for dataset in DATASETS:\n",
    "    print(f\"Cohen's Kappa Agreement (Huiyun vs Ricardo {dataset['label']})\")\n",
    "    print(\"Identifier columns used: ['PR ID']\")\n",
    "    table = load_table(dataset['file'])\n",
    "    for field in FIELD_NAMES:\n",
    "        total, match_rate, kappa = compute_metrics(table, field)\n",
    "        print(f\"Rows compared: {total}\")\n",
    "        field_name = field.lower().replace(' ', '_')\n",
    "        print(f\"  {field_name}: kappa={kappa:.4f}, match_rate={match_rate * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "final_labels_round3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human final labels: 83 PRs (LLM agreement: 43, manual: 40, fallback: 0). Saved to Round3_Final_Human_PRs.csv.\n",
      "Agent final labels: 324 PRs (LLM agreement: 169, manual: 155, fallback: 0). Saved to Round3_Final_Agent_PRs.csv.\n",
      "Human sample rows:\n",
      "{'PR ID': '2260441374', 'PR URL': 'https://github.com/OpenHFT/Chronicle-Core/pull/684', 'Final Pattern': 'Control-Flow and Branching Optimizations', 'Final Sub Pattern': 'Rearranging for early return', 'Label Source': 'manual_review'}\n",
      "{'PR ID': '2260678480', 'PR URL': 'https://github.com/OpenHFT/Chronicle-Wire/pull/984', 'Final Pattern': 'I/O and Synchronization', 'Final Sub Pattern': 'Concurrency Control / Lock Optimization', 'Label Source': 'llm_agreement'}\n",
      "{'PR ID': '2269202548', 'PR URL': 'https://github.com/seasonedcc/remix-forms/pull/272', 'Final Pattern': 'Code Smells and Structural Simplification', 'Final Sub Pattern': 'Remove Unnecessary Method Calls', 'Label Source': 'manual_review'}\n",
      "Agent sample rows:\n",
      "{'PR ID': '2766896431', 'PR URL': 'https://github.com/onlook-dev/onlook/pull/982', 'Final Pattern': 'Build & Compilation & Infrastructure Optimization', 'Final Sub Pattern': 'Performance-Optimized Dependency Selection', 'Label Source': 'manual_review'}\n",
      "{'PR ID': '2843312341', 'PR URL': 'https://github.com/promptfoo/promptfoo/pull/3046', 'Final Pattern': 'Memory and Data Locality Optimizations', 'Final Sub Pattern': 'Caching', 'Label Source': 'manual_review'}\n",
      "{'PR ID': '2843334531', 'PR URL': 'https://github.com/promptfoo/promptfoo/pull/3047', 'Final Pattern': 'Memory and Data Locality Optimizations', 'Final Sub Pattern': 'Caching', 'Label Source': 'manual_review'}\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "csv.field_size_limit(1_000_000_000)\n",
    "\n",
    "ROUND3_ROOT = Path('../llm_data/new_catalog_round_3')\n",
    "DATASETS = [\n",
    "    {\n",
    "        'name': 'human',\n",
    "        'gpt': ROUND3_ROOT / 'human_data/human_perf_prs_with_gpt_analysis_new_full_catalog.csv',\n",
    "        'gemini': ROUND3_ROOT / 'human_data/human_perf_prs_with_gemini_analysis_new_full_catalog.csv',\n",
    "        'manual': Path('./Final_Human_PRs.csv'),\n",
    "        'output': Path('./Round3_Final_Human_PRs.csv'),\n",
    "    },\n",
    "    {\n",
    "        'name': 'agent',\n",
    "        'gpt': ROUND3_ROOT / 'agent_data/ai_perf_prs_with_gpt_analysis_new_full_catalog.csv',\n",
    "        'gemini': ROUND3_ROOT / 'agent_data/ai_perf_prs_with_gemini_analysis_new_full_catalog.csv',\n",
    "        'manual': Path('./Final_Agent_PRs.csv'),\n",
    "        'output': Path('./Round3_Final_Agent_PRs.csv'),\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "def normalize(value: str) -> str:\n",
    "    return value.strip() if value else ''\n",
    "\n",
    "\n",
    "def load_llm_rows(path: Path) -> dict:\n",
    "    rows = {}\n",
    "    with path.open(newline='', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            pr_id = row['id'].strip()\n",
    "            rows[pr_id] = {\n",
    "                'url': row.get('html_url', '').strip(),\n",
    "                'pattern': normalize(row.get('optimization_pattern', '')),\n",
    "                'subpattern': normalize(row.get('optimization_subpattern', '')),\n",
    "            }\n",
    "    return rows\n",
    "\n",
    "\n",
    "def load_manual_rows(path: Path) -> dict:\n",
    "    rows = {}\n",
    "    with path.open(newline='', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            pr_id = row['PR ID'].strip()\n",
    "            rows[pr_id] = {\n",
    "                'url': row.get('PR URL', '').strip(),\n",
    "                'pattern': normalize(row.get('Final_Pattern', '')),\n",
    "                'subpattern': normalize(row.get('Final_Sub_Pattern', '')),\n",
    "            }\n",
    "    return rows\n",
    "\n",
    "\n",
    "def write_output(path: Path, rows: list):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    fieldnames = ['PR ID', 'PR URL', 'Final Pattern', 'Final Sub Pattern', 'Label Source']\n",
    "    with path.open('w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(rows)\n",
    "\n",
    "\n",
    "final_label_tables = {}\n",
    "final_label_stats = {}\n",
    "\n",
    "for cfg in DATASETS:\n",
    "    gpt_rows = load_llm_rows(cfg['gpt'])\n",
    "    gemini_rows = load_llm_rows(cfg['gemini'])\n",
    "    manual_rows = load_manual_rows(cfg['manual'])\n",
    "\n",
    "    combined = []\n",
    "    stats = {'llm_agreement': 0, 'manual_override': 0, 'fallback': 0}\n",
    "    seen = set()\n",
    "\n",
    "    for pr_id, gpt_row in gpt_rows.items():\n",
    "        gemini_row = gemini_rows.get(pr_id, {})\n",
    "        if (\n",
    "            gpt_row['pattern'] == gemini_row.get('pattern', '')\n",
    "            and gpt_row['subpattern'] == gemini_row.get('subpattern', '')\n",
    "        ):\n",
    "            combined.append({\n",
    "                'PR ID': pr_id,\n",
    "                'PR URL': gpt_row['url'] or gemini_row.get('url', ''),\n",
    "                'Final Pattern': gpt_row['pattern'],\n",
    "                'Final Sub Pattern': gpt_row['subpattern'],\n",
    "                'Label Source': 'llm_agreement',\n",
    "            })\n",
    "            stats['llm_agreement'] += 1\n",
    "        elif pr_id in manual_rows:\n",
    "            manual_row = manual_rows[pr_id]\n",
    "            combined.append({\n",
    "                'PR ID': pr_id,\n",
    "                'PR URL': manual_row['url'] or gpt_row['url'] or gemini_row.get('url', ''),\n",
    "                'Final Pattern': manual_row['pattern'],\n",
    "                'Final Sub Pattern': manual_row['subpattern'],\n",
    "                'Label Source': 'manual_review',\n",
    "            })\n",
    "            stats['manual_override'] += 1\n",
    "        else:\n",
    "            combined.append({\n",
    "                'PR ID': pr_id,\n",
    "                'PR URL': gpt_row['url'] or gemini_row.get('url', ''),\n",
    "                'Final Pattern': gpt_row['pattern'],\n",
    "                'Final Sub Pattern': gpt_row['subpattern'],\n",
    "                'Label Source': 'llm_fallback',\n",
    "            })\n",
    "            stats['fallback'] += 1\n",
    "        seen.add(pr_id)\n",
    "\n",
    "    for pr_id, manual_row in manual_rows.items():\n",
    "        if pr_id not in seen:\n",
    "            combined.append({\n",
    "                'PR ID': pr_id,\n",
    "                'PR URL': manual_row['url'],\n",
    "                'Final Pattern': manual_row['pattern'],\n",
    "                'Final Sub Pattern': manual_row['subpattern'],\n",
    "                'Label Source': 'manual_only',\n",
    "            })\n",
    "            stats['manual_override'] += 1\n",
    "            seen.add(pr_id)\n",
    "\n",
    "    combined.sort(key=lambda row: row['PR ID'])\n",
    "    write_output(cfg['output'], combined)\n",
    "    final_label_tables[cfg['name']] = combined\n",
    "    final_label_stats[cfg['name']] = stats\n",
    "    print(\n",
    "        f\"{cfg['name'].title()} final labels: {len(combined)} PRs \"\n",
    "        f\"(LLM agreement: {stats['llm_agreement']}, manual: {stats['manual_override']}, \"\n",
    "        f\"fallback: {stats['fallback']}). Saved to {cfg['output'].name}.\"\n",
    "    )\n",
    "\n",
    "for name, rows in final_label_tables.items():\n",
    "    print(f\"{name.title()} sample rows:\")\n",
    "    for row in rows[:3]:\n",
    "        print(row)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
