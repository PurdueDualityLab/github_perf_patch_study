{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0486a43",
   "metadata": {},
   "source": [
    "## Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd880dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install pandas numpy matplotlib seaborn scipy wordcloud pyarrow datasets --quiet\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import re\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "# Plot settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Environment ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588b1d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compatibility shim: some versions of fsspec don't expose url_to_fs at top-level.\n",
    "# This ensures code that expects fsspec.url_to_fs (used by some IO backends) continues to work.\n",
    "try:\n",
    "    import fsspec\n",
    "    if not hasattr(fsspec, \"url_to_fs\"):\n",
    "        try:\n",
    "            from fsspec.core import url_to_fs as _url_to_fs\n",
    "        except Exception:\n",
    "            try:\n",
    "                import fsspec.core as _core\n",
    "                _url_to_fs = _core.url_to_fs\n",
    "            except Exception:\n",
    "                # Fallback shim: create a minimal url_to_fs that returns a filesystem and the path.\n",
    "                def _url_to_fs(url, **kwargs):\n",
    "                    protocol = url.split(\"://\")[0] if \"://\" in url else \"file\"\n",
    "                    fs = fsspec.filesystem(protocol)\n",
    "                    return fs, url\n",
    "        fsspec.url_to_fs = _url_to_fs\n",
    "except Exception:\n",
    "    # If anything goes wrong, continue without failing here; subsequent IO calls will raise their own errors.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bd5c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Load datasets\n",
    "print(\"Loading AIDev datasets...\")\n",
    "\n",
    "# AI Agent PRs\n",
    "pr_df = pd.read_parquet(\"hf://datasets/hao-li/AIDev/pull_request.parquet\")\n",
    "pr_task_type_df = pd.read_parquet(\"hf://datasets/hao-li/AIDev/pr_task_type.parquet\")\n",
    "ai_perf_prs = (\n",
    "    pr_df\n",
    "    .merge(\n",
    "        pr_task_type_df[[\"id\", \"type\", \"reason\"]],\n",
    "        on=\"id\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    .query(\"type == 'perf'\")\n",
    "    .copy()\n",
    ")\n",
    "ai_perf_prs['classification_reason'] = ai_perf_prs['reason']\n",
    "ai_perf_prs['author_type'] = 'AI Agent'\n",
    "\n",
    "# Human PRs\n",
    "human_pr_df = pd.read_parquet(\"hf://datasets/hao-li/AIDev/human_pull_request.parquet\")\n",
    "human_pr_task_type_df = pd.read_parquet(\"hf://datasets/hao-li/AIDev/human_pr_task_type.parquet\")\n",
    "human_perf_prs = (\n",
    "    human_pr_df\n",
    "    .merge(\n",
    "        human_pr_task_type_df[[\"id\", \"type\", \"reason\"]],\n",
    "        on=\"id\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    .query(\"type == 'perf'\")\n",
    "    .copy()\n",
    ")\n",
    "human_perf_prs['classification_reason'] = human_perf_prs['reason']\n",
    "human_perf_prs['author_type'] = 'Human'\n",
    "human_perf_prs['agent'] = 'Human'\n",
    "\n",
    "# Store original counts\n",
    "original_ai_count = len(ai_perf_prs)\n",
    "original_human_count = len(human_perf_prs)\n",
    "\n",
    "# Repository data for language info\n",
    "all_repo_df = pd.read_parquet(\"hf://datasets/hao-li/AIDev/all_repository.parquet\")\n",
    "\n",
    "# Get list of performance PR IDs we care about\n",
    "perf_pr_ids = set(ai_perf_prs['id'].tolist() + human_perf_prs['id'].tolist())\n",
    "print(f\"\\n✓ Performance PR IDs to process: {len(perf_pr_ids):,}\")\n",
    "\n",
    "# PR commits details - FILTER FIRST, then aggregate\n",
    "print(\"\\nProcessing commit details (filtering to performance PRs only)...\")\n",
    "pr_commits_details = pd.read_parquet(\"hf://datasets/hao-li/AIDev/pr_commit_details.parquet\")\n",
    "\n",
    "# Pr commit details for human PRs\n",
    "human_pr_commit_details = pd.read_parquet(\"./../../datasets/human_pr/human_pr_commit_details_original.parquet\")\n",
    "human_pr_commits = pd.read_parquet(\"./../../datasets/human_pr/human_pr_commits_original.parquet\")\n",
    "\n",
    "# Extract only the columns you need from the second table\n",
    "msg_df = human_pr_commits[[\"sha\", \"commit_message\"]]\n",
    "\n",
    "human_pr_commit_details = (\n",
    "    human_pr_commit_details\n",
    "        .merge(msg_df, on=\"sha\", how=\"left\")\n",
    ")\n",
    "\n",
    "human_pr_commit_details.rename(columns={\"commit_message\": \"message\"}, inplace=True)\n",
    "\n",
    "pr_commits_details = pd.concat(\n",
    "    [pr_commits_details, human_pr_commit_details],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "if 'pr_id' in pr_commits_details.columns:\n",
    "    print(f\"  Total commit records in dataset: {len(pr_commits_details):,}\")\n",
    "    \n",
    "    # FILTER: Keep only commits for performance PRs\n",
    "    pr_commits_filtered = pr_commits_details[pr_commits_details['pr_id'].isin(perf_pr_ids)].copy()\n",
    "    print(f\"  Filtered to performance PRs: {len(pr_commits_filtered):,} commit records\")\n",
    "    print(f\"  Unique performance PRs with commits: {pr_commits_filtered['pr_id'].nunique():,}\")\n",
    "    \n",
    "    # ADDITIONAL FILTERING: Remove commits with null filename\n",
    "    if 'filename' in pr_commits_filtered.columns:\n",
    "        before_filename_filter = len(pr_commits_filtered)\n",
    "        pr_commits_filtered = pr_commits_filtered[pr_commits_filtered['filename'].notna()].copy()\n",
    "        print(f\"  Filtered out null filenames: {before_filename_filter - len(pr_commits_filtered):,} records removed\")\n",
    "        print(f\"  Remaining after filename filter: {len(pr_commits_filtered):,} commit records\")\n",
    "        \n",
    "    # ADDITIONAL FILTERING: Remove config/metadata-only files\n",
    "    if 'filename' in pr_commits_filtered.columns:\n",
    "        before_config_filter = len(pr_commits_filtered)\n",
    "        \n",
    "        # Define patterns for non-code files to exclude\n",
    "        config_patterns = [\n",
    "            r'^\\.mvn/',                          # Maven wrapper configs\n",
    "            r'^\\.gradle/',                       # Gradle configs\n",
    "            r'^\\.idea/',                         # IntelliJ configs\n",
    "            r'^\\.vscode/',                       # VSCode configs\n",
    "            r'^\\.github/workflows/',             # GitHub Actions (unless it's code)\n",
    "            r'\\.properties$',                    # Properties files\n",
    "            r'\\.xml$',                           # XML config files (pom.xml, etc.)\n",
    "            r'\\.yml$',                           # YAML configs\n",
    "            r'\\.yaml$',                          # YAML configs\n",
    "            r'\\.json$',                          # JSON configs (package.json, etc.)\n",
    "            r'\\.md$',                            # Markdown docs\n",
    "            r'\\.txt$',                           # Text files\n",
    "            r'\\.gitignore$',                     # Git configs\n",
    "            r'\\.dockerignore$',                  # Docker ignore files\n",
    "            r'/Dockerfile$',                     # Dockerfiles (anywhere in path)\n",
    "            r'^Dockerfile$',                     # Dockerfile at root\n",
    "            r'/docker-compose',                  # Docker compose (anywhere)\n",
    "            r'^docker-compose',                  # Docker compose at root\n",
    "            r'\\.lock$',                          # Lock files (package-lock, yarn.lock)\n",
    "            r'^LICENSE',                         # License files\n",
    "            r'^README',                          # README files\n",
    "        ]\n",
    "        \n",
    "        config_pattern = '|'.join(config_patterns)\n",
    "        \n",
    "        # Mark config files\n",
    "        pr_commits_filtered['is_config_file'] = pr_commits_filtered['filename'].str.contains(\n",
    "            config_pattern, case=False, na=False, regex=True\n",
    "        )\n",
    "        \n",
    "        # Keep track of which files are code files per PR\n",
    "        pr_commits_filtered['is_code_file'] = ~pr_commits_filtered['is_config_file']\n",
    "        \n",
    "        # For each PR, check if it has ANY code files\n",
    "        pr_has_code = pr_commits_filtered.groupby('pr_id')['is_code_file'].any().reset_index()\n",
    "        pr_has_code.columns = ['pr_id', 'has_code_files']\n",
    "        \n",
    "        # Filter to keep only PRs that have at least one code file\n",
    "        pr_commits_filtered = pr_commits_filtered.merge(pr_has_code, on='pr_id', how='left')\n",
    "        pr_commits_filtered = pr_commits_filtered[pr_commits_filtered['has_code_files']].copy()\n",
    "        \n",
    "        # Clean up temporary columns\n",
    "        pr_commits_filtered = pr_commits_filtered.drop(columns=['is_config_file', 'is_code_file', 'has_code_files'])\n",
    "        \n",
    "        print(f\"  Filtered out config-only commits: {before_config_filter - len(pr_commits_filtered):,} records removed\")\n",
    "        print(f\"  Remaining after config filter: {len(pr_commits_filtered):,} commit records\")\n",
    "    \n",
    "    # ADDITIONAL FILTERING: Remove merge commits\n",
    "    if 'message' in pr_commits_filtered.columns:\n",
    "        before_merge_filter = len(pr_commits_filtered)\n",
    "        # Common merge commit patterns\n",
    "        merge_patterns = [\n",
    "            r'^Merge\\s+branch',\n",
    "            r'^Merge\\s+pull\\s+request',\n",
    "            r'^Merge\\s+remote-tracking\\s+branch',\n",
    "            r'^Merge\\s+.*\\s+into\\s+',\n",
    "            r\"^Merged\\s+in\\s+\",\n",
    "        ]\n",
    "        merge_pattern = '|'.join(merge_patterns)\n",
    "        pr_commits_filtered = pr_commits_filtered[\n",
    "            ~pr_commits_filtered['message'].str.match(merge_pattern, case=False, na=False)\n",
    "        ].copy()\n",
    "        print(f\"  Filtered out merge commits: {before_merge_filter - len(pr_commits_filtered):,} records removed\")\n",
    "        print(f\"  Remaining after merge filter: {len(pr_commits_filtered):,} commit records\")\n",
    "\n",
    "        # FINAL FILTER: drop PRs whose repositories were deleted\n",
    "        deleted_repo_pr_ids = {3271610326, 3209206554}\n",
    "        before_deleted_repo_filter = len(pr_commits_filtered)\n",
    "        pr_commits_filtered = pr_commits_filtered[~pr_commits_filtered['pr_id'].isin(deleted_repo_pr_ids)].copy()\n",
    "        removed_deleted_repo_commits = before_deleted_repo_filter - len(pr_commits_filtered)\n",
    "        if removed_deleted_repo_commits > 0:\n",
    "            print(f\"  Removed deleted repo PR commits: {removed_deleted_repo_commits:,} records removed\")\n",
    "        print(f\"  Remaining after deleted repo filter: {len(pr_commits_filtered):,} commit records\")\n",
    "    \n",
    "    print(f\"  Unique performance PRs after all filters: {pr_commits_filtered['pr_id'].nunique():,}\")\n",
    "    \n",
    "    if len(pr_commits_filtered) > 0:\n",
    "        # AGGREGATE: Now aggregate only the filtered commits\n",
    "        commit_aggregated = pr_commits_filtered.groupby('pr_id').agg({\n",
    "            'additions': 'sum',      # Total lines added across all commits\n",
    "            'deletions': 'sum',      # Total lines deleted across all commits\n",
    "            'patch': lambda x: '\\n\\n'.join([str(p) for p in x if pd.notna(p)])  # Concatenate all patches\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Add derived metrics\n",
    "        commit_aggregated['num_commits'] = pr_commits_filtered.groupby('pr_id').size().values\n",
    "        \n",
    "        # Calculate patch length (for analysis)\n",
    "        commit_aggregated['patch_length'] = commit_aggregated['patch'].str.len()\n",
    "        \n",
    "        print(f\"  ✓ Aggregated to {len(commit_aggregated):,} unique performance PRs\")\n",
    "        print(f\"  Avg commits per PR: {commit_aggregated['num_commits'].mean():.1f}\")\n",
    "        \n",
    "        # Merge commit stats into AI Agent PR table\n",
    "        ai_perf_prs = ai_perf_prs.merge(\n",
    "            commit_aggregated,\n",
    "            left_on='id',\n",
    "            right_on='pr_id',\n",
    "            how='left'\n",
    "        )\n",
    "        if 'pr_id' in ai_perf_prs.columns:\n",
    "            ai_perf_prs = ai_perf_prs.drop(columns=['pr_id'])\n",
    "        \n",
    "        # Filter to keep only PRs with commit data\n",
    "        ai_before_filter = len(ai_perf_prs)\n",
    "        ai_with_commits = ai_perf_prs[ai_perf_prs['additions'].notna()].copy()\n",
    "        print(f\"  AI Agent PRs with commit data: {len(ai_with_commits):,} / {ai_before_filter:,} ({len(ai_with_commits)/ai_before_filter*100:.1f}%)\")\n",
    "        \n",
    "        # Merge commit stats into Human PR table\n",
    "        human_perf_prs = human_perf_prs.merge(\n",
    "            commit_aggregated,\n",
    "            left_on='id',\n",
    "            right_on='pr_id',\n",
    "            how='left'\n",
    "        )\n",
    "        if 'pr_id' in human_perf_prs.columns:\n",
    "            human_perf_prs = human_perf_prs.drop(columns=['pr_id'])\n",
    "        \n",
    "        # Filter to keep only PRs with commit data\n",
    "        human_before_filter = len(human_perf_prs)\n",
    "        human_with_commits = human_perf_prs[human_perf_prs['additions'].notna()].copy()\n",
    "        print(f\"  Human PRs with commit data: {len(human_with_commits):,} / {human_before_filter:,} ({len(human_with_commits)/human_before_filter*100:.1f}%)\")\n",
    "    else:\n",
    "        print(\"  ⚠ No commits found for performance PRs after filtering\")\n",
    "        # Create empty dataframes with same structure\n",
    "        ai_with_commits = ai_perf_prs.iloc[0:0].copy()\n",
    "        human_with_commits = human_perf_prs.iloc[0:0].copy()\n",
    "    \n",
    "else:\n",
    "    print('⚠ pr_commit_details missing pr_id column; skipping commit merges.')\n",
    "    # Create empty dataframes\n",
    "    ai_with_commits = ai_perf_prs.iloc[0:0].copy()\n",
    "    human_with_commits = human_perf_prs.iloc[0:0].copy()\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Original Performance PRs:\")\n",
    "print(f\"  AI Agent: {original_ai_count:,}\")\n",
    "print(f\"  Human: {original_human_count:,}\")\n",
    "print(f\"  Total: {original_ai_count + original_human_count:,}\")\n",
    "print(f\"\\nAfter Commit Filtering:\")\n",
    "print(f\"✓ AI Agent Performance PRs: {len(ai_with_commits):,}\")\n",
    "print(f\"✓ Human Performance PRs: {len(human_with_commits):,}\")\n",
    "print(f\"✓ Total Performance PRs: {len(ai_with_commits) + len(human_with_commits):,}\")\n",
    "\n",
    "# Distribution by AI agent\n",
    "if len(ai_with_commits) > 0:\n",
    "    print(f\"\\nAI Agent Distribution:\")\n",
    "    for agent, count in ai_with_commits['agent'].value_counts().items():\n",
    "        pct = count / len(ai_with_commits) * 100\n",
    "        print(f\"  {agent:20s} {count:5,d} ({pct:5.1f}%)\")\n",
    "\n",
    "# Commit statistics summary\n",
    "if len(ai_with_commits) > 0 or len(human_with_commits) > 0:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"COMMIT STATISTICS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    for author_type, df in [('AI Agent', ai_with_commits), ('Human', human_with_commits)]:\n",
    "        if len(df) > 0:\n",
    "            print(f\"\\n{author_type}:\")\n",
    "            print(f\"  PRs with commit data: {len(df):,}\")\n",
    "            print(f\"  Avg commits per PR: {df['num_commits'].mean():.1f}\")\n",
    "            print(f\"  Median commits per PR: {df['num_commits'].median():.1f}\")\n",
    "            print(f\"  Avg additions: {df['additions'].mean():.0f} lines\")\n",
    "            print(f\"  Median additions: {df['additions'].median():.0f} lines\")\n",
    "            print(f\"  Avg deletions: {df['deletions'].mean():.0f} lines\")\n",
    "            print(f\"  Median deletions: {df['deletions'].median():.0f} lines\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b0ab2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine AI and Human PRs\n",
    "perf_prs = pd.concat([ai_with_commits, human_with_commits], ignore_index=True)\n",
    "\n",
    "print(f\"Combined dataset: {len(perf_prs):,} performance PRs\")\n",
    "print(f\"  AI Agents: {(perf_prs['author_type'] == 'AI Agent').sum():,}\")\n",
    "print(f\"  Humans: {(perf_prs['author_type'] == 'Human').sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c76e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ai_todo_ids = pd.read_csv('./llm_data/final_data/ai_round3_todo.csv')\n",
    "# human_todo_ids = pd.read_csv('./llm_data/final_data/human_round3_todo.csv')\n",
    "\n",
    "# ai_id_list = ai_todo_ids['id'].tolist()\n",
    "# human_id_list = human_todo_ids['id'].tolist()\n",
    "\n",
    "# # Filter ai_with_commits to keep only rows where pr_id is in the list\n",
    "# ai_with_commits_filtered = ai_with_commits[ai_with_commits['id'].isin(ai_id_list)]\n",
    "\n",
    "# # Filter human_with_commits to keep only rows where pr_id is in the list\n",
    "# human_with_commits_filtered = human_with_commits[human_with_commits['id'].isin(human_id_list)]\n",
    "\n",
    "# # Combine AI and Human PRs\n",
    "# perf_prs = pd.concat([ai_with_commits_filtered, human_with_commits_filtered], ignore_index=True)\n",
    "\n",
    "# print(f\"Combined dataset: {len(perf_prs):,} performance PRs\")\n",
    "# print(f\"  AI Agents: {(perf_prs['author_type'] == 'AI Agent').sum():,}\")\n",
    "# print(f\"  Humans: {(perf_prs['author_type'] == 'Human').sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5d5acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Performance Optimization Pattern Detection with Gemini 3-Pro\n",
    "# ============================================================================\n",
    "!pip install google-genai python-dotenv pydantic --quiet\n",
    "\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from pydantic import BaseModel, Field\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Define the structured output schema\n",
    "class AnalysisResult(BaseModel):\n",
    "    explanation: str = Field(description=\"Brief description of what the code is doing\")\n",
    "    optimization_comparison: str = Field(description=\"Detailed comparison highlighting specific optimizations\")\n",
    "    high_level_pattern: str = Field(description=\"Single most representative high-level optimization pattern\")\n",
    "    sub_pattern: str = Field(description=\"Most representative sub-pattern within the category\")\n",
    "\n",
    "def analyze_optimization_with_gemini(title, body, patch):\n",
    "    \"\"\"\n",
    "    Call Gemini to analyze performance optimization patterns in a commit.\n",
    "    \n",
    "    Parameters:\n",
    "    - title: PR/commit title\n",
    "    - body: PR/commit description\n",
    "    - patch: Git diff/patch content\n",
    "    \n",
    "    Returns:\n",
    "    - dict with analysis results or error info\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prepare the context\n",
    "    context_parts = []\n",
    "    \n",
    "    if pd.notna(title) and str(title).strip():\n",
    "        context_parts.append(f\"**Title**: {title}\")\n",
    "    \n",
    "    if pd.notna(body) and str(body).strip():\n",
    "        context_parts.append(f\"**Description**: {body}\")\n",
    "    \n",
    "    if pd.notna(patch) and str(patch).strip():\n",
    "        # Truncate very long patches to avoid token limits\n",
    "        patch_str = str(patch)\n",
    "        if len(patch_str) > 15000:  # Rough character limit\n",
    "            patch_str = patch_str[:15000] + \"\\n\\n... [patch truncated for length] ...\"\n",
    "        context_parts.append(f\"**Code Changes (Patch)**:\\n```diff\\n{patch_str}\\n```\")\n",
    "    \n",
    "    if not context_parts:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": \"No content available\",\n",
    "            \"explanation\": None,\n",
    "            \"optimization_comparison\": None,\n",
    "            \"high_level_pattern\": None,\n",
    "            \"sub_pattern\": None,\n",
    "            \"tokens_used\": 0\n",
    "        }\n",
    "    \n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    try:\n",
    "        load_dotenv(override=True)\n",
    "    except Exception:\n",
    "        # dotenv not installed / .env not loaded; rely on environment variables\n",
    "        pass\n",
    "\n",
    "    api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise RuntimeError(\"GEMINI_API_KEY not found in environment. Add it to your .env or export it.\")\n",
    "\n",
    "    # Initialize the client\n",
    "    client = genai.Client(api_key=api_key)\n",
    "    \n",
    "    # Construct the prompt\n",
    "    # Load the optimization patterns taxonomy from CSV\n",
    "    def load_optimization_taxonomy(csv_path):\n",
    "        \"\"\"Load and format the optimization patterns taxonomy from CSV.\"\"\"\n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Format the taxonomy as a structured string\n",
    "        taxonomy_text = \"### Optimization Patterns Taxonomy:\\n\\n\"\n",
    "        \n",
    "        # Group by high-level pattern\n",
    "        for high_level in df['High-level Pattern'].unique():\n",
    "            taxonomy_text += f\"- **{high_level}**\\n\"\n",
    "            \n",
    "            # Get all sub-patterns for this high-level pattern\n",
    "            sub_patterns = df[df['High-level Pattern'] == high_level]\n",
    "            \n",
    "            for _, row in sub_patterns.iterrows():\n",
    "                taxonomy_text += f\"    - {row['Sub pattern']}\\n\"\n",
    "                if pd.notna(row['Description']):\n",
    "                    taxonomy_text += f\"        - Description: {row['Description']}\\n\"\n",
    "                if pd.notna(row['Example']):\n",
    "                    taxonomy_text += f\"        - Example: {row['Example']}\\n\"\n",
    "                if pd.notna(row['Optimized Metrics']):\n",
    "                    taxonomy_text += f\"        - Metrics: {row['Optimized Metrics']}\\n\"\n",
    "                if pd.notna(row['Detection']):\n",
    "                    taxonomy_text += f\"        - Detection: {row['Detection']}\\n\"\n",
    "        \n",
    "        return taxonomy_text\n",
    "\n",
    "    # Load taxonomy (adjust path as needed)\n",
    "    taxonomy = load_optimization_taxonomy('./catalog/updated_optimization_catalog.csv')\n",
    "\n",
    "    # Construct the prompt\n",
    "    prompt = f\"\"\"I have a performance optimization commit with the following information. Please analyze with the following goals:\n",
    "\n",
    "    1. **Code Function Explanation**: Briefly explain what the code is doing—what problem it solves and how it works.\n",
    "\n",
    "    2. **Optimization Comparison**: Compare the original and optimized versions to identify:\n",
    "    - **Algorithmic changes**: Any differences in logic, algorithm design, or problem-solving approach.\n",
    "    - **Performance improvements**: Enhancements related to time complexity, space efficiency, or runtime behavior.\n",
    "    - **Redundant code removal**: Elimination of unnecessary logic, method calls, or control structures.\n",
    "    - **Other noteworthy changes**: Any structural or stylistic differences that could impact performance or readability.\n",
    "    \n",
    "    3. **Optimization Pattern Classification**:\n",
    "    Based on the overall nature of the optimized code, assign the following:\n",
    "    - **Exactly one high-level optimization pattern** from the list below  \n",
    "    - **One most representative sub-pattern** within that high-level category\n",
    "    \n",
    "    {taxonomy}\n",
    "            \n",
    "    Here are the info:\n",
    "                \n",
    "    {context}\n",
    "\n",
    "    **Output Structure**:  \n",
    "    Please respond in JSON format with the following structure:\n",
    "    {{\n",
    "    \"explanation\": \"Brief description of what the code is doing\",\n",
    "    \"optimization_comparison\": \"Detailed comparison highlighting specific optimizations\",\n",
    "    \"high_level_pattern\": \"Single most representative high-level optimization pattern\",\n",
    "    \"sub_pattern\": \"Most representative sub-pattern within high_level_pattern\",\n",
    "    }}\n",
    "\n",
    "    Ensure your response is valid JSON that can be parsed.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Generate response with structured output\n",
    "        response = client.models.generate_content(\n",
    "            model=\"gemini-3-pro-preview\",\n",
    "            contents=prompt,\n",
    "            config={\n",
    "                \"temperature\": 0,\n",
    "                \"response_mime_type\": \"application/json\",\n",
    "                \"response_json_schema\": AnalysisResult.model_json_schema(),\n",
    "                \"system_instruction\": \"You are an expert software engineer specializing in performance optimization analysis. Analyze code changes and classify optimization patterns accurately.\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Parse the response using Pydantic\n",
    "        result = AnalysisResult.model_validate_json(response.text)\n",
    "        \n",
    "        # Get token usage (if available)\n",
    "        tokens_used = 0\n",
    "        if hasattr(response, 'usage_metadata') and response.usage_metadata:\n",
    "            tokens_used = getattr(response.usage_metadata, 'total_token_count', 0)\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"explanation\": result.explanation,\n",
    "            \"optimization_comparison\": result.optimization_comparison,\n",
    "            \"high_level_pattern\": result.high_level_pattern,\n",
    "            \"sub_pattern\": result.sub_pattern,\n",
    "            \"tokens_used\": tokens_used,\n",
    "            \"error\": None\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e),\n",
    "            \"explanation\": None,\n",
    "            \"optimization_comparison\": None,\n",
    "            \"high_level_pattern\": None,\n",
    "            \"sub_pattern\": None,\n",
    "            \"tokens_used\": 0\n",
    "        }\n",
    "\n",
    "\n",
    "def batch_analyze_performance_prs(perf_prs, batch_size=10, delay=1.0, resume=False, checkpoint_prefix='perf_prs_checkpoint', output_file='perf_prs_with_gemini_analysis.csv'):\n",
    "    \"\"\"\n",
    "    Analyze all performance PRs in batches using Gemini.\n",
    "\n",
    "    Parameters:\n",
    "    - perf_prs: DataFrame with performance PRs\n",
    "    - batch_size: Number of PRs to process before saving checkpoint\n",
    "    - delay: Delay between API calls in seconds\n",
    "    - resume: Continue from the last available checkpoint if True\n",
    "    - checkpoint_prefix: Filename prefix used for checkpoint files\n",
    "    - output_file: Final CSV filename for the aggregated results\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with analysis results added\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Starting Gemini analysis of {len(perf_prs):,} performance PRs...\")\n",
    "\n",
    "    checkpoint_files = []\n",
    "    processed_count = 0\n",
    "\n",
    "    if resume:\n",
    "        checkpoint_files = sorted(Path('.').glob(f\"{checkpoint_prefix}_*.csv\"))\n",
    "        if checkpoint_files:\n",
    "            def _processed_from_path(path_obj):\n",
    "                suffix = path_obj.stem.rsplit('_', 1)[-1]\n",
    "                return int(suffix) if suffix.isdigit() else 0\n",
    "\n",
    "            latest_checkpoint = max(checkpoint_files, key=_processed_from_path)\n",
    "            checkpoint_progress = _processed_from_path(latest_checkpoint)\n",
    "            perf_prs = pd.read_csv(latest_checkpoint)\n",
    "            processed_count = min(checkpoint_progress, len(perf_prs))\n",
    "            print(f\"↻ Resuming from checkpoint {latest_checkpoint} ({processed_count} PRs processed)...\")\n",
    "        else:\n",
    "            print(\"↻ Resume requested but no checkpoint found. Starting from scratch.\")\n",
    "\n",
    "    result_defaults = {\n",
    "        'gemini_explanation': None,\n",
    "        'gemini_comparison': None,\n",
    "        'optimization_pattern': None,\n",
    "        'optimization_subpattern': None,\n",
    "        'gemini_success': False,\n",
    "        'gemini_error': None,\n",
    "        'gemini_tokens': 0\n",
    "    }\n",
    "\n",
    "    for column, default in result_defaults.items():\n",
    "        if resume and column in perf_prs.columns:\n",
    "            continue\n",
    "        perf_prs[column] = default\n",
    "\n",
    "    start_idx = processed_count if resume else 0\n",
    "    iterator = range(start_idx, len(perf_prs))\n",
    "    progress_bar = tqdm(iterator, total=len(perf_prs), desc=\"Analyzing PRs\", initial=start_idx)\n",
    "\n",
    "    for idx in progress_bar:\n",
    "        row = perf_prs.iloc[idx]\n",
    "        result = analyze_optimization_with_gemini(\n",
    "            title=row.get('title'),\n",
    "            body=row.get('body'),\n",
    "            patch=row.get('patch')\n",
    "        )\n",
    "\n",
    "        perf_prs.at[idx, 'gemini_success'] = result['success']\n",
    "        perf_prs.at[idx, 'gemini_tokens'] = result['tokens_used']\n",
    "\n",
    "        if result['success']:\n",
    "            perf_prs.at[idx, 'gemini_explanation'] = result['explanation']\n",
    "            perf_prs.at[idx, 'gemini_comparison'] = result['optimization_comparison']\n",
    "            perf_prs.at[idx, 'optimization_pattern'] = result['high_level_pattern']\n",
    "            perf_prs.at[idx, 'optimization_subpattern'] = result['sub_pattern']\n",
    "            perf_prs.at[idx, 'gemini_error'] = None\n",
    "        else:\n",
    "            perf_prs.at[idx, 'gemini_error'] = result['error']\n",
    "\n",
    "        time.sleep(delay)\n",
    "\n",
    "        if (idx + 1) % batch_size == 0:\n",
    "            checkpoint_file = f\"{checkpoint_prefix}_{idx+1}.csv\"\n",
    "            perf_prs.to_csv(checkpoint_file, index=False)\n",
    "            print(f\"✓ Checkpoint saved: {checkpoint_file}\")\n",
    "\n",
    "    perf_prs.to_csv(output_file, index=False)\n",
    "    print(f\"✓ Analysis complete! Saved to: {output_file}\")\n",
    "\n",
    "    success_series = perf_prs['gemini_success'].fillna(False)\n",
    "    success_count = success_series.sum()\n",
    "    success_rate = (success_count / len(perf_prs) * 100) if len(perf_prs) else 0\n",
    "    failure_count = success_series.eq(False).sum()\n",
    "    total_tokens = perf_prs['gemini_tokens'].sum()\n",
    "\n",
    "    print(f\"{'='*80}\")\n",
    "    print(\"ANALYSIS SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total PRs analyzed: {len(perf_prs):,}\")\n",
    "    print(f\"Successful: {success_count:,} ({success_rate:.1f}%)\")\n",
    "    print(f\"Failed: {failure_count:,}\")\n",
    "    print(f\"Total tokens used: {total_tokens:,}\")\n",
    "\n",
    "    if success_count > 0:\n",
    "        print(f\"{'='*80}\")\n",
    "        print(\"OPTIMIZATION PATTERN DISTRIBUTION\")\n",
    "        print(f\"{'='*80}\")\n",
    "        pattern_counts = perf_prs[perf_prs['gemini_success'] == True]['optimization_pattern'].value_counts()\n",
    "        for pattern, count in pattern_counts.items():\n",
    "            pct = count / success_count * 100\n",
    "            print(f\"  {pattern:50s} {count:4d} ({pct:5.1f}%)\")\n",
    "\n",
    "    return perf_prs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532b365f",
   "metadata": {},
   "source": [
    "## Usage scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107368e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Usage\n",
    "# ============================================================================\n",
    "\n",
    "# run ai and human pr analysis separately\n",
    "\n",
    "# ai pr analysis\n",
    "ai_sample = perf_prs[perf_prs['author_type'] == 'AI Agent'].copy().reset_index(drop=True)\n",
    "print(f\"Testing Gemini analysis on {len(ai_sample)} AI PRs\")\n",
    "\n",
    "# Run the analysis\n",
    "perf_prs_analyzed = batch_analyze_performance_prs(\n",
    "    ai_sample,\n",
    "    batch_size=2,    # Save checkpoint every 10 PRs\n",
    "    delay=0.5,        # 0.5 second delay between API calls\n",
    "    resume=True,      # Continue from the last saved checkpoint if available\n",
    "    checkpoint_prefix='gemini_ai_perf_prs_checkpoint',\n",
    "    output_file='ai_perf_prs_with_gemini_analysis_new_catalog.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e170d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the AI analysis results\n",
    "import pandas as pd\n",
    "ai_analyzed = pd.read_csv('ai_perf_prs_with_gemini_analysis_new_full_catalog.csv')\n",
    "\n",
    "# Find failed analyses\n",
    "failed_analyses = ai_analyzed[ai_analyzed['gemini_success'] == False]\n",
    "print(f\"Found {len(failed_analyses)} failed analyses\")\n",
    "print(failed_analyses[['id', 'title', 'gemini_error']].head(10))\n",
    "\n",
    "# Re-analyze failed PRs\n",
    "if len(failed_analyses) > 0:\n",
    "    print(f\"\\nRe-analyzing {len(failed_analyses)} failed PRs...\")\n",
    "    \n",
    "    for idx in failed_analyses.index:\n",
    "        row = ai_analyzed.iloc[idx]\n",
    "        print(f\"Re-analyzing PR {idx}: {row['title'][:50]}...\")\n",
    "        \n",
    "        result = analyze_optimization_with_gemini(\n",
    "            title=row.get('title'),\n",
    "            body=row.get('body'),\n",
    "            patch=row.get('patch')\n",
    "        )\n",
    "        \n",
    "        ai_analyzed.at[idx, 'gemini_success'] = result['success']\n",
    "        ai_analyzed.at[idx, 'gemini_tokens'] = result['tokens_used']\n",
    "        \n",
    "        if result['success']:\n",
    "            ai_analyzed.at[idx, 'gemini_explanation'] = result['explanation']\n",
    "            ai_analyzed.at[idx, 'gemini_comparison'] = result['optimization_comparison']\n",
    "            ai_analyzed.at[idx, 'optimization_pattern'] = result['high_level_pattern']\n",
    "            ai_analyzed.at[idx, 'optimization_subpattern'] = result['sub_pattern']\n",
    "            ai_analyzed.at[idx, 'gemini_error'] = None\n",
    "        else:\n",
    "            ai_analyzed.at[idx, 'gemini_error'] = result['error']\n",
    "        \n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    # Save updated results\n",
    "    ai_analyzed.to_csv('ai_perf_prs_with_gemini_analysis_updated.csv', index=False)\n",
    "    print(\"✓ Updated results saved!\")\n",
    "    \n",
    "    # Show summary\n",
    "    success_count = (ai_analyzed['gemini_success'] == True).sum()\n",
    "    print(f\"\\nFinal success rate: {success_count}/{len(ai_analyzed)} ({success_count/len(ai_analyzed)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ba3fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# human pr analysis\n",
    "human_sample = perf_prs[perf_prs['author_type'] == 'Human'].copy().reset_index(drop=True)\n",
    "\n",
    "# Run the analysis\n",
    "perf_prs_analyzed = batch_analyze_performance_prs(\n",
    "    human_sample,\n",
    "    batch_size=2,    # Save checkpoint every 10 PRs\n",
    "    delay=0.5,        # 0.5 second delay between API calls\n",
    "    resume=True,      # Continue from the last saved checkpoint if available\n",
    "    checkpoint_prefix='human_perf_prs_checkpoint_gemini',\n",
    "    output_file='human_perf_prs_with_gemini_analysis_new_full_catalog.csv'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc1b105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and check the human PRs analysis file\n",
    "human_analyzed = pd.read_csv('human_perf_prs_with_gemini_analysis.csv')\n",
    "\n",
    "print(f\"Total entries in human_perf_prs_with_gemini_analysis.csv: {len(human_analyzed):,}\")\n",
    "print(f\"Successful analyses: {(human_analyzed['gemini_success'] == True).sum():,}\")\n",
    "print(f\"Failed analyses: {(human_analyzed['gemini_success'] == False).sum():,}\")\n",
    "print(f\"Success rate: {(human_analyzed['gemini_success'] == True).sum() / len(human_analyzed) * 100:.1f}%\")\n",
    "\n",
    "# Remove failed analyses and rewrite to the original file\n",
    "failed_analyses = human_analyzed[human_analyzed['gemini_success'] == False]\n",
    "print(f\"Removing {len(failed_analyses)} failed analyses...\")\n",
    "\n",
    "human_analyzed = human_analyzed[human_analyzed['gemini_success'] == True].copy()\n",
    "human_analyzed.to_csv('human_perf_prs_with_gemini_analysis.csv', index=False)\n",
    "\n",
    "print(f\"Updated file saved with {len(human_analyzed)} successful analyses\")\n",
    "print(f\"Success rate: {len(human_analyzed) / (len(failed_analyses) + len(human_analyzed)) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291d666d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze results by author type\n",
    "print(\"=\"*80)\n",
    "print(\"PATTERN COMPARISON: AI AGENTS VS HUMANS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for author_type in ['AI Agent', 'Human']:\n",
    "    subset = perf_prs_analyzed[\n",
    "        (perf_prs_analyzed['author_type'] == author_type) & \n",
    "        (perf_prs_analyzed['gemini_success'] == True)\n",
    "    ]\n",
    "    \n",
    "    if len(subset) > 0:\n",
    "        print(f\"{author_type} (n={len(subset):,}):\")\n",
    "        pattern_dist = subset['optimization_pattern'].value_counts().head(5)\n",
    "        for pattern, count in pattern_dist.items():\n",
    "            pct = count / len(subset) * 100\n",
    "            print(f\"  {pattern:50s} {count:4d} ({pct:5.1f}%)\")\n",
    "\n",
    "# Compare sub-patterns\n",
    "print(\"=\"*80)\n",
    "print(\"TOP SUB-PATTERNS BY AUTHOR TYPE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for author_type in ['AI Agent', 'Human']:\n",
    "    subset = perf_prs_analyzed[\n",
    "        (perf_prs_analyzed['author_type'] == author_type) & \n",
    "        (perf_prs_analyzed['gemini_success'] == True)\n",
    "    ]\n",
    "    \n",
    "    if len(subset) > 0:\n",
    "        print(f\"{author_type}:\")\n",
    "        subpattern_dist = subset['optimization_subpattern'].value_counts().head(5)\n",
    "        for subpattern, count in subpattern_dist.items():\n",
    "            pct = count / len(subset) * 100\n",
    "            print(f\"  {subpattern:50s} {count:4d} ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef32eb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rerun-empty-patterns",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerun Gemini for human PRs missing high-level patterns\n",
    "human_analysis_file = 'human_perf_prs_with_gemini_analysis_new_full_catalog.csv'\n",
    "human_results = pd.read_csv(human_analysis_file)\n",
    "\n",
    "missing_pattern_ids = human_results[human_results['optimization_pattern'].fillna('').str.strip() == '']['id'].tolist()\n",
    "\n",
    "print(f'Found {len(missing_pattern_ids)} human PRs with empty Gemini patterns.')\n",
    "\n",
    "if 'human_sample' not in locals():\n",
    "    human_sample = perf_prs[perf_prs['author_type'] == 'Human'].copy().reset_index(drop=True)\n",
    "\n",
    "if missing_pattern_ids:\n",
    "    retry_sample = human_sample[human_sample['id'].isin(missing_pattern_ids)].copy().reset_index(drop=True)\n",
    "    print(f'Retrying Gemini analysis for {len(retry_sample)} PRs...')\n",
    "    retry_results = batch_analyze_performance_prs(\n",
    "        retry_sample,\n",
    "        batch_size=2,\n",
    "        delay=0.5,\n",
    "        resume=False,\n",
    "        checkpoint_prefix='human_perf_prs_retry_empty_patterns',\n",
    "        output_file='human_perf_prs_with_gemini_analysis_retry_empty_patterns.csv'\n",
    "    )\n",
    "else:\n",
    "    print('No reruns needed – all human PRs have a high-level pattern.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38a2852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerun Gemini for AI PRs missing high-level patterns\n",
    "ai_analysis_file = 'ai_perf_prs_with_gemini_analysis_new_catalog.csv'\n",
    "ai_results = pd.read_csv(ai_analysis_file)\n",
    "\n",
    "missing_pattern_ids = ai_results[\n",
    "    ai_results['optimization_pattern'].fillna('').str.strip() == ''\n",
    "]['id'].tolist()\n",
    "\n",
    "print(f'Found {len(missing_pattern_ids)} AI PRs with empty Gemini patterns.')\n",
    "\n",
    "if 'ai_sample' not in locals():\n",
    "    ai_sample = perf_prs[perf_prs['author_type'] == 'AI Agent'].copy().reset_index(drop=True)\n",
    "\n",
    "if missing_pattern_ids:\n",
    "    retry_sample = ai_sample[ai_sample['id'].isin(missing_pattern_ids)].copy().reset_index(drop=True)\n",
    "    print(f'Retrying Gemini analysis for {len(retry_sample)} PRs...')\n",
    "    retry_results = batch_analyze_performance_prs(\n",
    "        retry_sample,\n",
    "        batch_size=2,\n",
    "        delay=0.5,\n",
    "        resume=False,\n",
    "        checkpoint_prefix='ai_perf_prs_retry_empty_patterns',\n",
    "        output_file='ai_perf_prs_with_gemini_analysis_retry_empty_patterns.csv'\n",
    "    )\n",
    "else:\n",
    "    print('No reruns needed – all AI PRs have a high-level pattern.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde98c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate taxonomy alignment for the generated Human CSV\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "human_csv_path = Path('ai_perf_prs_with_gemini_analysis_new_full_catalog.csv')\n",
    "taxonomy_path = Path('./catalog/updated_optimization_catalog.csv')\n",
    "\n",
    "if not human_csv_path.exists():\n",
    "    raise FileNotFoundError(f\"Human CSV not found: {human_csv_path}\")\n",
    "\n",
    "if not taxonomy_path.exists():\n",
    "    raise FileNotFoundError(f\"Taxonomy CSV not found: {taxonomy_path}\")\n",
    "\n",
    "human_results = pd.read_csv(human_csv_path)\n",
    "taxonomy_df = pd.read_csv(taxonomy_path)\n",
    "\n",
    "subpattern_to_pattern = (\n",
    "    taxonomy_df\n",
    "    .dropna(subset=['Sub pattern'])\n",
    "    .drop_duplicates(subset=['Sub pattern'])\n",
    "    .set_index('Sub pattern')['High-level Pattern']\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "def _normalize(cell_value):\n",
    "    if pd.isna(cell_value):\n",
    "        return None\n",
    "    text = str(cell_value).strip()\n",
    "    if not text or text.lower() in {'none', 'null', 'nan'}:\n",
    "        return None\n",
    "    return text\n",
    "\n",
    "validation_df = human_results.copy()\n",
    "validation_df['optimization_pattern_norm'] = validation_df['optimization_pattern'].map(_normalize)\n",
    "validation_df['optimization_subpattern_norm'] = validation_df['optimization_subpattern'].map(_normalize)\n",
    "\n",
    "checked = validation_df.dropna(subset=['optimization_pattern_norm', 'optimization_subpattern_norm']).copy()\n",
    "checked['expected_pattern'] = checked['optimization_subpattern_norm'].map(subpattern_to_pattern)\n",
    "\n",
    "missing_taxonomy = checked[checked['expected_pattern'].isna()]\n",
    "pattern_mismatches = checked[\n",
    "    checked['expected_pattern'].notna() &\n",
    "    (checked['expected_pattern'] != checked['optimization_pattern_norm'])\n",
    "]\n",
    "\n",
    "print(f\"Rows checked: {len(checked):,} / {len(human_results):,}\")\n",
    "print(f\"Missing taxonomy entries: {len(missing_taxonomy):,}\")\n",
    "print(f\"Pattern mismatches: {len(pattern_mismatches):,}\")\n",
    "\n",
    "if missing_taxonomy.empty and pattern_mismatches.empty:\n",
    "    print('✅ Human CSV is valid: every subpattern maps to the expected high-level pattern.')\n",
    "else:\n",
    "    if not missing_taxonomy.empty:\n",
    "        display(missing_taxonomy[['id', 'optimization_pattern_norm', 'optimization_subpattern_norm']].head())\n",
    "    if not pattern_mismatches.empty:\n",
    "        display(pattern_mismatches[['id', 'optimization_pattern_norm', 'optimization_subpattern_norm', 'expected_pattern']].head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
