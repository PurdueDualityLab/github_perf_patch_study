{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0486a43",
   "metadata": {},
   "source": [
    "## Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd880dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Environment ready!\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "!pip install pandas numpy matplotlib seaborn scipy wordcloud pyarrow datasets --quiet\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import re\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "# Plot settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Environment ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "588b1d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compatibility shim: some versions of fsspec don't expose url_to_fs at top-level.\n",
    "# This ensures code that expects fsspec.url_to_fs (used by some IO backends) continues to work.\n",
    "try:\n",
    "    import fsspec\n",
    "    if not hasattr(fsspec, \"url_to_fs\"):\n",
    "        try:\n",
    "            from fsspec.core import url_to_fs as _url_to_fs\n",
    "        except Exception:\n",
    "            try:\n",
    "                import fsspec.core as _core\n",
    "                _url_to_fs = _core.url_to_fs\n",
    "            except Exception:\n",
    "                # Fallback shim: create a minimal url_to_fs that returns a filesystem and the path.\n",
    "                def _url_to_fs(url, **kwargs):\n",
    "                    protocol = url.split(\"://\")[0] if \"://\" in url else \"file\"\n",
    "                    fs = fsspec.filesystem(protocol)\n",
    "                    return fs, url\n",
    "        fsspec.url_to_fs = _url_to_fs\n",
    "except Exception:\n",
    "    # If anything goes wrong, continue without failing here; subsequent IO calls will raise their own errors.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06bd5c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading AIDev datasets...\n",
      "\n",
      "✓ Performance PR IDs to process: 428\n",
      "\n",
      "Processing commit details (filtering to performance PRs only)...\n",
      "  Total commit records in dataset: 719,797\n",
      "  Filtered to performance PRs: 15,284 commit records\n",
      "  Unique performance PRs with commits: 427\n",
      "  Filtered out null filenames: 46 records removed\n",
      "  Remaining after filename filter: 15,238 commit records\n",
      "  Filtered out config-only commits: 44 records removed\n",
      "  Remaining after config filter: 15,194 commit records\n",
      "  Filtered out merge commits: 2,945 records removed\n",
      "  Remaining after merge filter: 12,249 commit records\n",
      "  Unique performance PRs after all filters: 409\n",
      "  ✓ Aggregated to 409 unique performance PRs\n",
      "  Avg commits per PR: 29.9\n",
      "  AI Agent PRs with commit data: 326 / 340 (95.9%)\n",
      "  Human PRs with commit data: 83 / 88 (94.3%)\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "Original Performance PRs:\n",
      "  AI Agent: 340\n",
      "  Human: 88\n",
      "  Total: 428\n",
      "\n",
      "After Commit Filtering:\n",
      "✓ AI Agent Performance PRs: 326\n",
      "✓ Human Performance PRs: 83\n",
      "✓ Total Performance PRs: 409\n",
      "\n",
      "AI Agent Distribution:\n",
      "  OpenAI_Codex           205 ( 62.9%)\n",
      "  Devin                   59 ( 18.1%)\n",
      "  Copilot                 37 ( 11.3%)\n",
      "  Cursor                  22 (  6.7%)\n",
      "  Claude_Code              3 (  0.9%)\n",
      "\n",
      "================================================================================\n",
      "COMMIT STATISTICS\n",
      "================================================================================\n",
      "\n",
      "AI Agent:\n",
      "  PRs with commit data: 326\n",
      "  Avg commits per PR: 13.4\n",
      "  Median commits per PR: 4.0\n",
      "  Avg additions: 373 lines\n",
      "  Median additions: 61 lines\n",
      "  Avg deletions: 283 lines\n",
      "  Median deletions: 25 lines\n",
      "\n",
      "Human:\n",
      "  PRs with commit data: 83\n",
      "  Avg commits per PR: 94.8\n",
      "  Median commits per PR: 4.0\n",
      "  Avg additions: 2315 lines\n",
      "  Median additions: 58 lines\n",
      "  Avg deletions: 959 lines\n",
      "  Median deletions: 28 lines\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "print(\"Loading AIDev datasets...\")\n",
    "\n",
    "# AI Agent PRs\n",
    "pr_df = pd.read_parquet(\"hf://datasets/hao-li/AIDev/pull_request.parquet\")\n",
    "pr_task_type_df = pd.read_parquet(\"hf://datasets/hao-li/AIDev/pr_task_type.parquet\")\n",
    "ai_perf_prs = (\n",
    "    pr_df\n",
    "    .merge(\n",
    "        pr_task_type_df[[\"id\", \"type\", \"reason\"]],\n",
    "        on=\"id\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    .query(\"type == 'perf'\")\n",
    "    .copy()\n",
    ")\n",
    "ai_perf_prs['classification_reason'] = ai_perf_prs['reason']\n",
    "ai_perf_prs['author_type'] = 'AI Agent'\n",
    "\n",
    "# Human PRs\n",
    "human_pr_df = pd.read_parquet(\"hf://datasets/hao-li/AIDev/human_pull_request.parquet\")\n",
    "human_pr_task_type_df = pd.read_parquet(\"hf://datasets/hao-li/AIDev/human_pr_task_type.parquet\")\n",
    "human_perf_prs = (\n",
    "    human_pr_df\n",
    "    .merge(\n",
    "        human_pr_task_type_df[[\"id\", \"type\", \"reason\"]],\n",
    "        on=\"id\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    .query(\"type == 'perf'\")\n",
    "    .copy()\n",
    ")\n",
    "human_perf_prs['classification_reason'] = human_perf_prs['reason']\n",
    "human_perf_prs['author_type'] = 'Human'\n",
    "human_perf_prs['agent'] = 'Human'\n",
    "\n",
    "# Store original counts\n",
    "original_ai_count = len(ai_perf_prs)\n",
    "original_human_count = len(human_perf_prs)\n",
    "\n",
    "# Repository data for language info\n",
    "all_repo_df = pd.read_parquet(\"hf://datasets/hao-li/AIDev/all_repository.parquet\")\n",
    "\n",
    "# Get list of performance PR IDs we care about\n",
    "perf_pr_ids = set(ai_perf_prs['id'].tolist() + human_perf_prs['id'].tolist())\n",
    "print(f\"\\n✓ Performance PR IDs to process: {len(perf_pr_ids):,}\")\n",
    "\n",
    "# PR commits details - FILTER FIRST, then aggregate\n",
    "print(\"\\nProcessing commit details (filtering to performance PRs only)...\")\n",
    "pr_commits_details = pd.read_parquet(\"hf://datasets/hao-li/AIDev/pr_commit_details.parquet\")\n",
    "\n",
    "# Pr commit details for human PRs\n",
    "human_pr_commit_details = pd.read_parquet(\"../.././datasets/human_pr/human_pr_commit_details.parquet\")\n",
    "\n",
    "\n",
    "pr_commits_details = pd.concat(\n",
    "    [pr_commits_details, human_pr_commit_details],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "if 'pr_id' in pr_commits_details.columns:\n",
    "    print(f\"  Total commit records in dataset: {len(pr_commits_details):,}\")\n",
    "    \n",
    "    # FILTER: Keep only commits for performance PRs\n",
    "    pr_commits_filtered = pr_commits_details[pr_commits_details['pr_id'].isin(perf_pr_ids)].copy()\n",
    "    print(f\"  Filtered to performance PRs: {len(pr_commits_filtered):,} commit records\")\n",
    "    print(f\"  Unique performance PRs with commits: {pr_commits_filtered['pr_id'].nunique():,}\")\n",
    "    \n",
    "    # ADDITIONAL FILTERING: Remove commits with null filename\n",
    "    if 'filename' in pr_commits_filtered.columns:\n",
    "        before_filename_filter = len(pr_commits_filtered)\n",
    "        pr_commits_filtered = pr_commits_filtered[pr_commits_filtered['filename'].notna()].copy()\n",
    "        print(f\"  Filtered out null filenames: {before_filename_filter - len(pr_commits_filtered):,} records removed\")\n",
    "        print(f\"  Remaining after filename filter: {len(pr_commits_filtered):,} commit records\")\n",
    "        \n",
    "    # ADDITIONAL FILTERING: Remove config/metadata-only files\n",
    "    if 'filename' in pr_commits_filtered.columns:\n",
    "        before_config_filter = len(pr_commits_filtered)\n",
    "        \n",
    "        # Define patterns for non-code files to exclude\n",
    "        config_patterns = [\n",
    "            r'^\\.mvn/',                          # Maven wrapper configs\n",
    "            r'^\\.gradle/',                       # Gradle configs\n",
    "            r'^\\.idea/',                         # IntelliJ configs\n",
    "            r'^\\.vscode/',                       # VSCode configs\n",
    "            r'^\\.github/workflows/',             # GitHub Actions (unless it's code)\n",
    "            r'\\.properties$',                    # Properties files\n",
    "            r'\\.xml$',                           # XML config files (pom.xml, etc.)\n",
    "            r'\\.yml$',                           # YAML configs\n",
    "            r'\\.yaml$',                          # YAML configs\n",
    "            r'\\.json$',                          # JSON configs (package.json, etc.)\n",
    "            r'\\.md$',                            # Markdown docs\n",
    "            r'\\.txt$',                           # Text files\n",
    "            r'\\.gitignore$',                     # Git configs\n",
    "            r'\\.dockerignore$',                  # Docker ignore files\n",
    "            r'/Dockerfile$',                     # Dockerfiles (anywhere in path)\n",
    "            r'^Dockerfile$',                     # Dockerfile at root\n",
    "            r'/docker-compose',                  # Docker compose (anywhere)\n",
    "            r'^docker-compose',                  # Docker compose at root\n",
    "            r'\\.lock$',                          # Lock files (package-lock, yarn.lock)\n",
    "            r'^LICENSE',                         # License files\n",
    "            r'^README',                          # README files\n",
    "        ]\n",
    "        \n",
    "        config_pattern = '|'.join(config_patterns)\n",
    "        \n",
    "        # Mark config files\n",
    "        pr_commits_filtered['is_config_file'] = pr_commits_filtered['filename'].str.contains(\n",
    "            config_pattern, case=False, na=False, regex=True\n",
    "        )\n",
    "        \n",
    "        # Keep track of which files are code files per PR\n",
    "        pr_commits_filtered['is_code_file'] = ~pr_commits_filtered['is_config_file']\n",
    "        \n",
    "        # For each PR, check if it has ANY code files\n",
    "        pr_has_code = pr_commits_filtered.groupby('pr_id')['is_code_file'].any().reset_index()\n",
    "        pr_has_code.columns = ['pr_id', 'has_code_files']\n",
    "        \n",
    "        # Filter to keep only PRs that have at least one code file\n",
    "        pr_commits_filtered = pr_commits_filtered.merge(pr_has_code, on='pr_id', how='left')\n",
    "        pr_commits_filtered = pr_commits_filtered[pr_commits_filtered['has_code_files']].copy()\n",
    "        \n",
    "        # Clean up temporary columns\n",
    "        pr_commits_filtered = pr_commits_filtered.drop(columns=['is_config_file', 'is_code_file', 'has_code_files'])\n",
    "        \n",
    "        print(f\"  Filtered out config-only commits: {before_config_filter - len(pr_commits_filtered):,} records removed\")\n",
    "        print(f\"  Remaining after config filter: {len(pr_commits_filtered):,} commit records\")\n",
    "    \n",
    "    # ADDITIONAL FILTERING: Remove merge commits\n",
    "    if 'message' in pr_commits_filtered.columns:\n",
    "        before_merge_filter = len(pr_commits_filtered)\n",
    "        # Common merge commit patterns\n",
    "        merge_patterns = [\n",
    "            r'^Merge\\s+branch',\n",
    "            r'^Merge\\s+pull\\s+request',\n",
    "            r'^Merge\\s+remote-tracking\\s+branch',\n",
    "            r'^Merge\\s+.*\\s+into\\s+',\n",
    "            r\"^Merged\\s+in\\s+\",\n",
    "        ]\n",
    "        merge_pattern = '|'.join(merge_patterns)\n",
    "        pr_commits_filtered = pr_commits_filtered[\n",
    "            ~pr_commits_filtered['message'].str.match(merge_pattern, case=False, na=False)\n",
    "        ].copy()\n",
    "        print(f\"  Filtered out merge commits: {before_merge_filter - len(pr_commits_filtered):,} records removed\")\n",
    "        print(f\"  Remaining after merge filter: {len(pr_commits_filtered):,} commit records\")\n",
    "    \n",
    "    print(f\"  Unique performance PRs after all filters: {pr_commits_filtered['pr_id'].nunique():,}\")\n",
    "    \n",
    "    if len(pr_commits_filtered) > 0:\n",
    "        # AGGREGATE: Now aggregate only the filtered commits\n",
    "        commit_aggregated = pr_commits_filtered.groupby('pr_id').agg({\n",
    "            'additions': 'sum',      # Total lines added across all commits\n",
    "            'deletions': 'sum',      # Total lines deleted across all commits\n",
    "            'patch': lambda x: '\\n\\n'.join([str(p) for p in x if pd.notna(p)])  # Concatenate all patches\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Add derived metrics\n",
    "        commit_aggregated['num_commits'] = pr_commits_filtered.groupby('pr_id').size().values\n",
    "        \n",
    "        # Calculate patch length (for analysis)\n",
    "        commit_aggregated['patch_length'] = commit_aggregated['patch'].str.len()\n",
    "        \n",
    "        print(f\"  ✓ Aggregated to {len(commit_aggregated):,} unique performance PRs\")\n",
    "        print(f\"  Avg commits per PR: {commit_aggregated['num_commits'].mean():.1f}\")\n",
    "        \n",
    "        # Merge commit stats into AI Agent PR table\n",
    "        ai_perf_prs = ai_perf_prs.merge(\n",
    "            commit_aggregated,\n",
    "            left_on='id',\n",
    "            right_on='pr_id',\n",
    "            how='left'\n",
    "        )\n",
    "        if 'pr_id' in ai_perf_prs.columns:\n",
    "            ai_perf_prs = ai_perf_prs.drop(columns=['pr_id'])\n",
    "        \n",
    "        # Filter to keep only PRs with commit data\n",
    "        ai_before_filter = len(ai_perf_prs)\n",
    "        ai_with_commits = ai_perf_prs[ai_perf_prs['additions'].notna()].copy()\n",
    "        print(f\"  AI Agent PRs with commit data: {len(ai_with_commits):,} / {ai_before_filter:,} ({len(ai_with_commits)/ai_before_filter*100:.1f}%)\")\n",
    "        \n",
    "        # Merge commit stats into Human PR table\n",
    "        human_perf_prs = human_perf_prs.merge(\n",
    "            commit_aggregated,\n",
    "            left_on='id',\n",
    "            right_on='pr_id',\n",
    "            how='left'\n",
    "        )\n",
    "        if 'pr_id' in human_perf_prs.columns:\n",
    "            human_perf_prs = human_perf_prs.drop(columns=['pr_id'])\n",
    "        \n",
    "        # Filter to keep only PRs with commit data\n",
    "        human_before_filter = len(human_perf_prs)\n",
    "        human_with_commits = human_perf_prs[human_perf_prs['additions'].notna()].copy()\n",
    "        print(f\"  Human PRs with commit data: {len(human_with_commits):,} / {human_before_filter:,} ({len(human_with_commits)/human_before_filter*100:.1f}%)\")\n",
    "    else:\n",
    "        print(\"  ⚠ No commits found for performance PRs after filtering\")\n",
    "        # Create empty dataframes with same structure\n",
    "        ai_with_commits = ai_perf_prs.iloc[0:0].copy()\n",
    "        human_with_commits = human_perf_prs.iloc[0:0].copy()\n",
    "    \n",
    "else:\n",
    "    print('⚠ pr_commit_details missing pr_id column; skipping commit merges.')\n",
    "    # Create empty dataframes\n",
    "    ai_with_commits = ai_perf_prs.iloc[0:0].copy()\n",
    "    human_with_commits = human_perf_prs.iloc[0:0].copy()\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Original Performance PRs:\")\n",
    "print(f\"  AI Agent: {original_ai_count:,}\")\n",
    "print(f\"  Human: {original_human_count:,}\")\n",
    "print(f\"  Total: {original_ai_count + original_human_count:,}\")\n",
    "print(f\"\\nAfter Commit Filtering:\")\n",
    "print(f\"✓ AI Agent Performance PRs: {len(ai_with_commits):,}\")\n",
    "print(f\"✓ Human Performance PRs: {len(human_with_commits):,}\")\n",
    "print(f\"✓ Total Performance PRs: {len(ai_with_commits) + len(human_with_commits):,}\")\n",
    "\n",
    "# Distribution by AI agent\n",
    "if len(ai_with_commits) > 0:\n",
    "    print(f\"\\nAI Agent Distribution:\")\n",
    "    for agent, count in ai_with_commits['agent'].value_counts().items():\n",
    "        pct = count / len(ai_with_commits) * 100\n",
    "        print(f\"  {agent:20s} {count:5,d} ({pct:5.1f}%)\")\n",
    "\n",
    "# Commit statistics summary\n",
    "if len(ai_with_commits) > 0 or len(human_with_commits) > 0:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"COMMIT STATISTICS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    for author_type, df in [('AI Agent', ai_with_commits), ('Human', human_with_commits)]:\n",
    "        if len(df) > 0:\n",
    "            print(f\"\\n{author_type}:\")\n",
    "            print(f\"  PRs with commit data: {len(df):,}\")\n",
    "            print(f\"  Avg commits per PR: {df['num_commits'].mean():.1f}\")\n",
    "            print(f\"  Median commits per PR: {df['num_commits'].median():.1f}\")\n",
    "            print(f\"  Avg additions: {df['additions'].mean():.0f} lines\")\n",
    "            print(f\"  Median additions: {df['additions'].median():.0f} lines\")\n",
    "            print(f\"  Avg deletions: {df['deletions'].mean():.0f} lines\")\n",
    "            print(f\"  Median deletions: {df['deletions'].median():.0f} lines\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa5d5acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset: 409 performance PRs\n",
      "  AI Agents: 326\n",
      "  Humans: 83\n"
     ]
    }
   ],
   "source": [
    "# Combine AI and Human PRs\n",
    "perf_prs = pd.concat([ai_with_commits, human_with_commits], ignore_index=True)\n",
    "\n",
    "print(f\"Combined dataset: {len(perf_prs):,} performance PRs\")\n",
    "print(f\"  AI Agents: {(perf_prs['author_type'] == 'AI Agent').sum():,}\")\n",
    "print(f\"  Humans: {(perf_prs['author_type'] == 'Human').sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a247c4a",
   "metadata": {},
   "source": [
    "# Optimization Pattern Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b725c6f4",
   "metadata": {},
   "source": [
    "## LLM inference script\n",
    "Script to map optimization to the performance optimization pattern catalog using LLM, based on PR title, body, and patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ed2759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Performance Optimization Pattern Detection with GPT\n",
    "# ============================================================================\n",
    "!pip install openai dotenv --quiet\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from pydantic import BaseModel \n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def analyze_optimization_with_gpt(title, body, patch):\n",
    "    \"\"\"\n",
    "    Call GPT to analyze performance optimization patterns in a commit.\n",
    "    \n",
    "    Parameters:\n",
    "    - title: PR/commit title\n",
    "    - body: PR/commit description\n",
    "    - patch: Git diff/patch content\n",
    "    \n",
    "    Returns:\n",
    "    - dict with analysis results or error info\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prepare the context\n",
    "    context_parts = []\n",
    "    \n",
    "    if pd.notna(title) and str(title).strip():\n",
    "        context_parts.append(f\"**Title**: {title}\")\n",
    "    \n",
    "    if pd.notna(body) and str(body).strip():\n",
    "        context_parts.append(f\"**Description**: {body}\")\n",
    "    \n",
    "    if pd.notna(patch) and str(patch).strip():\n",
    "        # Truncate very long patches to avoid token limits\n",
    "        patch_str = str(patch)\n",
    "        if len(patch_str) > 15000:  # Rough character limit\n",
    "            patch_str = patch_str[:15000] + \"\\n\\n... [patch truncated for length] ...\"\n",
    "        context_parts.append(f\"**Code Changes (Patch)**:\\n```diff\\n{patch_str}\\n```\")\n",
    "    \n",
    "    if not context_parts:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": \"No content available\",\n",
    "            \"explanation\": None,\n",
    "            \"optimization_comparison\": None,\n",
    "            \"high_level_pattern\": None,\n",
    "            \"sub_pattern\": None,\n",
    "            \"tokens_used\": 0\n",
    "        }\n",
    "    \n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    try:\n",
    "        load_dotenv()\n",
    "    except Exception:\n",
    "        # dotenv not installed / .env not loaded; rely on environment variables\n",
    "        pass\n",
    "\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise RuntimeError(\"OPENAI_API_KEY not found in environment. Add it to your .env or export it.\")\n",
    "\n",
    "    client = OpenAI(api_key=api_key)\n",
    "    \n",
    "    # Construct the prompt\n",
    "    prompt = f\"\"\"I have a performance optimization commit with the following information. Please analyze with the following goals:\n",
    "\n",
    "1. **Code Function Explanation**: Briefly explain what the code is doing—what problem it solves and how it works.\n",
    "\n",
    "2. **Optimization Comparison**: Compare the original and optimized versions to identify:\n",
    "   - **Algorithmic changes**: Any differences in logic, algorithm design, or problem-solving approach.\n",
    "   - **Performance improvements**: Enhancements related to time complexity, space efficiency, or runtime behavior.\n",
    "   - **Redundant code removal**: Elimination of unnecessary logic, method calls, or control structures.\n",
    "   - **Other noteworthy changes**: Any structural or stylistic differences that could impact performance or readability.\n",
    "   \n",
    "3. **Optimization Pattern Classification**:\n",
    "   Based on the overall nature of the optimized code, assign the following. Return \"No Meaningful Change\" if no meaningful change is made.\n",
    "   - **Exactly one high-level optimization pattern** from the list below  \n",
    "   - **One most representative sub-pattern** within that high-level category\n",
    "   \n",
    "   ### High-Level Optimization Patterns Taxonomy:\n",
    "   - **Algorithm-Level Optimizations**\n",
    "        - Select Computationally Efficient Algorithms\n",
    "        - Select Algorithm Based on Instruction Speed\n",
    "        - Structure Algorithm to Support instruction level parallelism (ILP)\n",
    "        - Select Space Efficient Algorithm\n",
    "        - Inheritance over Delegation for Energy Efficiency\n",
    "   - **Control-Flow and Branching Optimizations**\n",
    "        - Make Conditional Branches More Predictable\n",
    "        - Remove Branches with min/max Instructions\n",
    "        - Remove Branches by Doing Extra Work\n",
    "        - Remove Branching with Masking\n",
    "        - Rearranging Branches\n",
    "        - Combining Branches\n",
    "   - **Memory and Data Locality Optimizations**\n",
    "        - Access Data with Appropriate Type (Prevent Store Forwarding Issues)\n",
    "        - Increase Cache Efficiency via Locality\n",
    "        - Arrange Data for Optimal Hardware Prefetching\n",
    "        - Avoid cache capacity issues by segmenting work\n",
    "        - Increase Workload to Mitigate Memory Access Latency\n",
    "        - Use Smaller Data Types\n",
    "        - Caching\n",
    "        - Buffering\n",
    "        - Improve cache locality via data structure\n",
    "        - Optimize Object Use\n",
    "        - Reduce memory bloat from RTSJ Immortal Memory\n",
    "   - **Loop Transformations**\n",
    "        - Remove Conditional by Loop Unrolling\n",
    "        - Loop Distribution (Fission)\n",
    "        - Loop Fusion\n",
    "        - Loop Peeling\n",
    "        - Loop Interchanging\n",
    "        - Loop Invariant Branches\n",
    "        - Loop Strip-mining\n",
    "   - **I/O and Synchronization**\n",
    "        - Selection of I/O Size\n",
    "        - Polling\n",
    "        - Non-Blocking I/O\n",
    "   - **Data Structure Selection and Adaptation**\n",
    "        - Choose Structure for Energy Efficiency\n",
    "        - Darwinian Data Structure Selection\n",
    "        - Choose more energy-efficient data structure across Java Collections Framework, Apache Common Collections, and Eclipse Collections\n",
    "        - Choose energy-efficient data structure by method calls\n",
    "   - **Code Smells and Structural Simplification**\n",
    "        - Remove code bloat by removing optional features\n",
    "        - Remove Unnecessary Method Calls\n",
    "        - Remove long method by extracting new method\n",
    "        - Remove Duplicate code\n",
    "        - Minimize feature envy by moving methods\n",
    "        - Minimize occurrences of God Class\n",
    "        - Type Checking\n",
    "         \n",
    "Here are the info:\n",
    "            \n",
    "{context}\n",
    "\n",
    "**Output Structure**:  \n",
    "Please respond in JSON format with the following structure:\n",
    "{{\n",
    "  \"explanation\": \"Brief description of what the code is doing\",\n",
    "  \"optimization_comparison\": \"Detailed comparison highlighting specific optimizations\",\n",
    "  \"high_level_pattern\": \"Single most representative high-level optimization pattern (or 'No Meaningful Change')\",\n",
    "  \"sub_pattern\": \"Most representative sub-pattern within the category (or null if No Meaningful Change)\",\n",
    "}}\n",
    "\n",
    "Ensure your response is valid JSON that can be parsed.\n",
    "\"\"\"\n",
    "\n",
    "    class AnalysisResult(BaseModel):  \n",
    "        explanation: str\n",
    "        optimization_comparison: str\n",
    "        high_level_pattern: str\n",
    "        sub_pattern: str\n",
    "    \n",
    "    try:        \n",
    "        response = client.beta.chat.completions.parse(\n",
    "                    model = \"gpt-5.1\",\n",
    "                    messages = [\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": \"You are an expert software engineer specializing in performance optimization analysis. Analyze code changes and classify optimization patterns accurately.\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": prompt\n",
    "                        }\n",
    "                    ],\n",
    "                    response_format=AnalysisResult,\n",
    "                    temperature=0,\n",
    "                )\n",
    "\n",
    "        # Parse the response\n",
    "        content = response.choices[0].message.content\n",
    "        result = json.loads(content)\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"explanation\": result.get(\"explanation\", \"\"),\n",
    "            \"optimization_comparison\": result.get(\"optimization_comparison\", \"\"),\n",
    "            \"high_level_pattern\": result.get(\"high_level_pattern\", \"\"),\n",
    "            \"sub_pattern\": result.get(\"sub_pattern\", \"\"),\n",
    "            \"tokens_used\": response.usage.total_tokens,\n",
    "            \"error\": None\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e),\n",
    "            \"explanation\": None,\n",
    "            \"optimization_comparison\": None,\n",
    "            \"high_level_pattern\": None,\n",
    "            \"sub_pattern\": None,\n",
    "            \"tokens_used\": 0\n",
    "        }\n",
    "\n",
    "\n",
    "def batch_analyze_performance_prs(perf_prs, batch_size=10, delay=1.0,resume=False, checkpoint_prefix='perf_prs_checkpoint', output_file='perf_prs_with_gpt_analysis.csv'):\n",
    "    \"\"\"\n",
    "    Analyze all performance PRs in batches.\n",
    "\n",
    "    Parameters:\n",
    "    - perf_prs: DataFrame with performance PRs\n",
    "    - batch_size: Number of PRs to process before saving checkpoint\n",
    "    - delay: Delay between API calls in seconds\n",
    "    - resume: Continue from the last available checkpoint if True\n",
    "    - checkpoint_prefix: Filename prefix used for checkpoint files\n",
    "    - output_file: Final CSV filename for the aggregated results\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with analysis results added\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Starting GPT analysis of {len(perf_prs):,} performance PRs...\")\n",
    "\n",
    "    checkpoint_files = []\n",
    "    processed_count = 0\n",
    "\n",
    "    if resume:\n",
    "        checkpoint_files = sorted(Path('.').glob(f\"{checkpoint_prefix}_*.csv\"))\n",
    "        if checkpoint_files:\n",
    "            def _processed_from_path(path_obj):\n",
    "                suffix = path_obj.stem.rsplit('_', 1)[-1]\n",
    "                return int(suffix) if suffix.isdigit() else 0\n",
    "\n",
    "            latest_checkpoint = max(checkpoint_files, key=_processed_from_path)\n",
    "            checkpoint_progress = _processed_from_path(latest_checkpoint)\n",
    "            perf_prs = pd.read_csv(latest_checkpoint)\n",
    "            processed_count = min(checkpoint_progress, len(perf_prs))\n",
    "            print(f\"↻ Resuming from checkpoint {latest_checkpoint} ({processed_count} PRs processed)...\")\n",
    "        else:\n",
    "            print(\"↻ Resume requested but no checkpoint found. Starting from scratch.\")\n",
    "\n",
    "    result_defaults = {\n",
    "        'gpt_explanation': None,\n",
    "        'gpt_comparison': None,\n",
    "        'optimization_pattern': None,\n",
    "        'optimization_subpattern': None,\n",
    "        'gpt_success': False,\n",
    "        'gpt_error': None,\n",
    "        'gpt_tokens': 0\n",
    "    }\n",
    "\n",
    "    for column, default in result_defaults.items():\n",
    "        if resume and column in perf_prs.columns:\n",
    "            continue\n",
    "        perf_prs[column] = default\n",
    "\n",
    "    start_idx = processed_count if resume else 0\n",
    "    iterator = range(start_idx, len(perf_prs))\n",
    "    progress_bar = tqdm(iterator, total=len(perf_prs), desc=\"Analyzing PRs\", initial=start_idx)\n",
    "\n",
    "    for idx in progress_bar:\n",
    "        row = perf_prs.iloc[idx]\n",
    "        result = analyze_optimization_with_gpt(\n",
    "            title=row.get('title'),\n",
    "            body=row.get('body'),\n",
    "            patch=row.get('patch')\n",
    "        )\n",
    "\n",
    "        perf_prs.at[idx, 'gpt_success'] = result['success']\n",
    "        perf_prs.at[idx, 'gpt_tokens'] = result['tokens_used']\n",
    "\n",
    "        if result['success']:\n",
    "            perf_prs.at[idx, 'gpt_explanation'] = result['explanation']\n",
    "            perf_prs.at[idx, 'gpt_comparison'] = result['optimization_comparison']\n",
    "            perf_prs.at[idx, 'optimization_pattern'] = result['high_level_pattern']\n",
    "            perf_prs.at[idx, 'optimization_subpattern'] = result['sub_pattern']\n",
    "            perf_prs.at[idx, 'gpt_error'] = None\n",
    "        else:\n",
    "            perf_prs.at[idx, 'gpt_error'] = result['error']\n",
    "\n",
    "        time.sleep(delay)\n",
    "\n",
    "        if (idx + 1) % batch_size == 0:\n",
    "            checkpoint_file = f\"{checkpoint_prefix}_{idx+1}.csv\"\n",
    "            perf_prs.to_csv(checkpoint_file, index=False)\n",
    "            print(f\"✓ Checkpoint saved: {checkpoint_file}\")\n",
    "\n",
    "    perf_prs.to_csv(output_file, index=False)\n",
    "    print(f\"✓ Analysis complete! Saved to: {output_file}\")\n",
    "\n",
    "    success_series = perf_prs['gpt_success'].fillna(False)\n",
    "    success_count = success_series.sum()\n",
    "    success_rate = (success_count / len(perf_prs) * 100) if len(perf_prs) else 0\n",
    "    failure_count = success_series.eq(False).sum()\n",
    "    total_tokens = perf_prs['gpt_tokens'].sum()\n",
    "\n",
    "    print(f\"{'='*80}\")\n",
    "    print(\"ANALYSIS SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total PRs analyzed: {len(perf_prs):,}\")\n",
    "    print(f\"Successful: {success_count:,} ({success_rate:.1f}%)\")\n",
    "    print(f\"Failed: {failure_count:,}\")\n",
    "    print(f\"Total tokens used: {total_tokens:,}\")\n",
    "\n",
    "    if success_count > 0:\n",
    "        print(f\"{'='*80}\")\n",
    "        print(\"OPTIMIZATION PATTERN DISTRIBUTION\")\n",
    "        print(f\"{'='*80}\")\n",
    "        pattern_counts = perf_prs[perf_prs['gpt_success'] == True]['optimization_pattern'].value_counts()\n",
    "        for pattern, count in pattern_counts.items():\n",
    "            pct = count / success_count * 100\n",
    "            print(f\"  {pattern:50s} {count:4d} ({pct:5.1f}%)\")\n",
    "\n",
    "    return perf_prs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532b365f",
   "metadata": {},
   "source": [
    "## Usage scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107368e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Usage\n",
    "# ============================================================================\n",
    "\n",
    "# run ai and human pr analysis separately\n",
    "\n",
    "# ai pr analysis\n",
    "ai_sample = perf_prs[perf_prs['author_type'] == 'AI Agent']\n",
    "print(f\"Testing GPT analysis on {len(ai_sample)} AI PRs\")\n",
    "\n",
    "# Run the analysis\n",
    "perf_prs_analyzed = batch_analyze_performance_prs(\n",
    "    ai_sample,\n",
    "    batch_size=10,    # Save checkpoint every 10 PRs\n",
    "    delay=0.5,        # 0.5 second delay between API calls\n",
    "    resume=True,      # Continue from the last saved checkpoint if available\n",
    "    checkpoint_prefix='ai_perf_prs_checkpoint',\n",
    "    output_file='ai_perf_prs_with_gpt_analysis.csv'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6ba3fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing GPT analysis on 83 Human PRs\n",
      "Starting GPT analysis of 83 performance PRs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing PRs:  12%|█▏        | 10/83 [01:56<15:04, 12.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint saved: human_perf_prs_checkpoint_10.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing PRs:  24%|██▍       | 20/83 [04:15<15:11, 14.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint saved: human_perf_prs_checkpoint_20.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing PRs:  36%|███▌      | 30/83 [06:36<12:06, 13.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint saved: human_perf_prs_checkpoint_30.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing PRs:  48%|████▊     | 40/83 [08:59<12:42, 17.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint saved: human_perf_prs_checkpoint_40.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing PRs:  60%|██████    | 50/83 [11:53<08:40, 15.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint saved: human_perf_prs_checkpoint_50.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing PRs:  72%|███████▏  | 60/83 [14:26<06:16, 16.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint saved: human_perf_prs_checkpoint_60.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing PRs:  84%|████████▍ | 70/83 [16:53<03:43, 17.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint saved: human_perf_prs_checkpoint_70.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing PRs:  96%|█████████▋| 80/83 [18:55<00:37, 12.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint saved: human_perf_prs_checkpoint_80.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing PRs: 100%|██████████| 83/83 [19:58<00:00, 14.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Analysis complete! Saved to: human_perf_prs_with_gpt_analysis.csv\n",
      "================================================================================\n",
      "ANALYSIS SUMMARY\n",
      "================================================================================\n",
      "Total PRs analyzed: 83\n",
      "Successful: 83 (100.0%)\n",
      "Failed: 0\n",
      "Total tokens used: 374,517\n",
      "================================================================================\n",
      "OPTIMIZATION PATTERN DISTRIBUTION\n",
      "================================================================================\n",
      "  Code Smells and Structural Simplification            39 ( 47.0%)\n",
      "  Memory and Data Locality Optimizations               17 ( 20.5%)\n",
      "  Algorithm-Level Optimizations                        14 ( 16.9%)\n",
      "  No Meaningful Change                                  8 (  9.6%)\n",
      "  I/O and Synchronization                               3 (  3.6%)\n",
      "  Data Structure Selection and Adaptation               1 (  1.2%)\n",
      "  Control-Flow and Branching Optimizations              1 (  1.2%)\n"
     ]
    }
   ],
   "source": [
    "# human pr analysis\n",
    "human_sample = perf_prs[perf_prs['author_type'] == 'Human'].copy().reset_index(drop=True)\n",
    "print(f\"Testing GPT analysis on {len(human_sample)} Human PRs\")\n",
    "\n",
    "# Run the analysis\n",
    "perf_prs_analyzed = batch_analyze_performance_prs(\n",
    "    human_sample,\n",
    "    batch_size=10,    # Save checkpoint every 10 PRs\n",
    "    delay=0.5,        # 0.5 second delay between API calls\n",
    "    resume=False,      # Continue from the last saved checkpoint if available\n",
    "    checkpoint_prefix='human_perf_prs_checkpoint',\n",
    "    output_file='human_perf_prs_with_gpt_analysis.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f452210c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
