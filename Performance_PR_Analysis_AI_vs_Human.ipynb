{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance PR Analysis: AI Agents vs Humans\n",
    "\n",
    "## Comprehensive Analysis of Performance Optimization PRs\n",
    "\n",
    "This notebook addresses all research questions comparing AI coding agents and human developers on performance optimization tasks:\n",
    "\n",
    "### 1. **Adoption and Practices**\n",
    "\n",
    "- What practices (e.g., PR size, task type, and commit granularity) correlate with the quality of **performance optimization commits**?\n",
    "    - How can these practices inform **guidelines for developers and AI agents** to produce effective and efficient performance-improving patches?\n",
    "- Which programming languages most frequently appear in performance-improving tasks?\n",
    "\n",
    "### 2. **Optimization Patch Characteristics**\n",
    "\n",
    "- How do performance-oriented code patches modify software structure (e.g., changes in loops, data structures, parallelization, or algorithmic complexity; additions, deletions, files touched)?\n",
    "- What are the implications for maintainability?\n",
    "\n",
    "### 3. **Testing and Evaluation Behavior**\n",
    "\n",
    "- How do AI agents **measure and validate performance improvements** (e.g., benchmarks, profiling metrics, unit tests)?\n",
    "- How frequently and systematically are performance tests executed after code optimization?\n",
    "\n",
    "### 4. **Review Dynamics**\n",
    "\n",
    "- Do **performance-optimizing pull requests** receive distinctive review attention (e.g., profiling validation, discussion of tradeoffs)?\n",
    "- What kinds of review comments (e.g., correctness, maintainability, performance tradeoffs) are most associated with optimization patches?\n",
    "\n",
    "### 5. **Failure Patterns and Risks**\n",
    "\n",
    "- What **common failure patterns** and code quality issues appear in optimization-oriented PRs?\n",
    "- Why do these failures occur, and how can insights from them be used to **improve automated optimization systems and human‚ÄìAI collaboration**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Environment ready!\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "!pip install pandas numpy matplotlib seaborn scipy wordcloud pyarrow datasets --quiet\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import re\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "# Plot settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Environment ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compatibility shim: some versions of fsspec don't expose url_to_fs at top-level.\n",
    "# This ensures code that expects fsspec.url_to_fs (used by some IO backends) continues to work.\n",
    "try:\n",
    "    import fsspec\n",
    "    if not hasattr(fsspec, \"url_to_fs\"):\n",
    "        try:\n",
    "            from fsspec.core import url_to_fs as _url_to_fs\n",
    "        except Exception:\n",
    "            try:\n",
    "                import fsspec.core as _core\n",
    "                _url_to_fs = _core.url_to_fs\n",
    "            except Exception:\n",
    "                # Fallback shim: create a minimal url_to_fs that returns a filesystem and the path.\n",
    "                def _url_to_fs(url, **kwargs):\n",
    "                    protocol = url.split(\"://\")[0] if \"://\" in url else \"file\"\n",
    "                    fs = fsspec.filesystem(protocol)\n",
    "                    return fs, url\n",
    "        fsspec.url_to_fs = _url_to_fs\n",
    "except Exception:\n",
    "    # If anything goes wrong, continue without failing here; subsequent IO calls will raise their own errors.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sha</th>\n",
       "      <th>pr_id</th>\n",
       "      <th>pr_number</th>\n",
       "      <th>commit_stats_total</th>\n",
       "      <th>commit_stats_additions</th>\n",
       "      <th>commit_stats_deletions</th>\n",
       "      <th>filename</th>\n",
       "      <th>status</th>\n",
       "      <th>additions</th>\n",
       "      <th>deletions</th>\n",
       "      <th>changes</th>\n",
       "      <th>patch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d0f43c98b9a98bbacaa9a56f513bc97684d32ccc</td>\n",
       "      <td>2486573779</td>\n",
       "      <td>90516</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>src/sentry/utils/performance_issues/base.py</td>\n",
       "      <td>modified</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>@@ -95,8 +95,9 @@ def visit_span(self, span: Span) -&gt; None:\\n     def on_complete(self) -&gt; None:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d0f43c98b9a98bbacaa9a56f513bc97684d32ccc</td>\n",
       "      <td>2486573779</td>\n",
       "      <td>90516</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>src/sentry/utils/performance_issues/detectors/experiments/n_plus_one_api_calls_detector.py</td>\n",
       "      <td>modified</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>@@ -48,7 +48,8 @@ def __init__(self, settings: dict[DetectorType, Any], event: dict[str, Any]) -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d0f43c98b9a98bbacaa9a56f513bc97684d32ccc</td>\n",
       "      <td>2486573779</td>\n",
       "      <td>90516</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>src/sentry/utils/performance_issues/detectors/io_main_thread_detector.py</td>\n",
       "      <td>modified</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>@@ -7,7 +7,6 @@\\n import sentry_sdk\\n from symbolic.proguard import ProguardMapper\\n \\n-from sen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d0f43c98b9a98bbacaa9a56f513bc97684d32ccc</td>\n",
       "      <td>2486573779</td>\n",
       "      <td>90516</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>src/sentry/utils/performance_issues/performance_detection.py</td>\n",
       "      <td>modified</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>@@ -353,7 +353,7 @@ def _detect_performance_problems(\\n         detectors: list[PerformanceDetec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>91d120e06968062bca53859a962c3b41456606fa</td>\n",
       "      <td>2486573779</td>\n",
       "      <td>90516</td>\n",
       "      <td>27</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>src/sentry/utils/performance_issues/base.py</td>\n",
       "      <td>modified</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>@@ -96,7 +96,12 @@ def on_complete(self) -&gt; None:\\n         pass\\n \\n     @classmethod\\n-    def...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        sha       pr_id  pr_number  \\\n",
       "0  d0f43c98b9a98bbacaa9a56f513bc97684d32ccc  2486573779      90516   \n",
       "1  d0f43c98b9a98bbacaa9a56f513bc97684d32ccc  2486573779      90516   \n",
       "2  d0f43c98b9a98bbacaa9a56f513bc97684d32ccc  2486573779      90516   \n",
       "3  d0f43c98b9a98bbacaa9a56f513bc97684d32ccc  2486573779      90516   \n",
       "4  91d120e06968062bca53859a962c3b41456606fa  2486573779      90516   \n",
       "\n",
       "   commit_stats_total  commit_stats_additions  commit_stats_deletions  \\\n",
       "0                  20                       6                      14   \n",
       "1                  20                       6                      14   \n",
       "2                  20                       6                      14   \n",
       "3                  20                       6                      14   \n",
       "4                  27                      22                       5   \n",
       "\n",
       "                                                                                     filename  \\\n",
       "0                                                 src/sentry/utils/performance_issues/base.py   \n",
       "1  src/sentry/utils/performance_issues/detectors/experiments/n_plus_one_api_calls_detector.py   \n",
       "2                    src/sentry/utils/performance_issues/detectors/io_main_thread_detector.py   \n",
       "3                                src/sentry/utils/performance_issues/performance_detection.py   \n",
       "4                                                 src/sentry/utils/performance_issues/base.py   \n",
       "\n",
       "     status  additions  deletions  changes  \\\n",
       "0  modified          3          6        9   \n",
       "1  modified          2          1        3   \n",
       "2  modified          0          5        5   \n",
       "3  modified          1          2        3   \n",
       "4  modified         20          3       23   \n",
       "\n",
       "                                                                                                 patch  \n",
       "0  @@ -95,8 +95,9 @@ def visit_span(self, span: Span) -> None:\\n     def on_complete(self) -> None:...  \n",
       "1  @@ -48,7 +48,8 @@ def __init__(self, settings: dict[DetectorType, Any], event: dict[str, Any]) -...  \n",
       "2  @@ -7,7 +7,6 @@\\n import sentry_sdk\\n from symbolic.proguard import ProguardMapper\\n \\n-from sen...  \n",
       "3  @@ -353,7 +353,7 @@ def _detect_performance_problems(\\n         detectors: list[PerformanceDetec...  \n",
       "4  @@ -96,7 +96,12 @@ def on_complete(self) -> None:\\n         pass\\n \\n     @classmethod\\n-    def...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_pr_commit_details = pd.read_parquet(\"./datasets/human_pr/human_pr_commit_details.parquet\")\n",
    "\n",
    "human_pr_commit_details.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sha</th>\n",
       "      <th>pr_id</th>\n",
       "      <th>pr_number</th>\n",
       "      <th>repo_owner</th>\n",
       "      <th>repo_name</th>\n",
       "      <th>author</th>\n",
       "      <th>committer</th>\n",
       "      <th>commit_message</th>\n",
       "      <th>pr_title</th>\n",
       "      <th>pr_description</th>\n",
       "      <th>pr_comments_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d0f43c98b9a98bbacaa9a56f513bc97684d32ccc</td>\n",
       "      <td>2486573779</td>\n",
       "      <td>90516</td>\n",
       "      <td>getsentry</td>\n",
       "      <td>sentry</td>\n",
       "      <td>Leander Rodrigues</td>\n",
       "      <td>Leander Rodrigues</td>\n",
       "      <td>‚ôªÔ∏è consolidate detection fallthroughs</td>\n",
       "      <td>ref(perf-issues): Consolidate File IO override option</td>\n",
       "      <td>This PR removes the `performance_issues.file_io_main_thread.disabled` override option for the Fi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>91d120e06968062bca53859a962c3b41456606fa</td>\n",
       "      <td>2486573779</td>\n",
       "      <td>90516</td>\n",
       "      <td>getsentry</td>\n",
       "      <td>sentry</td>\n",
       "      <td>Leander Rodrigues</td>\n",
       "      <td>Leander Rodrigues</td>\n",
       "      <td>üöö rename creation -&gt; detection + docs</td>\n",
       "      <td>ref(perf-issues): Consolidate File IO override option</td>\n",
       "      <td>This PR removes the `performance_issues.file_io_main_thread.disabled` override option for the Fi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dc9d1965ce24fa1c89c194cfcdad2ba5e120060a</td>\n",
       "      <td>2419106029</td>\n",
       "      <td>87963</td>\n",
       "      <td>getsentry</td>\n",
       "      <td>sentry</td>\n",
       "      <td>Markus Unterwaditzer</td>\n",
       "      <td>Markus Unterwaditzer</td>\n",
       "      <td>ref(span-buffer): Move from sets to arrays\\n\\nArrays might be faster as they might not run compa...</td>\n",
       "      <td>ref(span-buffer): Move from sets to arrays</td>\n",
       "      <td>Arrays might be faster as they might not run comparisons on payloads to\\ndetermine whether they ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>79e57d9480e0fa817fc1cdd0fa9ce9b99bace87d</td>\n",
       "      <td>2425248848</td>\n",
       "      <td>18585</td>\n",
       "      <td>oven-sh</td>\n",
       "      <td>bun</td>\n",
       "      <td>Dylan Conway</td>\n",
       "      <td>Dylan Conway</td>\n",
       "      <td>dont encode as double if possible</td>\n",
       "      <td>avoid encoding as double in `napi_create_double` if possible</td>\n",
       "      <td>### What does this PR do?\\r\\nArithmetic on numbers encoded as doubles in JSC seems to hit more s...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bdfde2a522d7982f7b9e37c7964b73abd4e82245</td>\n",
       "      <td>2613893429</td>\n",
       "      <td>20612</td>\n",
       "      <td>oven-sh</td>\n",
       "      <td>bun</td>\n",
       "      <td>Jarred Sumner</td>\n",
       "      <td>Jarred Sumner</td>\n",
       "      <td>Optimize  `napi_get_value_string_utf8` `napi_get_value_string_latin1` `napi_get_value_string_utf16`</td>\n",
       "      <td>Optimize  `napi_get_value_string_utf8` `napi_get_value_string_latin1`  `napi_get_value_string_ut...</td>\n",
       "      <td>\\r\\n\\r\\n### What does this PR do?\\r\\n\\r\\nAvoid resolving string slices\\r\\n\\r\\nCheck for exceptio...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        sha       pr_id  pr_number repo_owner  \\\n",
       "0  d0f43c98b9a98bbacaa9a56f513bc97684d32ccc  2486573779      90516  getsentry   \n",
       "1  91d120e06968062bca53859a962c3b41456606fa  2486573779      90516  getsentry   \n",
       "2  dc9d1965ce24fa1c89c194cfcdad2ba5e120060a  2419106029      87963  getsentry   \n",
       "3  79e57d9480e0fa817fc1cdd0fa9ce9b99bace87d  2425248848      18585    oven-sh   \n",
       "4  bdfde2a522d7982f7b9e37c7964b73abd4e82245  2613893429      20612    oven-sh   \n",
       "\n",
       "  repo_name                author             committer  \\\n",
       "0    sentry     Leander Rodrigues     Leander Rodrigues   \n",
       "1    sentry     Leander Rodrigues     Leander Rodrigues   \n",
       "2    sentry  Markus Unterwaditzer  Markus Unterwaditzer   \n",
       "3       bun          Dylan Conway          Dylan Conway   \n",
       "4       bun         Jarred Sumner         Jarred Sumner   \n",
       "\n",
       "                                                                                        commit_message  \\\n",
       "0                                                                ‚ôªÔ∏è consolidate detection fallthroughs   \n",
       "1                                                                üöö rename creation -> detection + docs   \n",
       "2  ref(span-buffer): Move from sets to arrays\\n\\nArrays might be faster as they might not run compa...   \n",
       "3                                                                    dont encode as double if possible   \n",
       "4  Optimize  `napi_get_value_string_utf8` `napi_get_value_string_latin1` `napi_get_value_string_utf16`   \n",
       "\n",
       "                                                                                              pr_title  \\\n",
       "0                                                ref(perf-issues): Consolidate File IO override option   \n",
       "1                                                ref(perf-issues): Consolidate File IO override option   \n",
       "2                                                           ref(span-buffer): Move from sets to arrays   \n",
       "3                                         avoid encoding as double in `napi_create_double` if possible   \n",
       "4  Optimize  `napi_get_value_string_utf8` `napi_get_value_string_latin1`  `napi_get_value_string_ut...   \n",
       "\n",
       "                                                                                        pr_description  \\\n",
       "0  This PR removes the `performance_issues.file_io_main_thread.disabled` override option for the Fi...   \n",
       "1  This PR removes the `performance_issues.file_io_main_thread.disabled` override option for the Fi...   \n",
       "2  Arrays might be faster as they might not run comparisons on payloads to\\ndetermine whether they ...   \n",
       "3  ### What does this PR do?\\r\\nArithmetic on numbers encoded as doubles in JSC seems to hit more s...   \n",
       "4  \\r\\n\\r\\n### What does this PR do?\\r\\n\\r\\nAvoid resolving string slices\\r\\n\\r\\nCheck for exceptio...   \n",
       "\n",
       "   pr_comments_count  \n",
       "0                  1  \n",
       "1                  1  \n",
       "2                  2  \n",
       "3                  3  \n",
       "4                  1  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_pr_commits = pd.read_parquet(\"./datasets/human_pr/human_pr_commits.parquet\")\n",
    "human_pr_commits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading AIDev datasets...\n",
      "\n",
      "‚úì Performance PR IDs to process: 428\n",
      "\n",
      "Processing commit details (filtering to performance PRs only)...\n",
      "  Total commit records in dataset: 719,797\n",
      "  Filtered to performance PRs: 15,284 commit records\n",
      "  Unique performance PRs with commits: 427\n",
      "  Filtered out null filenames: 46 records removed\n",
      "  Remaining after filename filter: 15,238 commit records\n",
      "  Filtered out config-only commits: 44 records removed\n",
      "  Remaining after config filter: 15,194 commit records\n",
      "  Filtered out merge commits: 10,166 records removed\n",
      "  Remaining after merge filter: 5,028 commit records\n",
      "  Removed deleted repo PR commits: 74 records removed\n",
      "  Remaining after deleted repo filter: 4,954 commit records\n",
      "  Unique performance PRs after all filters: 407\n",
      "  ‚úì Aggregated to 407 unique performance PRs\n",
      "  Avg commits per PR: 12.2\n",
      "  AI Agent PRs with commit data: 324 / 340 (95.3%)\n",
      "  Human PRs with commit data: 83 / 88 (94.3%)\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "Original Performance PRs:\n",
      "  AI Agent: 340\n",
      "  Human: 88\n",
      "  Total: 428\n",
      "\n",
      "After Commit Filtering:\n",
      "‚úì AI Agent Performance PRs: 324\n",
      "‚úì Human Performance PRs: 83\n",
      "‚úì Total Performance PRs: 407\n",
      "\n",
      "AI Agent Distribution:\n",
      "  OpenAI_Codex           205 ( 63.3%)\n",
      "  Devin                   59 ( 18.2%)\n",
      "  Copilot                 37 ( 11.4%)\n",
      "  Cursor                  20 (  6.2%)\n",
      "  Claude_Code              3 (  0.9%)\n",
      "\n",
      "================================================================================\n",
      "COMMIT STATISTICS\n",
      "================================================================================\n",
      "\n",
      "AI Agent:\n",
      "  PRs with commit data: 324\n",
      "  Avg commits per PR: 13.3\n",
      "  Median commits per PR: 4.0\n",
      "  Avg additions: 368 lines\n",
      "  Median additions: 61 lines\n",
      "  Avg deletions: 282 lines\n",
      "  Median deletions: 25 lines\n",
      "\n",
      "Human:\n",
      "  PRs with commit data: 83\n",
      "  Avg commits per PR: 7.8\n",
      "  Median commits per PR: 2.0\n",
      "  Avg additions: 204 lines\n",
      "  Median additions: 29 lines\n",
      "  Avg deletions: 142 lines\n",
      "  Median deletions: 19 lines\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Load datasets\n",
    "print(\"Loading AIDev datasets...\")\n",
    "\n",
    "# AI Agent PRs\n",
    "pr_df = pd.read_parquet(\"hf://datasets/hao-li/AIDev/pull_request.parquet\")\n",
    "pr_task_type_df = pd.read_parquet(\"hf://datasets/hao-li/AIDev/pr_task_type.parquet\")\n",
    "ai_perf_prs = (\n",
    "    pr_df\n",
    "    .merge(\n",
    "        pr_task_type_df[[\"id\", \"type\", \"reason\"]],\n",
    "        on=\"id\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    .query(\"type == 'perf'\")\n",
    "    .copy()\n",
    ")\n",
    "ai_perf_prs['classification_reason'] = ai_perf_prs['reason']\n",
    "ai_perf_prs['author_type'] = 'AI Agent'\n",
    "\n",
    "# Human PRs\n",
    "human_pr_df = pd.read_parquet(\"hf://datasets/hao-li/AIDev/human_pull_request.parquet\")\n",
    "human_pr_task_type_df = pd.read_parquet(\"hf://datasets/hao-li/AIDev/human_pr_task_type.parquet\")\n",
    "human_perf_prs = (\n",
    "    human_pr_df\n",
    "    .merge(\n",
    "        human_pr_task_type_df[[\"id\", \"type\", \"reason\"]],\n",
    "        on=\"id\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    .query(\"type == 'perf'\")\n",
    "    .copy()\n",
    ")\n",
    "human_perf_prs['classification_reason'] = human_perf_prs['reason']\n",
    "human_perf_prs['author_type'] = 'Human'\n",
    "human_perf_prs['agent'] = 'Human'\n",
    "\n",
    "# Store original counts\n",
    "original_ai_count = len(ai_perf_prs)\n",
    "original_human_count = len(human_perf_prs)\n",
    "\n",
    "# Repository data for language info\n",
    "all_repo_df = pd.read_parquet(\"hf://datasets/hao-li/AIDev/all_repository.parquet\")\n",
    "\n",
    "# Get list of performance PR IDs we care about\n",
    "perf_pr_ids = set(ai_perf_prs['id'].tolist() + human_perf_prs['id'].tolist())\n",
    "print(f\"\\n‚úì Performance PR IDs to process: {len(perf_pr_ids):,}\")\n",
    "\n",
    "# PR commits details - FILTER FIRST, then aggregate\n",
    "print(\"\\nProcessing commit details (filtering to performance PRs only)...\")\n",
    "pr_commits_details = pd.read_parquet(\"hf://datasets/hao-li/AIDev/pr_commit_details.parquet\")\n",
    "\n",
    "# Pr commit details for human PRs\n",
    "human_pr_commit_details = pd.read_parquet(\"./datasets/human_pr/human_pr_commit_details_original.parquet\")\n",
    "human_pr_commits = pd.read_parquet(\"./datasets/human_pr/human_pr_commits_original.parquet\")\n",
    "\n",
    "# Extract only the columns you need from the second table\n",
    "msg_df = human_pr_commits[[\"sha\", \"commit_message\"]]\n",
    "\n",
    "human_pr_commit_details = (\n",
    "    human_pr_commit_details\n",
    "        .merge(msg_df, on=\"sha\", how=\"left\")\n",
    ")\n",
    "\n",
    "human_pr_commit_details.rename(columns={\"commit_message\": \"message\"}, inplace=True)\n",
    "\n",
    "pr_commits_details = pd.concat(\n",
    "    [pr_commits_details, human_pr_commit_details],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "if 'pr_id' in pr_commits_details.columns:\n",
    "    print(f\"  Total commit records in dataset: {len(pr_commits_details):,}\")\n",
    "    \n",
    "    # FILTER: Keep only commits for performance PRs\n",
    "    pr_commits_filtered = pr_commits_details[pr_commits_details['pr_id'].isin(perf_pr_ids)].copy()\n",
    "    print(f\"  Filtered to performance PRs: {len(pr_commits_filtered):,} commit records\")\n",
    "    print(f\"  Unique performance PRs with commits: {pr_commits_filtered['pr_id'].nunique():,}\")\n",
    "    \n",
    "    # ADDITIONAL FILTERING: Remove commits with null filename\n",
    "    if 'filename' in pr_commits_filtered.columns:\n",
    "        before_filename_filter = len(pr_commits_filtered)\n",
    "        pr_commits_filtered = pr_commits_filtered[pr_commits_filtered['filename'].notna()].copy()\n",
    "        print(f\"  Filtered out null filenames: {before_filename_filter - len(pr_commits_filtered):,} records removed\")\n",
    "        print(f\"  Remaining after filename filter: {len(pr_commits_filtered):,} commit records\")\n",
    "        \n",
    "    # ADDITIONAL FILTERING: Remove config/metadata-only files\n",
    "    if 'filename' in pr_commits_filtered.columns:\n",
    "        before_config_filter = len(pr_commits_filtered)\n",
    "        \n",
    "        # Define patterns for non-code files to exclude\n",
    "        config_patterns = [\n",
    "            r'^\\.mvn/',                          # Maven wrapper configs\n",
    "            r'^\\.gradle/',                       # Gradle configs\n",
    "            r'^\\.idea/',                         # IntelliJ configs\n",
    "            r'^\\.vscode/',                       # VSCode configs\n",
    "            r'^\\.github/workflows/',             # GitHub Actions (unless it's code)\n",
    "            r'\\.properties$',                    # Properties files\n",
    "            r'\\.xml$',                           # XML config files (pom.xml, etc.)\n",
    "            r'\\.yml$',                           # YAML configs\n",
    "            r'\\.yaml$',                          # YAML configs\n",
    "            r'\\.json$',                          # JSON configs (package.json, etc.)\n",
    "            r'\\.md$',                            # Markdown docs\n",
    "            r'\\.txt$',                           # Text files\n",
    "            r'\\.gitignore$',                     # Git configs\n",
    "            r'\\.dockerignore$',                  # Docker ignore files\n",
    "            r'/Dockerfile$',                     # Dockerfiles (anywhere in path)\n",
    "            r'^Dockerfile$',                     # Dockerfile at root\n",
    "            r'/docker-compose',                  # Docker compose (anywhere)\n",
    "            r'^docker-compose',                  # Docker compose at root\n",
    "            r'\\.lock$',                          # Lock files (package-lock, yarn.lock)\n",
    "            r'^LICENSE',                         # License files\n",
    "            r'^README',                          # README files\n",
    "        ]\n",
    "        \n",
    "        config_pattern = '|'.join(config_patterns)\n",
    "        \n",
    "        # Mark config files\n",
    "        pr_commits_filtered['is_config_file'] = pr_commits_filtered['filename'].str.contains(\n",
    "            config_pattern, case=False, na=False, regex=True\n",
    "        )\n",
    "        \n",
    "        # Keep track of which files are code files per PR\n",
    "        pr_commits_filtered['is_code_file'] = ~pr_commits_filtered['is_config_file']\n",
    "        \n",
    "        # For each PR, check if it has ANY code files\n",
    "        pr_has_code = pr_commits_filtered.groupby('pr_id')['is_code_file'].any().reset_index()\n",
    "        pr_has_code.columns = ['pr_id', 'has_code_files']\n",
    "        \n",
    "        # Filter to keep only PRs that have at least one code file\n",
    "        pr_commits_filtered = pr_commits_filtered.merge(pr_has_code, on='pr_id', how='left')\n",
    "        pr_commits_filtered = pr_commits_filtered[pr_commits_filtered['has_code_files']].copy()\n",
    "        \n",
    "        # Clean up temporary columns\n",
    "        pr_commits_filtered = pr_commits_filtered.drop(columns=['is_config_file', 'is_code_file', 'has_code_files'])\n",
    "        \n",
    "        print(f\"  Filtered out config-only commits: {before_config_filter - len(pr_commits_filtered):,} records removed\")\n",
    "        print(f\"  Remaining after config filter: {len(pr_commits_filtered):,} commit records\")\n",
    "    \n",
    "    # ADDITIONAL FILTERING: Remove merge commits\n",
    "    if 'message' in pr_commits_filtered.columns:\n",
    "        before_merge_filter = len(pr_commits_filtered)\n",
    "        # Common merge commit patterns\n",
    "        merge_patterns = [\n",
    "            r'^Merge\\s+branch',\n",
    "            r'^Merge\\s+pull\\s+request',\n",
    "            r'^Merge\\s+remote-tracking\\s+branch',\n",
    "            r'^Merge\\s+.*\\s+into\\s+',\n",
    "            r\"^Merged\\s+in\\s+\",\n",
    "        ]\n",
    "        merge_pattern = '|'.join(merge_patterns)\n",
    "        pr_commits_filtered = pr_commits_filtered[\n",
    "            ~pr_commits_filtered['message'].str.match(merge_pattern, case=False, na=False)\n",
    "        ].copy()\n",
    "        print(f\"  Filtered out merge commits: {before_merge_filter - len(pr_commits_filtered):,} records removed\")\n",
    "        print(f\"  Remaining after merge filter: {len(pr_commits_filtered):,} commit records\")\n",
    "\n",
    "        # FINAL FILTER: drop PRs whose repositories were deleted\n",
    "        deleted_repo_pr_ids = {3271610326, 3209206554}\n",
    "        before_deleted_repo_filter = len(pr_commits_filtered)\n",
    "        pr_commits_filtered = pr_commits_filtered[~pr_commits_filtered['pr_id'].isin(deleted_repo_pr_ids)].copy()\n",
    "        removed_deleted_repo_commits = before_deleted_repo_filter - len(pr_commits_filtered)\n",
    "        if removed_deleted_repo_commits > 0:\n",
    "            print(f\"  Removed deleted repo PR commits: {removed_deleted_repo_commits:,} records removed\")\n",
    "        print(f\"  Remaining after deleted repo filter: {len(pr_commits_filtered):,} commit records\")\n",
    "    \n",
    "    print(f\"  Unique performance PRs after all filters: {pr_commits_filtered['pr_id'].nunique():,}\")\n",
    "    \n",
    "    if len(pr_commits_filtered) > 0:\n",
    "        # AGGREGATE: Now aggregate only the filtered commits\n",
    "        commit_aggregated = pr_commits_filtered.groupby('pr_id').agg({\n",
    "            'additions': 'sum',      # Total lines added across all commits\n",
    "            'deletions': 'sum',      # Total lines deleted across all commits\n",
    "            'patch': lambda x: '\\n\\n'.join([str(p) for p in x if pd.notna(p)])  # Concatenate all patches\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Add derived metrics\n",
    "        commit_aggregated['num_commits'] = pr_commits_filtered.groupby('pr_id').size().values\n",
    "        \n",
    "        # Calculate patch length (for analysis)\n",
    "        commit_aggregated['patch_length'] = commit_aggregated['patch'].str.len()\n",
    "        \n",
    "        print(f\"  ‚úì Aggregated to {len(commit_aggregated):,} unique performance PRs\")\n",
    "        print(f\"  Avg commits per PR: {commit_aggregated['num_commits'].mean():.1f}\")\n",
    "        \n",
    "        # Merge commit stats into AI Agent PR table\n",
    "        ai_perf_prs = ai_perf_prs.merge(\n",
    "            commit_aggregated,\n",
    "            left_on='id',\n",
    "            right_on='pr_id',\n",
    "            how='left'\n",
    "        )\n",
    "        if 'pr_id' in ai_perf_prs.columns:\n",
    "            ai_perf_prs = ai_perf_prs.drop(columns=['pr_id'])\n",
    "        \n",
    "        # Filter to keep only PRs with commit data\n",
    "        ai_before_filter = len(ai_perf_prs)\n",
    "        ai_with_commits = ai_perf_prs[ai_perf_prs['additions'].notna()].copy()\n",
    "        print(f\"  AI Agent PRs with commit data: {len(ai_with_commits):,} / {ai_before_filter:,} ({len(ai_with_commits)/ai_before_filter*100:.1f}%)\")\n",
    "        \n",
    "        # Merge commit stats into Human PR table\n",
    "        human_perf_prs = human_perf_prs.merge(\n",
    "            commit_aggregated,\n",
    "            left_on='id',\n",
    "            right_on='pr_id',\n",
    "            how='left'\n",
    "        )\n",
    "        if 'pr_id' in human_perf_prs.columns:\n",
    "            human_perf_prs = human_perf_prs.drop(columns=['pr_id'])\n",
    "        \n",
    "        # Filter to keep only PRs with commit data\n",
    "        human_before_filter = len(human_perf_prs)\n",
    "        human_with_commits = human_perf_prs[human_perf_prs['additions'].notna()].copy()\n",
    "        print(f\"  Human PRs with commit data: {len(human_with_commits):,} / {human_before_filter:,} ({len(human_with_commits)/human_before_filter*100:.1f}%)\")\n",
    "    else:\n",
    "        print(\"  ‚ö† No commits found for performance PRs after filtering\")\n",
    "        # Create empty dataframes with same structure\n",
    "        ai_with_commits = ai_perf_prs.iloc[0:0].copy()\n",
    "        human_with_commits = human_perf_prs.iloc[0:0].copy()\n",
    "    \n",
    "else:\n",
    "    print('‚ö† pr_commit_details missing pr_id column; skipping commit merges.')\n",
    "    # Create empty dataframes\n",
    "    ai_with_commits = ai_perf_prs.iloc[0:0].copy()\n",
    "    human_with_commits = human_perf_prs.iloc[0:0].copy()\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Original Performance PRs:\")\n",
    "print(f\"  AI Agent: {original_ai_count:,}\")\n",
    "print(f\"  Human: {original_human_count:,}\")\n",
    "print(f\"  Total: {original_ai_count + original_human_count:,}\")\n",
    "print(f\"\\nAfter Commit Filtering:\")\n",
    "print(f\"‚úì AI Agent Performance PRs: {len(ai_with_commits):,}\")\n",
    "print(f\"‚úì Human Performance PRs: {len(human_with_commits):,}\")\n",
    "print(f\"‚úì Total Performance PRs: {len(ai_with_commits) + len(human_with_commits):,}\")\n",
    "\n",
    "# Distribution by AI agent\n",
    "if len(ai_with_commits) > 0:\n",
    "    print(f\"\\nAI Agent Distribution:\")\n",
    "    for agent, count in ai_with_commits['agent'].value_counts().items():\n",
    "        pct = count / len(ai_with_commits) * 100\n",
    "        print(f\"  {agent:20s} {count:5,d} ({pct:5.1f}%)\")\n",
    "\n",
    "# Commit statistics summary\n",
    "if len(ai_with_commits) > 0 or len(human_with_commits) > 0:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"COMMIT STATISTICS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    for author_type, df in [('AI Agent', ai_with_commits), ('Human', human_with_commits)]:\n",
    "        if len(df) > 0:\n",
    "            print(f\"\\n{author_type}:\")\n",
    "            print(f\"  PRs with commit data: {len(df):,}\")\n",
    "            print(f\"  Avg commits per PR: {df['num_commits'].mean():.1f}\")\n",
    "            print(f\"  Median commits per PR: {df['num_commits'].median():.1f}\")\n",
    "            print(f\"  Avg additions: {df['additions'].mean():.0f} lines\")\n",
    "            print(f\"  Median additions: {df['additions'].median():.0f} lines\")\n",
    "            print(f\"  Avg deletions: {df['deletions'].mean():.0f} lines\")\n",
    "            print(f\"  Median deletions: {df['deletions'].median():.0f} lines\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine AI and Human PRs\n",
    "perf_prs = pd.concat([ai_with_commits, human_with_commits], ignore_index=True)\n",
    "\n",
    "print(f\"Combined dataset: {len(perf_prs):,} performance PRs\")\n",
    "print(f\"  AI Agents: {(perf_prs['author_type'] == 'AI Agent').sum():,}\")\n",
    "print(f\"  Humans: {(perf_prs['author_type'] == 'Human').sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_pr_id = perf_prs['id'].tolist()\n",
    "id = 3075266937\n",
    "if id in perf_pr_id:\n",
    "    print(f\"PR ID {id} found in performance PRs.\")\n",
    "else:\n",
    "    print(f\"PR ID {id} NOT found in performance PRs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add language from repository table\n",
    "print(\"Loading repository data for language information...\")\n",
    "\n",
    "if 'language' in all_repo_df.columns:\n",
    "    # For AI Agent PRs: join on repo_id\n",
    "    ai_mask = perf_prs['author_type'] == 'AI Agent'\n",
    "    ai_prs = perf_prs[ai_mask].copy()\n",
    "    human_prs = perf_prs[~ai_mask].copy()\n",
    "    \n",
    "    # Join AI PRs using repo_id\n",
    "    if 'repo_id' in ai_prs.columns:\n",
    "        ai_prs = ai_prs.merge(\n",
    "            all_repo_df[['id', 'language']], \n",
    "            left_on='repo_id', \n",
    "            right_on='id', \n",
    "            how='left',\n",
    "            suffixes=('', '_repo')\n",
    "        )\n",
    "        ai_prs['primary_language'] = ai_prs['language']\n",
    "        if 'id_repo' in ai_prs.columns:\n",
    "            ai_prs = ai_prs.drop(['id_repo'], axis=1)\n",
    "        if 'language' in ai_prs.columns:\n",
    "            ai_prs = ai_prs.drop(['language'], axis=1)\n",
    "    \n",
    "    # Join Human PRs using repo_url\n",
    "    # Human PRs have repo_url, match with url from all_repo_df\n",
    "    if 'repo_url' in human_prs.columns and 'url' in all_repo_df.columns:\n",
    "        human_prs = human_prs.merge(\n",
    "            all_repo_df[['url', 'language']], \n",
    "            left_on='repo_url',\n",
    "            right_on='url',\n",
    "            how='left',\n",
    "            suffixes=('', '_repo')\n",
    "        )\n",
    "        human_prs['primary_language'] = human_prs['language']\n",
    "        if 'url' in human_prs.columns:\n",
    "            human_prs = human_prs.drop(['url'], axis=1)\n",
    "        if 'language' in human_prs.columns:\n",
    "            human_prs = human_prs.drop(['language'], axis=1)\n",
    "    \n",
    "    # Combine back together\n",
    "    perf_prs = pd.concat([ai_prs, human_prs], ignore_index=True)\n",
    "    \n",
    "    ai_lang_count = perf_prs[perf_prs['author_type'] == 'AI Agent']['primary_language'].notna().sum()\n",
    "    human_lang_count = perf_prs[perf_prs['author_type'] == 'Human']['primary_language'].notna().sum()\n",
    "    \n",
    "    print(f\"‚úì Language data joined!\")\n",
    "    print(f\"  AI Agent PRs with language: {ai_lang_count:,}\")\n",
    "    print(f\"  Human PRs with language: {human_lang_count:,}\")\n",
    "    print(f\"  Total: {perf_prs['primary_language'].notna().sum():,} PRs with language info\")\n",
    "else:\n",
    "    perf_prs['primary_language'] = None\n",
    "    print(\"‚ö† No language column in repository table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date columns\n",
    "for col in ['created_at', 'closed_at', 'merged_at']:\n",
    "    if col in perf_prs.columns:\n",
    "        perf_prs[col] = pd.to_datetime(perf_prs[col])\n",
    "\n",
    "# Add derived status columns\n",
    "perf_prs['is_merged'] = perf_prs['merged_at'].notna()\n",
    "perf_prs['is_closed'] = perf_prs['closed_at'].notna()\n",
    "perf_prs['is_open'] = perf_prs['state'] == 'open'\n",
    "\n",
    "# Calculate time metrics\n",
    "perf_prs['time_to_close_hours'] = (\n",
    "    perf_prs['closed_at'] - perf_prs['created_at']\n",
    ").dt.total_seconds() / 3600\n",
    "\n",
    "perf_prs['time_to_merge_hours'] = (\n",
    "    perf_prs['merged_at'] - perf_prs['created_at']\n",
    ").dt.total_seconds() / 3600\n",
    "\n",
    "perf_prs['time_to_close_days'] = perf_prs['time_to_close_hours'] / 24\n",
    "perf_prs['time_to_merge_days'] = perf_prs['time_to_merge_hours'] / 24\n",
    "\n",
    "# Text metrics\n",
    "perf_prs['title_length'] = perf_prs['title'].str.len()\n",
    "perf_prs['title_word_count'] = perf_prs['title'].str.split().str.len()\n",
    "perf_prs['body_length'] = perf_prs['body'].str.len()\n",
    "perf_prs['body_word_count'] = perf_prs['body'].str.split().str.len()\n",
    "perf_prs['has_body'] = perf_prs['body'].notna() & (perf_prs['body'].str.strip() != '')\n",
    "\n",
    "print(\"‚úì Feature engineering complete\")\n",
    "print(f\"  Merge rate: {perf_prs['is_merged'].mean()*100:.1f}%\")\n",
    "print(f\"  PRs with descriptions: {perf_prs['has_body'].sum():,} ({perf_prs['has_body'].mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Pattern Analysis (Agents vs Human)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Visualization Script: Optimization Pattern Analysis (Revised)\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import textwrap\n",
    "import os\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Load data if exits\n",
    "print(\"Loading data from csv files...\")\n",
    "if not (os.path.exists('datasets/ai_perf_prs_with_gpt_analysis.csv') and os.path.exists('datasets/human_perf_prs_with_gpt_analysis.csv')):\n",
    "    raise FileNotFoundError(\"Required CSV files not found. Please run the analysis notebook first.\")\n",
    "ai_perf_prs_analyzed = pd.read_csv('datasets/ai_perf_prs_with_gpt_analysis.csv')\n",
    "human_perf_prs_analyzed = pd.read_csv('datasets/human_perf_prs_with_gpt_analysis.csv')\n",
    "\n",
    "# Combine datasets\n",
    "analyzed = pd.concat([ai_perf_prs_analyzed, human_perf_prs_analyzed], ignore_index=True)\n",
    "print(f\"Combined dataset: {len(analyzed):,} performance PRs\")\n",
    "print(f\"  AI Agents: {(analyzed['author_type'] == 'AI Agent').sum():,}\")\n",
    "print(f\"  Humans: {(analyzed['author_type'] == 'Human').sum():,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Helper Functions\n",
    "# ============================================================================\n",
    "\n",
    "def wrap_labels(labels, width=30):\n",
    "    \"\"\"Wrap long labels for better readability\"\"\"\n",
    "    return [textwrap.fill(label, width) for label in labels]\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Pattern and Sub-Pattern Distribution (Single Figure per Author Type)\n",
    "# Using the same style as the provided script\n",
    "# ============================================================================\n",
    "\n",
    "def plot_pattern_subpattern_stacked(analyzed, author_type):\n",
    "    \"\"\"\n",
    "    Create stacked horizontal bar chart showing patterns and sub-patterns\n",
    "    Similar style to the provided script\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter by author type\n",
    "    subset = analyzed[analyzed['author_type'] == author_type].copy()\n",
    "    \n",
    "    # Get pattern and sub-pattern data\n",
    "    pattern_subpattern = subset.groupby(['optimization_pattern', 'optimization_subpattern']).size()\n",
    "    \n",
    "    # Build data dictionary\n",
    "    data = {}\n",
    "    for (pattern, subpattern), count in pattern_subpattern.items():\n",
    "        if pd.notna(pattern) and pd.notna(subpattern):\n",
    "            if pattern not in data:\n",
    "                data[pattern] = {}\n",
    "            data[pattern][subpattern] = count\n",
    "    \n",
    "    if not data:\n",
    "        print(f\"No data available for {author_type}\")\n",
    "        return\n",
    "    \n",
    "    # Sort patterns by total count\n",
    "    pattern_totals = {k: sum(v.values()) for k, v in data.items()}\n",
    "    sorted_patterns = sorted(pattern_totals.keys(), key=lambda x: pattern_totals[x], reverse=True)\n",
    "    data = {k: data[k] for k in sorted_patterns if k in data}\n",
    "    \n",
    "    # Short labels for patterns (y-axis)\n",
    "    short_labels = {}\n",
    "    for pattern in data.keys():\n",
    "        # Truncate long pattern names\n",
    "        if len(pattern) > 35:\n",
    "            short_labels[pattern] = pattern[:32] + '...'\n",
    "        else:\n",
    "            short_labels[pattern] = pattern\n",
    "    \n",
    "    # Short labels for sub-patterns (legend)\n",
    "    sub_short = {}\n",
    "    for pattern, subs in data.items():\n",
    "        for sub in subs.keys():\n",
    "            if len(sub) > 30:\n",
    "                sub_short[sub] = sub[:27] + '...'\n",
    "            else:\n",
    "                sub_short[sub] = sub\n",
    "    \n",
    "    # Color maps for each category (cycling through different colormaps)\n",
    "    available_colormaps = ['Blues', 'Greens', 'Oranges', 'Reds', 'Purples', 'YlOrBr', 'PuBu', 'RdPu']\n",
    "    colormaps = {}\n",
    "    for idx, pattern in enumerate(data.keys()):\n",
    "        num_subs = len(data[pattern])\n",
    "        colormaps[pattern] = (available_colormaps[idx % len(available_colormaps)], num_subs)\n",
    "    \n",
    "    # Flatten subpatterns\n",
    "    subpatterns = []\n",
    "    for cat, sub in data.items():\n",
    "        for sp in sub.keys():\n",
    "            subpatterns.append((cat, sp))\n",
    "    \n",
    "    # Build DataFrame\n",
    "    rows = []\n",
    "    for cat in data.keys():\n",
    "        row = [data[cat].get(sp, 0) for _, sp in subpatterns]\n",
    "        rows.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(rows,\n",
    "                      index=list(data.keys()),\n",
    "                      columns=[sp for _, sp in subpatterns])\n",
    "    \n",
    "    # Generate colors by category\n",
    "    colors = []\n",
    "    for cat, sp in subpatterns:\n",
    "        cmap_name, count = colormaps[cat]\n",
    "        cmap = plt.get_cmap(cmap_name)\n",
    "        idx = list(data[cat].keys()).index(sp)\n",
    "        colors.append(cmap((idx + 0.5) / max(count, 1)))\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(14, max(8, len(data) * 0.6)))\n",
    "    left = np.zeros(len(df))\n",
    "    y_labels = [short_labels[cat] for cat in df.index]\n",
    "    \n",
    "    for i, (_, sp) in enumerate(subpatterns):\n",
    "        ax.barh(y_labels, df.iloc[:, i], left=left,\n",
    "                label=sub_short[sp], color=colors[i])\n",
    "        left += df.iloc[:, i].values\n",
    "    \n",
    "    # Add total counts\n",
    "    totals = df.sum(axis=1).values\n",
    "    for i, total in enumerate(totals):\n",
    "        ax.text(total + max(totals)*0.01, i, str(int(total)), \n",
    "                va='center', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Final styling\n",
    "    ax.set_xlabel('Count', fontsize=17)\n",
    "    ax.set_title(f'Optimization Patterns: {author_type}', fontsize=18, fontweight='bold', pad=20)\n",
    "    ax.tick_params(axis='y', labelsize=16)\n",
    "    ax.tick_params(axis='x', labelsize=15)\n",
    "    ax.set_xlim(0, max(totals) * 1.15)\n",
    "    \n",
    "    # Legend at top center\n",
    "    ncol = min(4, (len(subpatterns) + 2) // 3)\n",
    "    legend = ax.legend(title='Sub-Patterns',\n",
    "                       loc='upper center',\n",
    "                       bbox_to_anchor=(0.5, 1.0),\n",
    "                       ncol=ncol,\n",
    "                       framealpha=0.9,\n",
    "                       fontsize=11,\n",
    "                       title_fontsize=13)\n",
    "    legend.get_frame().set_edgecolor('gray')\n",
    "    legend.get_frame().set_linewidth(0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save\n",
    "    filename = f'results/pattern/agent_human/optimization_pattern_{author_type.lower().replace(\" \", \"_\")}.png'\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"‚úì Saved: {filename}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Pattern Distribution Heatmap by Author Type\n",
    "# ============================================================================\n",
    "\n",
    "def plot_pattern_heatmap(analyzed):\n",
    "    \"\"\"Create heatmap showing pattern frequency by author type (percentages)\"\"\"\n",
    "    \n",
    "    # Create pivot table with percentages\n",
    "    pattern_pivot = pd.crosstab(\n",
    "        analyzed['optimization_pattern'],\n",
    "        analyzed['author_type'],\n",
    "        normalize='columns'\n",
    "    ) * 100\n",
    "    \n",
    "    # Sort by total frequency\n",
    "    pattern_totals = analyzed['optimization_pattern'].value_counts()\n",
    "    pattern_pivot = pattern_pivot.loc[pattern_totals.index]\n",
    "    \n",
    "    # Wrap labels\n",
    "    wrapped_labels = wrap_labels(pattern_pivot.index, width=40)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(10, max(8, len(pattern_pivot) * 0.4)))\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(\n",
    "        pattern_pivot,\n",
    "        annot=True,\n",
    "        fmt='.1f',\n",
    "        cmap='YlOrRd',\n",
    "        cbar_kws={'label': 'Percentage (%)'},\n",
    "        ax=ax,\n",
    "        linewidths=0.5,\n",
    "        linecolor='white'\n",
    "    )\n",
    "    \n",
    "    ax.set_title('Optimization Pattern Distribution by Author Type (%)', \n",
    "                 fontweight='bold', fontsize=14, pad=15)\n",
    "    ax.set_xlabel('Author Type', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Optimization Pattern', fontsize=12, fontweight='bold')\n",
    "    ax.set_yticklabels(wrapped_labels, rotation=0, fontsize=10)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=0, fontsize=11)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/pattern/agent_human/pattern_heatmap_by_author.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úì Saved: pattern_heatmap_by_author.png\")\n",
    "    \n",
    "    return pattern_pivot\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Statistical Comparison Table\n",
    "# ============================================================================\n",
    "\n",
    "def create_statistical_comparison_table(analyzed):\n",
    "    \"\"\"Create detailed statistical comparison table\"\"\"\n",
    "    \n",
    "    # Get all unique patterns\n",
    "    all_patterns = analyzed['optimization_pattern'].value_counts().index\n",
    "    \n",
    "    comparison_data = []\n",
    "    \n",
    "    for pattern in all_patterns:\n",
    "        ai_count = len(analyzed[(analyzed['author_type'] == 'AI Agent') & \n",
    "                                (analyzed['optimization_pattern'] == pattern)])\n",
    "        human_count = len(analyzed[(analyzed['author_type'] == 'Human') & \n",
    "                                   (analyzed['optimization_pattern'] == pattern)])\n",
    "        \n",
    "        total_ai = len(analyzed[analyzed['author_type'] == 'AI Agent'])\n",
    "        total_human = len(analyzed[analyzed['author_type'] == 'Human'])\n",
    "        \n",
    "        ai_pct = (ai_count / total_ai * 100) if total_ai > 0 else 0\n",
    "        human_pct = (human_count / total_human * 100) if total_human > 0 else 0\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'Pattern': pattern,\n",
    "            'AI Count': ai_count,\n",
    "            'AI %': ai_pct,\n",
    "            'Human Count': human_count,\n",
    "            'Human %': human_pct,\n",
    "            'Difference (AI - Human) %': ai_pct - human_pct\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    comparison_df = comparison_df.sort_values('AI Count', ascending=False)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, ax = plt.subplots(figsize=(16, max(10, len(comparison_df) * 0.5)))\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Wrap pattern names for table\n",
    "    comparison_df_display = comparison_df.copy()\n",
    "    comparison_df_display['Pattern'] = [textwrap.fill(p, width=50) for p in comparison_df_display['Pattern']]\n",
    "    \n",
    "    # Create table\n",
    "    table_data = comparison_df_display.round(1).values\n",
    "    table = ax.table(cellText=table_data,\n",
    "                    colLabels=comparison_df_display.columns,\n",
    "                    cellLoc='center',\n",
    "                    loc='center',\n",
    "                    colWidths=[0.4, 0.1, 0.1, 0.12, 0.1, 0.15])\n",
    "    \n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(9)\n",
    "    table.scale(1, 2.5)\n",
    "    \n",
    "    # Color code the difference column\n",
    "    for i in range(len(comparison_df)):\n",
    "        cell = table[(i+1, 5)]  # Difference column (0-indexed in data, +1 for header)\n",
    "        diff_val = comparison_df.iloc[i]['Difference (AI - Human) %']\n",
    "        if diff_val > 5:\n",
    "            cell.set_facecolor('#d4e6f1')  # Light blue for AI advantage\n",
    "        elif diff_val < -5:\n",
    "            cell.set_facecolor('#fadbd8')  # Light red for Human advantage\n",
    "        else:\n",
    "            cell.set_facecolor('#f8f9f9')  # Gray for similar\n",
    "    \n",
    "    # Header styling\n",
    "    for j in range(len(comparison_df_display.columns)):\n",
    "        cell = table[(0, j)]\n",
    "        cell.set_facecolor('#34495e')\n",
    "        cell.set_text_props(weight='bold', color='white')\n",
    "    \n",
    "    plt.title('Statistical Comparison: Optimization Patterns by Author Type', \n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "    \n",
    "    plt.savefig('results/pattern/agent_human/statistical_comparison_table.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úì Saved: statistical_comparison_table.png\")\n",
    "    \n",
    "    # Also save as CSV\n",
    "    comparison_df.to_csv('results/pattern/agent_human/pattern_comparison_statistics.csv', index=False)\n",
    "    print(\"‚úì Saved: pattern_comparison_statistics.csv\")\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Summary Statistics\n",
    "# ============================================================================\n",
    "\n",
    "def print_summary_statistics(analyzed):\n",
    "    \"\"\"Print summary statistics\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"OPTIMIZATION PATTERN ANALYSIS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nTotal PRs Analyzed: {len(analyzed):,}\")\n",
    "    print(f\"  AI Agents: {(analyzed['author_type'] == 'AI Agent').sum():,}\")\n",
    "    print(f\"  Humans: {(analyzed['author_type'] == 'Human').sum():,}\")\n",
    "    \n",
    "    print(f\"\\nUnique Patterns: {analyzed['optimization_pattern'].nunique()}\")\n",
    "    print(f\"Unique Sub-Patterns: {analyzed['optimization_subpattern'].nunique()}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"TOP 5 PATTERNS (OVERALL)\")\n",
    "    print(\"-\"*80)\n",
    "    top_patterns = analyzed['optimization_pattern'].value_counts().head(5)\n",
    "    for pattern, count in top_patterns.items():\n",
    "        pct = count / len(analyzed) * 100\n",
    "        print(f\"  {pattern[:60]:60s} {count:4d} ({pct:5.1f}%)\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"TOP 5 PATTERNS (AI AGENTS)\")\n",
    "    print(\"-\"*80)\n",
    "    ai_subset = analyzed[analyzed['author_type'] == 'AI Agent']\n",
    "    top_ai = ai_subset['optimization_pattern'].value_counts().head(5)\n",
    "    for pattern, count in top_ai.items():\n",
    "        pct = count / len(ai_subset) * 100\n",
    "        print(f\"  {pattern[:60]:60s} {count:4d} ({pct:5.1f}%)\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"TOP 5 PATTERNS (HUMANS)\")\n",
    "    print(\"-\"*80)\n",
    "    human_subset = analyzed[analyzed['author_type'] == 'Human']\n",
    "    top_human = human_subset['optimization_pattern'].value_counts().head(5)\n",
    "    for pattern, count in top_human.items():\n",
    "        pct = count / len(human_subset) * 100\n",
    "        print(f\"  {pattern[:60]:60s} {count:4d} ({pct:5.1f}%)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Main Execution\n",
    "# ============================================================================\n",
    "\n",
    "def generate_all_visualizations():\n",
    "    \"\"\"Generate all requested visualizations\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GENERATING VISUALIZATIONS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print_summary_statistics(analyzed)\n",
    "    \n",
    "    print(\"\\n1. Creating pattern-subpattern distribution for AI Agents...\")\n",
    "    plot_pattern_subpattern_stacked(analyzed, 'AI Agent')\n",
    "    \n",
    "    print(\"\\n2. Creating pattern-subpattern distribution for Humans...\")\n",
    "    plot_pattern_subpattern_stacked(analyzed, 'Human')\n",
    "    \n",
    "    print(\"\\n3. Creating pattern distribution heatmap...\")\n",
    "    pattern_pivot = plot_pattern_heatmap(analyzed)\n",
    "    \n",
    "    print(\"\\n4. Creating statistical comparison table...\")\n",
    "    comparison_df = create_statistical_comparison_table(analyzed)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úì ALL VISUALIZATIONS COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nGenerated files:\")\n",
    "    print(\"  1. optimization_pattern_ai_agent.png - Stacked pattern/sub-pattern for AI\")\n",
    "    print(\"  2. optimization_pattern_human.png - Stacked pattern/sub-pattern for Human\")\n",
    "    print(\"  3. pattern_heatmap_by_author.png - Pattern distribution heatmap\")\n",
    "    print(\"  4. statistical_comparison_table.png - Detailed comparison table\")\n",
    "    print(\"  5. pattern_comparison_statistics.csv - Data in CSV format\")\n",
    "    \n",
    "    return {\n",
    "        'pattern_pivot': pattern_pivot,\n",
    "        'comparison_df': comparison_df\n",
    "    }\n",
    "\n",
    "\n",
    "# Run all visualizations\n",
    "results = generate_all_visualizations()\n",
    "\n",
    "# Display comparison summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PATTERN DISTRIBUTION HEATMAP\")\n",
    "print(\"=\"*80)\n",
    "print(results['pattern_pivot'].round(1))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STATISTICAL COMPARISON (Top 10)\")\n",
    "print(\"=\"*80)\n",
    "print(results['comparison_df'].head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Pattern Analysis: between 5 AI Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Visualization Script: AI Agents Comparison - Optimization Pattern Analysis\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import textwrap\n",
    "import os\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Load data if exists\n",
    "print(\"Loading data from csv files...\")\n",
    "if not os.path.exists('datasets/ai_perf_prs_with_gpt_analysis.csv'):\n",
    "    raise FileNotFoundError(\"Required CSV file 'datasets/ai_perf_prs_with_gpt_analysis.csv' not found. Please run the analysis notebook first.\")\n",
    "\n",
    "ai_perf_prs_analyzed = pd.read_csv('datasets/ai_perf_prs_with_gpt_analysis.csv')\n",
    "\n",
    "# Filter only AI agents\n",
    "ai_agents = ai_perf_prs_analyzed[ai_perf_prs_analyzed['author_type'] == 'AI Agent'].copy()\n",
    "\n",
    "# Check for agent column\n",
    "if 'agent' not in ai_agents.columns:\n",
    "    raise ValueError(\"'agent' column not found in the dataset. Please ensure the data includes agent information.\")\n",
    "\n",
    "print(f\"AI Agent dataset: {len(ai_agents):,} performance PRs\")\n",
    "print(f\"\\nAgent distribution:\")\n",
    "for agent in ai_agents['agent'].unique():\n",
    "    count = (ai_agents['agent'] == agent).sum()\n",
    "    print(f\"  {agent}: {count:,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Helper Functions\n",
    "# ============================================================================\n",
    "\n",
    "def wrap_labels(labels, width=30):\n",
    "    \"\"\"Wrap long labels for better readability\"\"\"\n",
    "    return [textwrap.fill(label, width) for label in labels]\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Pattern Distribution Heatmap by AI Agent\n",
    "# ============================================================================\n",
    "\n",
    "def plot_pattern_heatmap(analyzed):\n",
    "    \"\"\"Create heatmap showing pattern frequency by AI agent (percentages)\"\"\"\n",
    "    \n",
    "    # Create pivot table with percentages\n",
    "    pattern_pivot = pd.crosstab(\n",
    "        analyzed['optimization_pattern'],\n",
    "        analyzed['agent'],\n",
    "        normalize='columns'\n",
    "    ) * 100\n",
    "    \n",
    "    # Sort by total frequency across all agents\n",
    "    pattern_totals = analyzed['optimization_pattern'].value_counts()\n",
    "    pattern_pivot = pattern_pivot.loc[pattern_totals.index]\n",
    "    \n",
    "    # Wrap labels\n",
    "    wrapped_labels = wrap_labels(pattern_pivot.index, width=40)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(12, max(8, len(pattern_pivot) * 0.4)))\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(\n",
    "        pattern_pivot,\n",
    "        annot=True,\n",
    "        fmt='.1f',\n",
    "        cmap='YlOrRd',\n",
    "        cbar_kws={'label': 'Percentage (%)'},\n",
    "        ax=ax,\n",
    "        linewidths=0.5,\n",
    "        linecolor='white'\n",
    "    )\n",
    "    \n",
    "    ax.set_title('Optimization Pattern Distribution by AI Agent (%)', \n",
    "                 fontweight='bold', fontsize=14, pad=15)\n",
    "    ax.set_xlabel('AI Agent', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Optimization Pattern', fontsize=12, fontweight='bold')\n",
    "    ax.set_yticklabels(wrapped_labels, rotation=0, fontsize=10)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, fontsize=11, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/pattern/between_agents/pattern_heatmap_by_agent.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úì Saved: pattern_heatmap_by_agent.png\")\n",
    "    \n",
    "    return pattern_pivot\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Statistical Comparison Table\n",
    "# ============================================================================\n",
    "\n",
    "def create_statistical_comparison_table(analyzed):\n",
    "    \"\"\"Create detailed statistical comparison table for all AI agents\"\"\"\n",
    "    \n",
    "    # Get all unique patterns\n",
    "    all_patterns = analyzed['optimization_pattern'].value_counts().index\n",
    "    agents = sorted(analyzed['agent'].unique())\n",
    "    \n",
    "    comparison_data = []\n",
    "    \n",
    "    for pattern in all_patterns:\n",
    "        row_data = {'Pattern': pattern}\n",
    "        \n",
    "        for agent in agents:\n",
    "            count = len(analyzed[(analyzed['agent'] == agent) & \n",
    "                                (analyzed['optimization_pattern'] == pattern)])\n",
    "            total_agent = len(analyzed[analyzed['agent'] == agent])\n",
    "            pct = (count / total_agent * 100) if total_agent > 0 else 0\n",
    "            \n",
    "            row_data[f'{agent} Count'] = count\n",
    "            row_data[f'{agent} %'] = pct\n",
    "        \n",
    "        comparison_data.append(row_data)\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    # Sort by most common pattern overall\n",
    "    pattern_totals = []\n",
    "    for idx, row in comparison_df.iterrows():\n",
    "        total = sum([row[f'{agent} Count'] for agent in agents])\n",
    "        pattern_totals.append(total)\n",
    "    comparison_df['Total'] = pattern_totals\n",
    "    comparison_df = comparison_df.sort_values('Total', ascending=False).drop('Total', axis=1)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, ax = plt.subplots(figsize=(18, max(10, len(comparison_df) * 0.5)))\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Wrap pattern names for table\n",
    "    comparison_df_display = comparison_df.copy()\n",
    "    comparison_df_display['Pattern'] = [textwrap.fill(p, width=40) for p in comparison_df_display['Pattern']]\n",
    "    \n",
    "    # Create table\n",
    "    table_data = comparison_df_display.round(1).values\n",
    "    table = ax.table(cellText=table_data,\n",
    "                    colLabels=comparison_df_display.columns,\n",
    "                    cellLoc='center',\n",
    "                    loc='center',\n",
    "                    colWidths=[0.25] + [0.075] * (len(agents) * 2))\n",
    "    \n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(8)\n",
    "    table.scale(1, 2.5)\n",
    "    \n",
    "    # Color code cells by value\n",
    "    for i in range(len(comparison_df)):\n",
    "        for j in range(1, len(comparison_df_display.columns)):\n",
    "            cell = table[(i+1, j)]\n",
    "            # Get percentage columns\n",
    "            if '% ' in comparison_df_display.columns[j] or comparison_df_display.columns[j].endswith('%'):\n",
    "                val = comparison_df.iloc[i, j]\n",
    "                if val > 15:\n",
    "                    cell.set_facecolor('#d4e6f1')  # Light blue for high\n",
    "                elif val > 5:\n",
    "                    cell.set_facecolor('#e8f4f8')  # Very light blue for medium\n",
    "                else:\n",
    "                    cell.set_facecolor('#f8f9f9')  # Gray for low\n",
    "    \n",
    "    # Header styling\n",
    "    for j in range(len(comparison_df_display.columns)):\n",
    "        cell = table[(0, j)]\n",
    "        cell.set_facecolor('#34495e')\n",
    "        cell.set_text_props(weight='bold', color='white')\n",
    "    \n",
    "    plt.title('Statistical Comparison: Optimization Patterns by AI Agent', \n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "    \n",
    "    plt.savefig('results/pattern/between_agents/statistical_comparison_table_agents.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úì Saved: statistical_comparison_table_agents.png\")\n",
    "    \n",
    "    # Also save as CSV\n",
    "    comparison_df.to_csv('results/pattern/between_agents/pattern_comparison_statistics_agents.csv', index=False)\n",
    "    print(\"‚úì Saved: pattern_comparison_statistics_agents.csv\")\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Summary Statistics\n",
    "# ============================================================================\n",
    "\n",
    "def print_summary_statistics(analyzed):\n",
    "    \"\"\"Print summary statistics\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"AI AGENTS OPTIMIZATION PATTERN ANALYSIS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nTotal AI Agent PRs Analyzed: {len(analyzed):,}\")\n",
    "    \n",
    "    agents = sorted(analyzed['agent'].unique())\n",
    "    for agent in agents:\n",
    "        count = (analyzed['agent'] == agent).sum()\n",
    "        pct = count / len(analyzed) * 100\n",
    "        print(f\"  {agent}: {count:,} ({pct:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nUnique Patterns: {analyzed['optimization_pattern'].nunique()}\")\n",
    "    print(f\"Unique Sub-Patterns: {analyzed['optimization_subpattern'].nunique()}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"TOP 5 PATTERNS (OVERALL)\")\n",
    "    print(\"-\"*80)\n",
    "    top_patterns = analyzed['optimization_pattern'].value_counts().head(5)\n",
    "    for pattern, count in top_patterns.items():\n",
    "        pct = count / len(analyzed) * 100\n",
    "        print(f\"  {pattern[:60]:60s} {count:4d} ({pct:5.1f}%)\")\n",
    "    \n",
    "    # Print top 5 for each agent\n",
    "    for agent in agents:\n",
    "        print(\"\\n\" + \"-\"*80)\n",
    "        print(f\"TOP 5 PATTERNS ({agent.upper()})\")\n",
    "        print(\"-\"*80)\n",
    "        agent_subset = analyzed[analyzed['agent'] == agent]\n",
    "        if len(agent_subset) > 0:\n",
    "            top_agent = agent_subset['optimization_pattern'].value_counts().head(5)\n",
    "            for pattern, count in top_agent.items():\n",
    "                pct = count / len(agent_subset) * 100\n",
    "                print(f\"  {pattern[:60]:60s} {count:4d} ({pct:5.1f}%)\")\n",
    "        else:\n",
    "            print(f\"  No data available\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Main Execution\n",
    "# ============================================================================\n",
    "\n",
    "def generate_all_visualizations():\n",
    "    \"\"\"Generate all requested visualizations\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GENERATING AI AGENT COMPARISON VISUALIZATIONS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print_summary_statistics(ai_agents)\n",
    "    \n",
    "    print(\"\\n1. Creating pattern distribution heatmap...\")\n",
    "    pattern_pivot = plot_pattern_heatmap(ai_agents)\n",
    "    \n",
    "    print(\"\\n2. Creating statistical comparison table...\")\n",
    "    comparison_df = create_statistical_comparison_table(ai_agents)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úì ALL VISUALIZATIONS COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nGenerated files:\")\n",
    "    print(\"  1. pattern_heatmap_by_agent.png - Pattern distribution heatmap\")\n",
    "    print(\"  2. statistical_comparison_table_agents.png - Detailed comparison table\")\n",
    "    print(\"  3. pattern_comparison_statistics_agents.csv - Data in CSV format\")\n",
    "    \n",
    "    return {\n",
    "        'pattern_pivot': pattern_pivot,\n",
    "        'comparison_df': comparison_df\n",
    "    }\n",
    "\n",
    "\n",
    "# Run all visualizations\n",
    "results = generate_all_visualizations()\n",
    "\n",
    "# Display comparison summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PATTERN DISTRIBUTION HEATMAP (ALL AGENTS)\")\n",
    "print(\"=\"*80)\n",
    "print(results['pattern_pivot'].round(1))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STATISTICAL COMPARISON (Top 10)\")\n",
    "print(\"=\"*80)\n",
    "print(results['comparison_df'].head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Load and Merge GPT Analysis Data\n",
    "# ============================================================================\n",
    "\n",
    "# Load analyzed performance PRs with GPT patterns\n",
    "print(\"Loading GPT-analyzed performance PRs...\")\n",
    "ai_perf_prs_analyzed = pd.read_csv('datasets/ai_perf_prs_with_gpt_analysis.csv')\n",
    "human_perf_prs_analyzed = pd.read_csv('datasets/human_perf_prs_with_gpt_analysis.csv')\n",
    "\n",
    "# Combine AI and Human analyzed datasets\n",
    "perf_prs_analyzed = pd.concat([ai_perf_prs_analyzed, human_perf_prs_analyzed], ignore_index=True)\n",
    "print(f\"  Combined: {len(perf_prs_analyzed):,} analyzed PRs\")\n",
    "\n",
    "# Define GPT analysis columns to merge\n",
    "gpt_analysis_cols = [\n",
    "    'id',\n",
    "    'gpt_success',\n",
    "    'gpt_explanation',\n",
    "    'gpt_comparison',\n",
    "    'optimization_pattern',\n",
    "    'optimization_subpattern'\n",
    "]\n",
    "\n",
    "# Select available columns\n",
    "available_gpt_cols = [col for col in gpt_analysis_cols if col in perf_prs_analyzed.columns]\n",
    "\n",
    "# Drop existing GPT columns from perf_prs to avoid conflicts\n",
    "existing_gpt_cols = [col for col in available_gpt_cols if col in perf_prs.columns and col != 'id']\n",
    "if existing_gpt_cols:\n",
    "    print(f\"  Dropping existing GPT columns: {existing_gpt_cols}\")\n",
    "    perf_prs = perf_prs.drop(columns=existing_gpt_cols)\n",
    "\n",
    "# Merge GPT analysis back into main perf_prs dataframe\n",
    "print(\"Merging GPT analysis into perf_prs...\")\n",
    "perf_prs = perf_prs.merge(\n",
    "    perf_prs_analyzed[available_gpt_cols],\n",
    "    on='id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"‚úì Merge complete\")\n",
    "print(f\"  Total PRs: {len(perf_prs):,}\")\n",
    "print(f\"  With GPT success: {perf_prs['gpt_success'].sum():,}\")\n",
    "print(f\"  With patterns: {perf_prs['optimization_pattern'].notna().sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and Evaluation Behavior [TODO]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description Quality [Needs to double check and find a better way to do it]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_description_quality(row):\n",
    "    \"\"\"Score description quality (0-5)\"\"\"\n",
    "    body = str(row['body']).lower()\n",
    "    \n",
    "    if pd.isna(row['body']) or body == 'nan' or len(body.strip()) < 50:\n",
    "        return 0\n",
    "    \n",
    "    score = 0\n",
    "    \n",
    "    # Problem statement\n",
    "    if any(word in body for word in ['problem', 'issue', 'slow', 'bottleneck', 'inefficient']):\n",
    "        score += 1\n",
    "    \n",
    "    # Solution description\n",
    "    if any(word in body for word in ['solution', 'implement', 'change', 'optimize', 'improve']):\n",
    "        score += 1\n",
    "    \n",
    "    # Measurements\n",
    "    if any(word in body for word in ['benchmark', 'test', 'measure', 'result']):\n",
    "        score += 1\n",
    "    \n",
    "    # Before/after comparison\n",
    "    if any(word in body for word in ['before', 'after', 'was', 'now', 'reduced', 'improved']):\n",
    "        score += 1\n",
    "    \n",
    "    return score\n",
    "\n",
    "perf_prs['description_quality_score'] = perf_prs.apply(assess_description_quality, axis=1)\n",
    "\n",
    "print(\"‚úì Description quality scored\")\n",
    "print(f\"  Avg quality score: {perf_prs['description_quality_score'].mean():.2f}/4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview: AI Agents vs Humans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High-level comparison \n",
    "comparison_stats = perf_prs.groupby('author_type').agg({\n",
    "    'id': 'count',\n",
    "    'is_merged': ['sum', 'mean'],\n",
    "    'time_to_merge_hours': ['mean', 'median'],\n",
    "    'has_body': 'mean',\n",
    "    'body_length': 'mean',\n",
    "    'description_quality_score': 'mean',\n",
    "    'optimization_pattern': 'nunique',  # Number of unique patterns used\n",
    "    'primary_language': 'nunique'\n",
    "}).round(2)\n",
    "\n",
    "comparison_stats.columns = [\n",
    "    'Total PRs', 'Merged PRs', 'Merge Rate', 'Avg Time to Merge (hrs)', \n",
    "    'Median Time to Merge (hrs)', 'Body Rate', 'Avg Body Length',\n",
    "    'Avg Quality Score', 'Unique Patterns', 'Unique Languages'\n",
    "]\n",
    "\n",
    "comparison_stats['Merge Rate (%)'] = (comparison_stats['Merge Rate'] * 100).round(1)\n",
    "comparison_stats['Body Rate (%)'] = (comparison_stats['Body Rate'] * 100).round(1)\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"PERFORMANCE PR COMPARISON: AI AGENTS VS HUMANS\")\n",
    "print(\"=\"*100)\n",
    "print(comparison_stats[[\n",
    "    'Total PRs', 'Merge Rate (%)', 'Median Time to Merge (hrs)',\n",
    "    'Body Rate (%)', 'Avg Body Length', 'Avg Quality Score',\n",
    "    'Unique Patterns', 'Unique Languages'\n",
    "]])\n",
    "\n",
    "# Statistical test for merge rate difference\n",
    "ai_merged = perf_prs[perf_prs['author_type'] == 'AI Agent']['is_merged']\n",
    "human_merged = perf_prs[perf_prs['author_type'] == 'Human']['is_merged']\n",
    "\n",
    "chi2, p_value = stats.chi2_contingency(\n",
    "    pd.crosstab(perf_prs['author_type'], perf_prs['is_merged'])\n",
    ")[:2]\n",
    "\n",
    "print(f\"\\nüìä Statistical Test (Merge Rate Difference):\")\n",
    "print(f\"  Chi-square: {chi2:.4f}\")\n",
    "print(f\"  P-value: {p_value:.6f}\")\n",
    "print(f\"  Significant: {'‚úì Yes' if p_value < 0.05 else '‚úó No'} (Œ±=0.05)\")\n",
    "\n",
    "diff = comparison_stats.loc['AI Agent', 'Merge Rate (%)'] - comparison_stats.loc['Human', 'Merge Rate (%)']\n",
    "print(f\"  Effect: AI agents {'+' if diff > 0 else ''}{diff:.1f}% merge rate vs humans\")\n",
    "\n",
    "# Additional pattern-based insights\n",
    "print(f\"\\nüéØ Optimization Pattern Insights:\")\n",
    "\n",
    "# Filter successful GPT analyses\n",
    "analyzed = perf_prs[perf_prs['gpt_success'] == True].copy()\n",
    "\n",
    "if len(analyzed) > 0:\n",
    "    # Most common patterns by author type\n",
    "    print(f\"\\nTop 3 Patterns by Author Type:\")\n",
    "    for author_type in ['AI Agent', 'Human']:\n",
    "        subset = analyzed[analyzed['author_type'] == author_type]\n",
    "        if len(subset) > 0:\n",
    "            top_patterns = subset['optimization_pattern'].value_counts().head(3)\n",
    "            print(f\"\\n  {author_type}:\")\n",
    "            for pattern, count in top_patterns.items():\n",
    "                pct = count / len(subset) * 100\n",
    "                # Truncate long pattern names\n",
    "                pattern_short = pattern[:50] + '...' if len(pattern) > 50 else pattern\n",
    "                print(f\"    {pattern_short:52s} {count:3d} ({pct:5.1f}%)\")\n",
    "    \n",
    "    # Pattern diversity comparison\n",
    "    ai_pattern_diversity = analyzed[analyzed['author_type'] == 'AI Agent']['optimization_pattern'].nunique()\n",
    "    human_pattern_diversity = analyzed[analyzed['author_type'] == 'Human']['optimization_pattern'].nunique()\n",
    "    \n",
    "    print(f\"\\nPattern Diversity:\")\n",
    "    print(f\"  AI Agents use {ai_pattern_diversity} unique patterns\")\n",
    "    print(f\"  Humans use {human_pattern_diversity} unique patterns\")\n",
    "    \n",
    "    # Success rate by pattern (top 5)\n",
    "    print(f\"\\nMerge Rate by Top 5 Optimization Patterns:\")\n",
    "    top_5_patterns = analyzed['optimization_pattern'].value_counts().head(5).index\n",
    "    \n",
    "    for pattern in top_5_patterns:\n",
    "        pattern_prs = analyzed[analyzed['optimization_pattern'] == pattern]\n",
    "        if len(pattern_prs) > 0:\n",
    "            merge_rate = pattern_prs['is_merged'].mean() * 100\n",
    "            pattern_short = pattern[:45] + '...' if len(pattern) > 45 else pattern\n",
    "            print(f\"  {pattern_short:47s} {merge_rate:5.1f}% ({len(pattern_prs):3d} PRs)\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n‚ö† No successful GPT analyses available for pattern insights\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize high-level comparison\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# 1. Sample size\n",
    "perf_prs.groupby('author_type').size().plot(kind='bar', ax=axes[0,0], color=['#3498db', '#e74c3c'])\n",
    "axes[0,0].set_title('Number of Performance PRs', fontweight='bold', fontsize=12)\n",
    "axes[0,0].set_xlabel('')\n",
    "axes[0,0].set_ylabel('Count')\n",
    "axes[0,0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# 2. Merge rate\n",
    "merge_rates = perf_prs.groupby('author_type')['is_merged'].mean() * 100\n",
    "merge_rates.plot(kind='bar', ax=axes[0,1], color=['#3498db', '#e74c3c'])\n",
    "axes[0,1].set_title('Merge Rate', fontweight='bold', fontsize=12)\n",
    "axes[0,1].set_xlabel('')\n",
    "axes[0,1].set_ylabel('Percentage')\n",
    "axes[0,1].set_ylim(0, 100)\n",
    "axes[0,1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# 3. Time to merge\n",
    "merged_prs = perf_prs[perf_prs['is_merged']]\n",
    "merged_prs.boxplot(column='time_to_merge_days', by='author_type', ax=axes[0,2])\n",
    "axes[0,2].set_title('Time to Merge Distribution', fontweight='bold', fontsize=12)\n",
    "axes[0,2].set_xlabel('')\n",
    "axes[0,2].set_ylabel('Days')\n",
    "plt.sca(axes[0,2])\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "# 4. Description quality\n",
    "quality_scores = perf_prs.groupby('author_type')['description_quality_score'].mean()\n",
    "quality_scores.plot(kind='bar', ax=axes[1,0], color=['#3498db', '#e74c3c'])\n",
    "axes[1,0].set_title('Avg Description Quality Score', fontweight='bold', fontsize=12)\n",
    "axes[1,0].set_xlabel('')\n",
    "axes[1,0].set_ylabel('Score (0-5)')\n",
    "axes[1,0].set_ylim(0, 5)\n",
    "axes[1,0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/ai_vs_human_overview.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Saved: ai_vs_human_overview.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ1: Adoption and Practices\n",
    "\n",
    "Analyzing PR size, description quality, and language usage patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ1.1: PR Size and Success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize by size\n",
    "perf_prs['pr_size_category'] = pd.cut(\n",
    "    perf_prs['body_length'],\n",
    "    bins=[0, 500, 1500, 5000, float('inf')],\n",
    "    labels=['Small (<500)', 'Medium (500-1500)', 'Large (1500-5000)', 'Very Large (>5000)']\n",
    ")\n",
    "\n",
    "# Analyze by author type and size\n",
    "size_analysis = perf_prs.groupby(['author_type', 'pr_size_category'], observed=True).agg({\n",
    "    'id': 'count',\n",
    "    'is_merged': 'mean',\n",
    "    'time_to_merge_hours': 'median'\n",
    "}).round(2)\n",
    "\n",
    "size_analysis.columns = ['Count', 'Merge Rate', 'Median Time (hrs)']\n",
    "size_analysis['Merge Rate (%)'] = (size_analysis['Merge Rate'] * 100).round(1)\n",
    "\n",
    "print(\"PR Size vs Success (AI vs Humans):\")\n",
    "print(\"=\"*80)\n",
    "print(size_analysis[['Count', 'Merge Rate (%)', 'Median Time (hrs)']])\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Size distribution\n",
    "size_dist = pd.crosstab(perf_prs['author_type'], perf_prs['pr_size_category'], normalize='index') * 100\n",
    "size_dist.T.plot(kind='bar', ax=axes[0], color=['#3498db', '#e74c3c'])\n",
    "axes[0].set_title('PR Size Distribution', fontweight='bold')\n",
    "axes[0].set_xlabel('Size Category')\n",
    "axes[0].set_ylabel('Percentage')\n",
    "axes[0].legend(title='Author Type')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Merge rate by size\n",
    "merge_by_size = perf_prs.groupby(['author_type', 'pr_size_category'], observed=True)['is_merged'].mean() * 100\n",
    "merge_by_size.unstack().T.plot(kind='bar', ax=axes[1], color=['#3498db', '#e74c3c'])\n",
    "axes[1].set_title('Merge Rate by PR Size', fontweight='bold')\n",
    "axes[1].set_xlabel('Size Category')\n",
    "axes[1].set_ylabel('Merge Rate (%)')\n",
    "axes[1].set_ylim(0, 100)\n",
    "axes[1].legend(title='Author Type')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/rq1/rq1_pr_size_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ1.2: Description Quality Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality score distribution\n",
    "quality_comparison = perf_prs.groupby(['author_type', 'description_quality_score']).agg({\n",
    "    'id': 'count',\n",
    "    'is_merged': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "quality_comparison.columns = ['Count', 'Merge Rate']\n",
    "quality_comparison['Merge Rate (%)'] = (quality_comparison['Merge Rate'] * 100).round(1)\n",
    "\n",
    "print(\"Description Quality Score vs Merge Rate:\")\n",
    "print(\"=\"*80)\n",
    "print(quality_comparison[['Count', 'Merge Rate (%)']])\n",
    "\n",
    "# Statistical test\n",
    "for author_type in ['AI Agent', 'Human']:\n",
    "    subset = perf_prs[perf_prs['author_type'] == author_type]\n",
    "    corr = subset[['description_quality_score', 'is_merged']].corr().iloc[0, 1]\n",
    "    print(f\"\\n{author_type} - Quality ‚Üî Merge correlation: {corr:.3f}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Quality score distribution\n",
    "perf_prs.groupby(['author_type', 'description_quality_score']).size().unstack(fill_value=0).T.plot(\n",
    "    kind='bar', ax=axes[0], color=['#3498db', '#e74c3c']\n",
    ")\n",
    "axes[0].set_title('Description Quality Distribution', fontweight='bold')\n",
    "axes[0].set_xlabel('Quality Score')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].legend(title='Author Type')\n",
    "\n",
    "# Merge rate by quality\n",
    "(perf_prs.groupby(['author_type', 'description_quality_score'])['is_merged'].mean().unstack() * 100).T.plot(\n",
    "    kind='line', marker='o', ax=axes[1], color=['#3498db', '#e74c3c'], linewidth=2, markersize=8\n",
    ")\n",
    "axes[1].set_title('Merge Rate by Quality Score', fontweight='bold')\n",
    "axes[1].set_xlabel('Quality Score')\n",
    "axes[1].set_ylabel('Merge Rate (%)')\n",
    "axes[1].set_ylim(0, 100)\n",
    "axes[1].legend(title='Author Type')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/rq1/rq1_quality_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ1.3: Programming Language Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language frequency by author type\n",
    "print(\"Top 10 Languages by Author Type:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for author_type in ['AI Agent', 'Human']:\n",
    "    subset = perf_prs[perf_prs['author_type'] == author_type]\n",
    "    lang_counts = subset['primary_language'].value_counts().head(10)\n",
    "    \n",
    "    print(f\"\\n{author_type}:\")\n",
    "    for lang, count in lang_counts.items():\n",
    "        pct = count / len(subset) * 100\n",
    "        merge_rate = subset[subset['primary_language'] == lang]['is_merged'].mean() * 100\n",
    "        print(f\"  {lang:15s} {count:4d} ({pct:5.1f}%) - Merge: {merge_rate:.1f}%\")\n",
    "\n",
    "# Compare top languages\n",
    "all_top_langs = list(set(\n",
    "    perf_prs[perf_prs['author_type'] == 'AI Agent']['primary_language'].value_counts().head(8).index.tolist() +\n",
    "    perf_prs[perf_prs['author_type'] == 'Human']['primary_language'].value_counts().head(8).index.tolist()\n",
    "))\n",
    "\n",
    "lang_comparison = []\n",
    "for lang in all_top_langs:\n",
    "    for author_type in ['AI Agent', 'Human']:\n",
    "        subset = perf_prs[(perf_prs['author_type'] == author_type) & (perf_prs['primary_language'] == lang)]\n",
    "        if len(subset) > 0:\n",
    "            lang_comparison.append({\n",
    "                'Language': lang,\n",
    "                'Author Type': author_type,\n",
    "                'Count': len(subset),\n",
    "                'Merge Rate (%)': subset['is_merged'].mean() * 100\n",
    "            })\n",
    "\n",
    "lang_df = pd.DataFrame(lang_comparison)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Count comparison\n",
    "lang_pivot_count = lang_df.pivot(index='Language', columns='Author Type', values='Count').fillna(0)\n",
    "lang_pivot_count.plot(kind='barh', ax=axes[0], color=['#3498db', '#e74c3c'])\n",
    "axes[0].set_title('Language Usage: AI vs Human', fontweight='bold')\n",
    "axes[0].set_xlabel('Number of PRs')\n",
    "axes[0].legend(title='Author Type')\n",
    "\n",
    "# Merge rate comparison\n",
    "lang_pivot_merge = lang_df.pivot(index='Language', columns='Author Type', values='Merge Rate (%)')\n",
    "lang_pivot_merge.plot(kind='barh', ax=axes[1], color=['#3498db', '#e74c3c'])\n",
    "axes[1].set_title('Merge Rate by Language: AI vs Human', fontweight='bold')\n",
    "axes[1].set_xlabel('Merge Rate (%)')\n",
    "axes[1].set_xlim(0, 100)\n",
    "axes[1].legend(title='Author Type')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/rq1/rq1_language_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ2: Optimization Patch Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RQ2: Optimization Patch Characteristics\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"OPTIMIZATION PATTERN ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Filter PRs with successful GPT analysis\n",
    "analyzed = perf_prs[perf_prs['gpt_success'] == True].copy()\n",
    "\n",
    "print(f\"\\nAnalyzing {len(analyzed):,} PRs with GPT pattern classification\")\n",
    "print(f\"  AI Agents: {(analyzed['author_type'] == 'AI Agent').sum():,}\")\n",
    "print(f\"  Humans: {(analyzed['author_type'] == 'Human').sum():,}\")\n",
    "\n",
    "# Most common optimization patterns\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Most Common Optimization Patterns:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for author_type in ['AI Agent', 'Human']:\n",
    "    subset = analyzed[analyzed['author_type'] == author_type]\n",
    "    pattern_counts = subset['optimization_pattern'].value_counts()\n",
    "    \n",
    "    print(f\"\\n{author_type} (top 10):\")\n",
    "    for pattern, count in pattern_counts.head(10).items():\n",
    "        pct = count / len(subset) * 100\n",
    "        pattern_short = pattern[:50] + '...' if len(pattern) > 50 else pattern\n",
    "        print(f\"  {pattern_short:52s} {count:4d} ({pct:5.1f}%)\")\n",
    "\n",
    "# Most common sub-patterns\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Most Common Sub-Patterns:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for author_type in ['AI Agent', 'Human']:\n",
    "    subset = analyzed[analyzed['author_type'] == author_type]\n",
    "    subpattern_counts = subset['optimization_subpattern'].value_counts()\n",
    "    \n",
    "    print(f\"\\n{author_type} (top 10):\")\n",
    "    for subpattern, count in subpattern_counts.head(10).items():\n",
    "        pct = count / len(subset) * 100\n",
    "        subpattern_short = subpattern[:50] + '...' if len(subpattern) > 50 else subpattern\n",
    "        print(f\"  {subpattern_short:52s} {count:4d} ({pct:5.1f}%)\")\n",
    "\n",
    "# Pattern diversity comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Pattern Diversity Metrics:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "diversity_stats = analyzed.groupby('author_type').agg({\n",
    "    'optimization_pattern': ['nunique', 'count'],\n",
    "    'optimization_subpattern': 'nunique'\n",
    "}).round(2)\n",
    "\n",
    "diversity_stats.columns = ['Unique Patterns', 'Total PRs', 'Unique Sub-Patterns']\n",
    "diversity_stats['Patterns per PR'] = (diversity_stats['Unique Patterns'] / diversity_stats['Total PRs']).round(3)\n",
    "\n",
    "print(diversity_stats)\n",
    "\n",
    "# Statistical test for pattern diversity\n",
    "ai_patterns = analyzed[analyzed['author_type'] == 'AI Agent']['optimization_pattern']\n",
    "human_patterns = analyzed[analyzed['author_type'] == 'Human']['optimization_pattern']\n",
    "\n",
    "# Pattern distribution comparison using Chi-square test\n",
    "if len(ai_patterns) > 0 and len(human_patterns) > 0:\n",
    "    # Get common patterns that appear in both groups\n",
    "    common_patterns = set(ai_patterns.unique()) & set(human_patterns.unique())\n",
    "    \n",
    "    if len(common_patterns) > 1:\n",
    "        # Filter to common patterns only\n",
    "        ai_filtered = ai_patterns[ai_patterns.isin(common_patterns)]\n",
    "        human_filtered = human_patterns[human_patterns.isin(common_patterns)]\n",
    "        \n",
    "        # Create contingency table\n",
    "        pattern_crosstab = pd.crosstab(\n",
    "            analyzed[analyzed['optimization_pattern'].isin(common_patterns)]['author_type'],\n",
    "            analyzed[analyzed['optimization_pattern'].isin(common_patterns)]['optimization_pattern']\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            chi2, p_value = stats.chi2_contingency(pattern_crosstab)[:2]\n",
    "            print(f\"\\nChi-square test (pattern distribution):\")\n",
    "            print(f\"  Chi-square: {chi2:.4f}\")\n",
    "            print(f\"  P-value: {p_value:.6f}\")\n",
    "            print(f\"  Significant: {'‚úì Yes' if p_value < 0.05 else '‚úó No'} (Œ±=0.05)\")\n",
    "            \n",
    "            if p_value < 0.05:\n",
    "                print(f\"  ‚Üí AI agents and humans show significantly different pattern preferences\")\n",
    "            else:\n",
    "                print(f\"  ‚Üí AI agents and humans show similar pattern distributions\")\n",
    "        except:\n",
    "            print(\"\\n‚ö† Unable to perform chi-square test (insufficient data)\")\n",
    "\n",
    "# Pattern co-occurrence analysis\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Key Insights:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for author_type in ['AI Agent', 'Human']:\n",
    "    subset = analyzed[analyzed['author_type'] == author_type]\n",
    "    \n",
    "    print(f\"\\n{author_type}:\")\n",
    "    \n",
    "    # Most common pattern\n",
    "    top_pattern = subset['optimization_pattern'].value_counts().index[0]\n",
    "    top_count = subset['optimization_pattern'].value_counts().values[0]\n",
    "    print(f\"  Most common pattern: {top_pattern}\")\n",
    "    print(f\"    {top_count} PRs ({top_count/len(subset)*100:.1f}%)\")\n",
    "    \n",
    "    # Most common sub-pattern\n",
    "    top_subpattern = subset['optimization_subpattern'].value_counts().index[0]\n",
    "    top_sub_count = subset['optimization_subpattern'].value_counts().values[0]\n",
    "    print(f\"  Most common sub-pattern: {top_subpattern}\")\n",
    "    print(f\"    {top_sub_count} PRs ({top_sub_count/len(subset)*100:.1f}%)\")\n",
    "    \n",
    "    # Pattern diversity\n",
    "    unique_patterns = subset['optimization_pattern'].nunique()\n",
    "    unique_subpatterns = subset['optimization_subpattern'].nunique()\n",
    "    print(f\"  Pattern diversity: {unique_patterns} patterns, {unique_subpatterns} sub-patterns\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ3: Testing and Evaluation Behavior [TODO]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ4: Review Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review time analysis\n",
    "merged_prs = perf_prs[perf_prs['is_merged']].copy()\n",
    "\n",
    "review_stats = merged_prs.groupby('author_type')['time_to_merge_hours'].describe()\n",
    "print(\"Review Time Statistics (Merged PRs):\")\n",
    "print(\"=\"*80)\n",
    "print(review_stats)\n",
    "\n",
    "# Statistical test\n",
    "ai_times = merged_prs[merged_prs['author_type'] == 'AI Agent']['time_to_merge_hours'].dropna()\n",
    "human_times = merged_prs[merged_prs['author_type'] == 'Human']['time_to_merge_hours'].dropna()\n",
    "\n",
    "if len(ai_times) > 0 and len(human_times) > 0:\n",
    "    statistic, p_value = stats.mannwhitneyu(ai_times, human_times, alternative='two-sided')\n",
    "    print(f\"\\nMann-Whitney U test (time to merge):\")\n",
    "    print(f\"  AI Agent median: {ai_times.median():.1f} hours ({ai_times.median()/24:.1f} days)\")\n",
    "    print(f\"  Human median: {human_times.median():.1f} hours ({human_times.median()/24:.1f} days)\")\n",
    "    print(f\"  P-value: {p_value:.6f}\")\n",
    "    print(f\"  Significant: {'Yes' if p_value < 0.05 else 'No'} (Œ±=0.05)\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "merged_prs.boxplot(column='time_to_merge_days', by='author_type', ax=ax)\n",
    "ax.set_title('Time to Merge Distribution: AI vs Human', fontweight='bold', fontsize=14)\n",
    "ax.set_xlabel('Author Type', fontsize=12)\n",
    "ax.set_ylabel('Days to Merge', fontsize=12)\n",
    "ax.set_ylim(0, merged_prs['time_to_merge_days'].quantile(0.95))\n",
    "plt.suptitle('')\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/rq4/rq4_review_time_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ5: Failure Patterns [Needs to update]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Failure analysis\n",
    "failed_prs = perf_prs[(perf_prs['state'] == 'closed') & (~perf_prs['is_merged'])].copy()\n",
    "successful_prs = perf_prs[perf_prs['is_merged']].copy()\n",
    "\n",
    "print(\"Failure Analysis by Author Type:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "failure_stats = perf_prs.groupby('author_type').agg({\n",
    "    'id': 'count',\n",
    "    'is_merged': 'mean'\n",
    "}).round(3)\n",
    "failure_stats.columns = ['Total', 'Success Rate']\n",
    "failure_stats['Failure Rate (%)'] = ((1 - failure_stats['Success Rate']) * 100).round(1)\n",
    "\n",
    "print(failure_stats[['Total', 'Failure Rate (%)']])\n",
    "\n",
    "# Characteristic comparison\n",
    "comparison = []\n",
    "for author_type in ['AI Agent', 'Human']:\n",
    "    success = successful_prs[successful_prs['author_type'] == author_type]\n",
    "    fail = failed_prs[failed_prs['author_type'] == author_type]\n",
    "    \n",
    "    comparison.append({\n",
    "        'Author Type': author_type,\n",
    "        'Outcome': 'Successful',\n",
    "        'Avg Body Length': success['body_length'].mean(),\n",
    "        'Has Body (%)': success['has_body'].mean() * 100,\n",
    "        'Avg Quality Score': success['description_quality_score'].mean(),\n",
    "    })\n",
    "    \n",
    "    if len(fail) > 0:\n",
    "        comparison.append({\n",
    "            'Author Type': author_type,\n",
    "            'Outcome': 'Failed',\n",
    "            'Avg Body Length': fail['body_length'].mean(),\n",
    "            'Has Body (%)': fail['has_body'].mean() * 100,\n",
    "            'Avg Quality Score': fail['description_quality_score'].mean(),\n",
    "        })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison).round(2)\n",
    "\n",
    "print(\"\\nCharacteristic Comparison: Successful vs Failed PRs\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df)\n",
    "\n",
    "# Key risk factors\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY RISK FACTORS:\")\n",
    "\n",
    "for author_type in ['AI Agent', 'Human']:\n",
    "    subset = perf_prs[perf_prs['author_type'] == author_type]\n",
    "    print(f\"\\n{author_type}:\")\n",
    "    \n",
    "    # Missing description\n",
    "    no_body_fail = 1 - subset[~subset['has_body']]['is_merged'].mean()\n",
    "    has_body_fail = 1 - subset[subset['has_body']]['is_merged'].mean()\n",
    "    if not pd.isna(no_body_fail) and not pd.isna(has_body_fail):\n",
    "        print(f\"  Missing description: +{(no_body_fail - has_body_fail)*100:.1f}% failure rate\")\n",
    "    \n",
    "    # Low quality\n",
    "    low_qual = subset[subset['description_quality_score'] < 2]\n",
    "    high_qual = subset[subset['description_quality_score'] >= 3]\n",
    "    if len(low_qual) > 0 and len(high_qual) > 0:\n",
    "        low_fail = 1 - low_qual['is_merged'].mean()\n",
    "        high_fail = 1 - high_qual['is_merged'].mean()\n",
    "        print(f\"  Low quality (score <2): +{(low_fail - high_fail)*100:.1f}% failure rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"PERFORMANCE PR ANALYSIS: KEY FINDINGS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Calculate key metrics\n",
    "ai_stats = perf_prs[perf_prs['author_type'] == 'AI Agent']\n",
    "human_stats = perf_prs[perf_prs['author_type'] == 'Human']\n",
    "\n",
    "print(f\"\\n1. OVERALL SUCCESS\")\n",
    "print(f\"   AI Agents: {ai_stats['is_merged'].mean()*100:.1f}% merge rate ({ai_stats['is_merged'].sum():,}/{len(ai_stats):,})\")\n",
    "print(f\"   Humans: {human_stats['is_merged'].mean()*100:.1f}% merge rate ({human_stats['is_merged'].sum():,}/{len(human_stats):,})\")\n",
    "diff = ai_stats['is_merged'].mean() - human_stats['is_merged'].mean()\n",
    "print(f\"   Difference: {'+' if diff > 0 else ''}{diff*100:.1f}% points\")\n",
    "\n",
    "print(f\"\\n2. DESCRIPTION QUALITY\")\n",
    "print(f\"   AI Agents: {ai_stats['description_quality_score'].mean():.2f}/5 avg quality\")\n",
    "print(f\"   Humans: {human_stats['description_quality_score'].mean():.2f}/5 avg quality\")\n",
    "print(f\"   AI with detailed descriptions: {ai_stats['has_body'].mean()*100:.1f}%\")\n",
    "print(f\"   Humans with detailed descriptions: {human_stats['has_body'].mean()*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n3. TESTING & MEASUREMENT\")\n",
    "print(f\"TODO: Add testing & measurement statistics here\")\n",
    "\n",
    "print(f\"\\n4. REVIEW TIME\")\n",
    "ai_merged = ai_stats[ai_stats['is_merged']]\n",
    "human_merged = human_stats[human_stats['is_merged']]\n",
    "if len(ai_merged) > 0:\n",
    "    print(f\"   AI Agents median: {ai_merged['time_to_merge_hours'].median():.1f} hours ({ai_merged['time_to_merge_days'].median():.1f} days)\")\n",
    "if len(human_merged) > 0:\n",
    "    print(f\"   Humans median: {human_merged['time_to_merge_hours'].median():.1f} hours ({human_merged['time_to_merge_days'].median():.1f} days)\")\n",
    "\n",
    "print(f\"\\n5. TOP LANGUAGES\")\n",
    "print(f\"   AI Agents: {', '.join(ai_stats['primary_language'].value_counts().head(3).index.tolist())}\")\n",
    "print(f\"   Humans: {', '.join(human_stats['primary_language'].value_counts().head(3).index.tolist())}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export datasets\n",
    "raw_export_cols = [\n",
    "    'id', 'number', 'title', 'body', 'classification_reason', 'agent', 'author_type',\n",
    "    'state', 'created_at', 'merged_at', 'is_merged', 'primary_language', 'repo_id', 'html_url'\n",
    "]\n",
    "\n",
    "# Ground-truth subset (no engineered annotations)\n",
    "perf_prs[raw_export_cols].to_csv('datasets/performance_prs_ai_vs_human_raw.csv', index=False)\n",
    "print(\"‚úì Exported: performance_prs_ai_vs_human_raw.csv (ground-truth subset)\")\n",
    "\n",
    "# Annotated dataset with engineered features\n",
    "export_cols = [\n",
    "    'id', 'number', 'title', 'body', 'classification_reason', 'agent', 'author_type', 'state',\n",
    "    'created_at', 'merged_at', 'is_merged', 'time_to_merge_days',\n",
    "    'primary_language', 'repo_id', 'html_url',\n",
    "    'has_body', 'body_length', 'description_quality_score',\n",
    "    'optimization_pattern', 'optimization_subpattern',\n",
    "    'gpt_explanation', 'gpt_comparison'\n",
    "]\n",
    "\n",
    "perf_prs[export_cols].to_csv('results/performance_prs_ai_vs_human.csv', index=False)\n",
    "print(\"‚úì Exported: performance_prs_ai_vs_human.csv (annotated dataset)\")\n",
    "\n",
    "# Export summary statistics\n",
    "comparison_stats.to_csv('results/summary_ai_vs_human.csv')\n",
    "print(\"‚úì Exported: summary_ai_vs_human.csv\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Generated files:\")\n",
    "print(f\"  ‚Ä¢ performance_prs_ai_vs_human_raw.csv - Ground-truth subset\")\n",
    "print(f\"  ‚Ä¢ performance_prs_ai_vs_human.csv - Annotated dataset\")\n",
    "print(f\"  ‚Ä¢ summary_ai_vs_human.csv - Summary statistics\")\n",
    "print(f\"  ‚Ä¢ ai_vs_human_overview.png - Overview comparison\")\n",
    "print(f\"  ‚Ä¢ rq1_pr_size_comparison.png - PR size analysis\")\n",
    "print(f\"  ‚Ä¢ rq1_quality_comparison.png - Quality analysis\")\n",
    "print(f\"  ‚Ä¢ rq1_language_comparison.png - Language analysis\")\n",
    "print(f\"  ‚Ä¢ rq3_testing_comparison.png - Testing practices\")\n",
    "print(f\"  ‚Ä¢ rq4_review_time_comparison.png - Review dynamics\")\n",
    "print(f\"Total performance PRs analyzed: {len(perf_prs):,}\")\n",
    "print(f\"  AI Agents: {len(ai_perf_prs):,}\")\n",
    "print(f\"  Humans: {len(human_perf_prs):,}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
