{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance PR Analysis: AI Agents vs Humans\n",
    "\n",
    "## Comprehensive Analysis of Performance Optimization PRs\n",
    "\n",
    "This notebook addresses all research questions comparing AI coding agents and human developers on performance optimization tasks:\n",
    "\n",
    "### 1. **Adoption and Practices**\n",
    "\n",
    "- What practices (e.g., PR size, task type, and commit granularity) correlate with the quality of **performance optimization commits**?\n",
    "    - How can these practices inform **guidelines for developers and AI agents** to produce effective and efficient performance-improving patches?\n",
    "- Which programming languages most frequently appear in performance-improving tasks?\n",
    "\n",
    "### 2. **Optimization Patch Characteristics**\n",
    "\n",
    "- How do performance-oriented code patches modify software structure (e.g., changes in loops, data structures, parallelization, or algorithmic complexity; additions, deletions, files touched)?\n",
    "- What are the implications for maintainability?\n",
    "\n",
    "### 3. **Testing and Evaluation Behavior**\n",
    "\n",
    "- How do AI agents **measure and validate performance improvements** (e.g., benchmarks, profiling metrics, unit tests)?\n",
    "- How frequently and systematically are performance tests executed after code optimization?\n",
    "\n",
    "### 4. **Review Dynamics**\n",
    "\n",
    "- Do **performance-optimizing pull requests** receive distinctive review attention (e.g., profiling validation, discussion of tradeoffs)?\n",
    "- What kinds of review comments (e.g., correctness, maintainability, performance tradeoffs) are most associated with optimization patches?\n",
    "\n",
    "### 5. **Failure Patterns and Risks**\n",
    "\n",
    "- What **common failure patterns** and code quality issues appear in optimization-oriented PRs?\n",
    "- Why do these failures occur, and how can insights from them be used to **improve automated optimization systems and human–AI collaboration**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Environment ready!\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "!pip install pandas numpy matplotlib seaborn scipy wordcloud pyarrow datasets --quiet\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import re\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "# Plot settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Environment ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading AIDev datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/peng397/.pyenv/versions/3.10.0/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Performance PR IDs to process: 428\n",
      "\n",
      "Processing commit details (filtering to performance PRs only)...\n",
      "  Total commit records in dataset: 711,923\n",
      "  Filtered to performance PRs: 7,410 commit records\n",
      "  Unique performance PRs with commits: 340\n",
      "  ✓ Aggregated to 340 unique performance PRs\n",
      "  Avg commits per PR: 21.8\n",
      "  AI Agent PRs with commit data: 340 / 340 (100.0%)\n",
      "  Human PRs with commit data: 0 / 88 (0.0%)\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "✓ AI Agent Performance PRs: 340\n",
      "✓ Human Performance PRs: 88\n",
      "✓ Total Performance PRs: 428\n",
      "\n",
      "AI Agent Distribution:\n",
      "  OpenAI_Codex           207 ( 60.9%)\n",
      "  Devin                   62 ( 18.2%)\n",
      "  Copilot                 44 ( 12.9%)\n",
      "  Cursor                  24 (  7.1%)\n",
      "  Claude_Code              3 (  0.9%)\n",
      "\n",
      "================================================================================\n",
      "COMMIT STATISTICS\n",
      "================================================================================\n",
      "\n",
      "AI Agent:\n",
      "  PRs with commit data: 340 (100.0%)\n",
      "  Avg commits per PR: 21.8\n",
      "  Median commits per PR: 4.0\n",
      "  Avg additions: 646 lines\n",
      "  Median additions: 72 lines\n",
      "  Avg deletions: 646 lines\n",
      "  Median deletions: 26 lines\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "print(\"Loading AIDev datasets...\")\n",
    "\n",
    "# AI Agent PRs\n",
    "pr_df = pd.read_parquet(\"hf://datasets/hao-li/AIDev/pull_request.parquet\")\n",
    "pr_task_type_df = pd.read_parquet(\"hf://datasets/hao-li/AIDev/pr_task_type.parquet\")\n",
    "ai_perf_prs = (\n",
    "    pr_df\n",
    "    .merge(\n",
    "        pr_task_type_df[[\"id\", \"type\", \"reason\"]],\n",
    "        on=\"id\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    .query(\"type == 'perf'\")\n",
    "    .copy()\n",
    ")\n",
    "ai_perf_prs['classification_reason'] = ai_perf_prs['reason']\n",
    "ai_perf_prs['author_type'] = 'AI Agent'\n",
    "\n",
    "# Human PRs\n",
    "human_pr_df = pd.read_parquet(\"hf://datasets/hao-li/AIDev/human_pull_request.parquet\")\n",
    "human_pr_task_type_df = pd.read_parquet(\"hf://datasets/hao-li/AIDev/human_pr_task_type.parquet\")\n",
    "human_perf_prs = (\n",
    "    human_pr_df\n",
    "    .merge(\n",
    "        human_pr_task_type_df[[\"id\", \"type\", \"reason\"]],\n",
    "        on=\"id\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    .query(\"type == 'perf'\")\n",
    "    .copy()\n",
    ")\n",
    "human_perf_prs['classification_reason'] = human_perf_prs['reason']\n",
    "human_perf_prs['author_type'] = 'Human'\n",
    "human_perf_prs['agent'] = 'Human'  # Add agent column for consistency\n",
    "\n",
    "# Repository data for language info\n",
    "all_repo_df = pd.read_parquet(\"hf://datasets/hao-li/AIDev/all_repository.parquet\")\n",
    "\n",
    "# Get list of performance PR IDs we care about\n",
    "perf_pr_ids = set(ai_perf_prs['id'].tolist() + human_perf_prs['id'].tolist())\n",
    "print(f\"\\n✓ Performance PR IDs to process: {len(perf_pr_ids):,}\")\n",
    "\n",
    "# PR commits details - FILTER FIRST, then aggregate\n",
    "print(\"\\nProcessing commit details (filtering to performance PRs only)...\")\n",
    "pr_commits_details = pd.read_parquet(\"hf://datasets/hao-li/AIDev/pr_commit_details.parquet\")\n",
    "\n",
    "if 'pr_id' in pr_commits_details.columns:\n",
    "    print(f\"  Total commit records in dataset: {len(pr_commits_details):,}\")\n",
    "    \n",
    "    # FILTER: Keep only commits for performance PRs\n",
    "    pr_commits_filtered = pr_commits_details[pr_commits_details['pr_id'].isin(perf_pr_ids)].copy()\n",
    "    print(f\"  Filtered to performance PRs: {len(pr_commits_filtered):,} commit records\")\n",
    "    print(f\"  Unique performance PRs with commits: {pr_commits_filtered['pr_id'].nunique():,}\")\n",
    "    \n",
    "    if len(pr_commits_filtered) > 0:\n",
    "        # AGGREGATE: Now aggregate only the filtered commits\n",
    "        commit_aggregated = pr_commits_filtered.groupby('pr_id').agg({\n",
    "            'additions': 'sum',      # Total lines added across all commits\n",
    "            'deletions': 'sum',      # Total lines deleted across all commits\n",
    "            'patch': lambda x: '\\n\\n'.join([str(p) for p in x if pd.notna(p)])  # Concatenate all patches\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Add derived metrics\n",
    "        commit_aggregated['num_commits'] = pr_commits_filtered.groupby('pr_id').size().values\n",
    "        \n",
    "        # Calculate patch length (for analysis)\n",
    "        commit_aggregated['patch_length'] = commit_aggregated['patch'].str.len()\n",
    "        \n",
    "        print(f\"  ✓ Aggregated to {len(commit_aggregated):,} unique performance PRs\")\n",
    "        print(f\"  Avg commits per PR: {commit_aggregated['num_commits'].mean():.1f}\")\n",
    "        \n",
    "        # Merge commit stats into AI Agent PR table\n",
    "        ai_perf_prs = ai_perf_prs.merge(\n",
    "            commit_aggregated,\n",
    "            left_on='id',\n",
    "            right_on='pr_id',\n",
    "            how='left'\n",
    "        )\n",
    "        if 'pr_id' in ai_perf_prs.columns:\n",
    "            ai_perf_prs = ai_perf_prs.drop(columns=['pr_id'])\n",
    "        \n",
    "        ai_with_commits = ai_perf_prs['additions'].notna().sum()\n",
    "        print(f\"  AI Agent PRs with commit data: {ai_with_commits:,} / {len(ai_perf_prs):,} ({ai_with_commits/len(ai_perf_prs)*100:.1f}%)\")\n",
    "        \n",
    "        # Merge commit stats into Human PR table\n",
    "        human_perf_prs = human_perf_prs.merge(\n",
    "            commit_aggregated,\n",
    "            left_on='id',\n",
    "            right_on='pr_id',\n",
    "            how='left'\n",
    "        )\n",
    "        if 'pr_id' in human_perf_prs.columns:\n",
    "            human_perf_prs = human_perf_prs.drop(columns=['pr_id'])\n",
    "        \n",
    "        human_with_commits = human_perf_prs['additions'].notna().sum()\n",
    "        print(f\"  Human PRs with commit data: {human_with_commits:,} / {len(human_perf_prs):,} ({human_with_commits/len(human_perf_prs)*100:.1f}%)\")\n",
    "    else:\n",
    "        print(\"  ⚠ No commits found for performance PRs\")\n",
    "        # Add placeholder columns\n",
    "        for df in [ai_perf_prs, human_perf_prs]:\n",
    "            df['additions'] = None\n",
    "            df['deletions'] = None\n",
    "            df['num_commits'] = None\n",
    "            df['patch'] = None\n",
    "            df['patch_length'] = None\n",
    "    \n",
    "else:\n",
    "    print('⚠ pr_commit_details missing pr_id column; skipping commit merges.')\n",
    "    # Add placeholder columns\n",
    "    for df in [ai_perf_prs, human_perf_prs]:\n",
    "        df['additions'] = None\n",
    "        df['deletions'] = None\n",
    "        df['num_commits'] = None\n",
    "        df['patch'] = None\n",
    "        df['patch_length'] = None\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"✓ AI Agent Performance PRs: {len(ai_perf_prs):,}\")\n",
    "print(f\"✓ Human Performance PRs: {len(human_perf_prs):,}\")\n",
    "print(f\"✓ Total Performance PRs: {len(ai_perf_prs) + len(human_perf_prs):,}\")\n",
    "\n",
    "# Distribution by AI agent\n",
    "print(f\"\\nAI Agent Distribution:\")\n",
    "for agent, count in ai_perf_prs['agent'].value_counts().items():\n",
    "    pct = count / len(ai_perf_prs) * 100\n",
    "    print(f\"  {agent:20s} {count:5,d} ({pct:5.1f}%)\")\n",
    "\n",
    "# Commit statistics summary\n",
    "if 'num_commits' in ai_perf_prs.columns and ai_perf_prs['num_commits'].notna().any():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"COMMIT STATISTICS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    for author_type, df in [('AI Agent', ai_perf_prs), ('Human', human_perf_prs)]:\n",
    "        with_commits = df['num_commits'].notna()\n",
    "        if with_commits.sum() > 0:\n",
    "            print(f\"\\n{author_type}:\")\n",
    "            print(f\"  PRs with commit data: {with_commits.sum():,} ({with_commits.mean()*100:.1f}%)\")\n",
    "            print(f\"  Avg commits per PR: {df.loc[with_commits, 'num_commits'].mean():.1f}\")\n",
    "            print(f\"  Median commits per PR: {df.loc[with_commits, 'num_commits'].median():.1f}\")\n",
    "            print(f\"  Avg additions: {df.loc[with_commits, 'additions'].mean():.0f} lines\")\n",
    "            print(f\"  Median additions: {df.loc[with_commits, 'additions'].median():.0f} lines\")\n",
    "            print(f\"  Avg deletions: {df.loc[with_commits, 'deletions'].mean():.0f} lines\")\n",
    "            print(f\"  Median deletions: {df.loc[with_commits, 'deletions'].median():.0f} lines\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset: 428 performance PRs\n",
      "  AI Agents: 340\n",
      "  Humans: 88\n"
     ]
    }
   ],
   "source": [
    "# Combine AI and Human PRs\n",
    "perf_prs = pd.concat([ai_perf_prs, human_perf_prs], ignore_index=True)\n",
    "\n",
    "print(f\"Combined dataset: {len(perf_prs):,} performance PRs\")\n",
    "print(f\"  AI Agents: {(perf_prs['author_type'] == 'AI Agent').sum():,}\")\n",
    "print(f\"  Humans: {(perf_prs['author_type'] == 'Human').sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading repository data for language information...\n",
      "✓ Language data joined!\n",
      "  AI Agent PRs with language: 339\n",
      "  Human PRs with language: 87\n",
      "  Total: 426 PRs with language info\n"
     ]
    }
   ],
   "source": [
    "# Add language from repository table\n",
    "print(\"Loading repository data for language information...\")\n",
    "\n",
    "if 'language' in all_repo_df.columns:\n",
    "    # For AI Agent PRs: join on repo_id\n",
    "    ai_mask = perf_prs['author_type'] == 'AI Agent'\n",
    "    ai_prs = perf_prs[ai_mask].copy()\n",
    "    human_prs = perf_prs[~ai_mask].copy()\n",
    "    \n",
    "    # Join AI PRs using repo_id\n",
    "    if 'repo_id' in ai_prs.columns:\n",
    "        ai_prs = ai_prs.merge(\n",
    "            all_repo_df[['id', 'language']], \n",
    "            left_on='repo_id', \n",
    "            right_on='id', \n",
    "            how='left',\n",
    "            suffixes=('', '_repo')\n",
    "        )\n",
    "        ai_prs['primary_language'] = ai_prs['language']\n",
    "        if 'id_repo' in ai_prs.columns:\n",
    "            ai_prs = ai_prs.drop(['id_repo'], axis=1)\n",
    "        if 'language' in ai_prs.columns:\n",
    "            ai_prs = ai_prs.drop(['language'], axis=1)\n",
    "    \n",
    "    # Join Human PRs using repo_url\n",
    "    # Human PRs have repo_url, match with url from all_repo_df\n",
    "    if 'repo_url' in human_prs.columns and 'url' in all_repo_df.columns:\n",
    "        human_prs = human_prs.merge(\n",
    "            all_repo_df[['url', 'language']], \n",
    "            left_on='repo_url',\n",
    "            right_on='url',\n",
    "            how='left',\n",
    "            suffixes=('', '_repo')\n",
    "        )\n",
    "        human_prs['primary_language'] = human_prs['language']\n",
    "        if 'url' in human_prs.columns:\n",
    "            human_prs = human_prs.drop(['url'], axis=1)\n",
    "        if 'language' in human_prs.columns:\n",
    "            human_prs = human_prs.drop(['language'], axis=1)\n",
    "    \n",
    "    # Combine back together\n",
    "    perf_prs = pd.concat([ai_prs, human_prs], ignore_index=True)\n",
    "    \n",
    "    ai_lang_count = perf_prs[perf_prs['author_type'] == 'AI Agent']['primary_language'].notna().sum()\n",
    "    human_lang_count = perf_prs[perf_prs['author_type'] == 'Human']['primary_language'].notna().sum()\n",
    "    \n",
    "    print(f\"✓ Language data joined!\")\n",
    "    print(f\"  AI Agent PRs with language: {ai_lang_count:,}\")\n",
    "    print(f\"  Human PRs with language: {human_lang_count:,}\")\n",
    "    print(f\"  Total: {perf_prs['primary_language'].notna().sum():,} PRs with language info\")\n",
    "else:\n",
    "    perf_prs['primary_language'] = None\n",
    "    print(\"⚠ No language column in repository table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Feature engineering complete\n",
      "  Merge rate: 57.2%\n",
      "  PRs with descriptions: 411 (96.0%)\n"
     ]
    }
   ],
   "source": [
    "# Convert date columns\n",
    "for col in ['created_at', 'closed_at', 'merged_at']:\n",
    "    if col in perf_prs.columns:\n",
    "        perf_prs[col] = pd.to_datetime(perf_prs[col])\n",
    "\n",
    "# Add derived status columns\n",
    "perf_prs['is_merged'] = perf_prs['merged_at'].notna()\n",
    "perf_prs['is_closed'] = perf_prs['closed_at'].notna()\n",
    "perf_prs['is_open'] = perf_prs['state'] == 'open'\n",
    "\n",
    "# Calculate time metrics\n",
    "perf_prs['time_to_close_hours'] = (\n",
    "    perf_prs['closed_at'] - perf_prs['created_at']\n",
    ").dt.total_seconds() / 3600\n",
    "\n",
    "perf_prs['time_to_merge_hours'] = (\n",
    "    perf_prs['merged_at'] - perf_prs['created_at']\n",
    ").dt.total_seconds() / 3600\n",
    "\n",
    "perf_prs['time_to_close_days'] = perf_prs['time_to_close_hours'] / 24\n",
    "perf_prs['time_to_merge_days'] = perf_prs['time_to_merge_hours'] / 24\n",
    "\n",
    "# Text metrics\n",
    "perf_prs['title_length'] = perf_prs['title'].str.len()\n",
    "perf_prs['title_word_count'] = perf_prs['title'].str.split().str.len()\n",
    "perf_prs['body_length'] = perf_prs['body'].str.len()\n",
    "perf_prs['body_word_count'] = perf_prs['body'].str.split().str.len()\n",
    "perf_prs['has_body'] = perf_prs['body'].notna() & (perf_prs['body'].str.strip() != '')\n",
    "\n",
    "print(\"✓ Feature engineering complete\")\n",
    "print(f\"  Merge rate: {perf_prs['is_merged'].mean()*100:.1f}%\")\n",
    "print(f\"  PRs with descriptions: {perf_prs['has_body'].sum():,} ({perf_prs['has_body'].mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Optimization Pattern Detection (Generation with GPT, DO NOT rerun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Performance Optimization Pattern Detection with GPT\n",
    "# ============================================================================\n",
    "!pip install openai --quiet\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from pydantic import BaseModel \n",
    "from pathlib import Path\n",
    "\n",
    "def analyze_optimization_with_gpt(title, body, patch):\n",
    "    \"\"\"\n",
    "    Call GPT to analyze performance optimization patterns in a commit.\n",
    "    \n",
    "    Parameters:\n",
    "    - title: PR/commit title\n",
    "    - body: PR/commit description\n",
    "    - patch: Git diff/patch content\n",
    "    \n",
    "    Returns:\n",
    "    - dict with analysis results or error info\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prepare the context\n",
    "    context_parts = []\n",
    "    \n",
    "    if pd.notna(title) and str(title).strip():\n",
    "        context_parts.append(f\"**Title**: {title}\")\n",
    "    \n",
    "    if pd.notna(body) and str(body).strip():\n",
    "        context_parts.append(f\"**Description**: {body}\")\n",
    "    \n",
    "    if pd.notna(patch) and str(patch).strip():\n",
    "        # Truncate very long patches to avoid token limits\n",
    "        patch_str = str(patch)\n",
    "        if len(patch_str) > 15000:  # Rough character limit\n",
    "            patch_str = patch_str[:15000] + \"\\n\\n... [patch truncated for length] ...\"\n",
    "        context_parts.append(f\"**Code Changes (Patch)**:\\n```diff\\n{patch_str}\\n```\")\n",
    "    \n",
    "    if not context_parts:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": \"No content available\",\n",
    "            \"explanation\": None,\n",
    "            \"optimization_comparison\": None,\n",
    "            \"high_level_pattern\": None,\n",
    "            \"sub_pattern\": None,\n",
    "            \"confidence\": None,\n",
    "            \"tokens_used\": 0\n",
    "        }\n",
    "    \n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    client=OpenAI(api_key=\"YOUR_OPENAI_API_KEY_HERE\")\n",
    "    \n",
    "    # Construct the prompt\n",
    "    prompt = f\"\"\"I have a performance optimization commit with the following information. Please analyze with the following goals:\n",
    "\n",
    "1. **Code Function Explanation**: Briefly explain what the code is doing—what problem it solves and how it works.\n",
    "\n",
    "2. **Optimization Comparison**: Compare the original and optimized versions to identify:\n",
    "   - **Algorithmic changes**: Any differences in logic, algorithm design, or problem-solving approach.\n",
    "   - **Performance improvements**: Enhancements related to time complexity, space efficiency, or runtime behavior.\n",
    "   - **Redundant code removal**: Elimination of unnecessary logic, method calls, or control structures.\n",
    "   - **Other noteworthy changes**: Any structural or stylistic differences that could impact performance or readability.\n",
    "   \n",
    "3. **Optimization Pattern Classification**:\n",
    "   Based on the overall nature of the optimized code, assign the following. Return \"No Meaningful Change\" if no meaningful change is made.\n",
    "   - **Exactly one high-level optimization pattern** from the list below  \n",
    "   - **One most representative sub-pattern** within that high-level category\n",
    "   \n",
    "   ### High-Level Optimization Patterns Taxonomy:\n",
    "   - **Algorithm-Level Optimizations**\n",
    "        - Select Computationally Efficient Algorithms\n",
    "        - Select Algorithm Based on Instruction Speed\n",
    "        - Structure Algorithm to Support ILP\n",
    "        - Select Space Efficient Algorithm\n",
    "        - Inheritance over Delegation for Energy Efficiency\n",
    "   - **Control-Flow and Branching Optimizations**\n",
    "        - Make Conditional Branches More Predictable\n",
    "        - Remove Branches with min/max Instructions\n",
    "        - Remove Branches by Doing Extra Work\n",
    "        - Remove Branching with Masking\n",
    "        - Rearranging Branches\n",
    "        - Combining Branches\n",
    "   - **Memory and Data Locality Optimizations**\n",
    "        - Access Data with Appropriate Type\n",
    "        - Increase Cache Efficiency via Locality\n",
    "        - Arrange Data for Optimal Prefetching\n",
    "        - Avoid Cache Capacity Issues\n",
    "        - Increase Workload to Hide Latency\n",
    "        - Use Smaller Data Types\n",
    "        - Caching, Buffering\n",
    "        - Improve Data Structure Locality\n",
    "        - Optimize Object Use\n",
    "        - Reduce RTSJ Immortal Memory Bloat\n",
    "   - **Loop Transformations**\n",
    "        - Remove Conditional by Loop Unrolling\n",
    "        - Loop Distribution (Fission)\n",
    "        - Loop Fusion, Peeling, Interchanging\n",
    "        - Loop Invariant Branches\n",
    "        - Loop Strip-mining\n",
    "   - **I/O and Synchronization**\n",
    "        - Selection of I/O Size\n",
    "        - Polling\n",
    "        - Non-Blocking I/O\n",
    "   - **Data Structure Selection and Adaptation**\n",
    "        - Choose Structure for Energy Efficiency\n",
    "        - Darwinian Selection\n",
    "        - Select via Method Calls\n",
    "        - Cross-Library Comparison\n",
    "   - **Code Smells and Structural Simplification**\n",
    "        - Remove Optional Features\n",
    "        - Remove Redundant Method Calls\n",
    "        - Extract Long Methods\n",
    "        - Remove Duplicates\n",
    "        - Move Methods to Reduce Feature Envy\n",
    "        - Minimize God Classes\n",
    "        - Type Checking\n",
    "         \n",
    "Here are the info:\n",
    "            \n",
    "{context}\n",
    "\n",
    "**Output Structure**:  \n",
    "Please respond in JSON format with the following structure:\n",
    "{{\n",
    "  \"explanation\": \"Brief description of what the code is doing\",\n",
    "  \"optimization_comparison\": \"Detailed comparison highlighting specific optimizations\",\n",
    "  \"high_level_pattern\": \"Single most representative high-level optimization pattern (or 'No Meaningful Change')\",\n",
    "  \"sub_pattern\": \"Most representative sub-pattern within the category (or null if No Meaningful Change)\",\n",
    "  \"confidence\": \"high/medium/low - your confidence in this classification\"\n",
    "}}\n",
    "\n",
    "Ensure your response is valid JSON that can be parsed.\n",
    "\"\"\"\n",
    "\n",
    "    class AnalysisResult(BaseModel):  \n",
    "        explanation: str\n",
    "        optimization_comparison: str\n",
    "        high_level_pattern: str\n",
    "        sub_pattern: str\n",
    "        confidence: str\n",
    "    \n",
    "    try:        \n",
    "        response = client.beta.chat.completions.parse(\n",
    "                    model = \"gpt-5-mini\",\n",
    "                    messages = [\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": \"You are an expert software engineer specializing in performance optimization analysis. Analyze code changes and classify optimization patterns accurately.\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": prompt\n",
    "                        }\n",
    "                    ],\n",
    "                    response_format=AnalysisResult,\n",
    "                )\n",
    "\n",
    "        # Parse the response\n",
    "        content = response.choices[0].message.content\n",
    "        result = json.loads(content)\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"explanation\": result.get(\"explanation\", \"\"),\n",
    "            \"optimization_comparison\": result.get(\"optimization_comparison\", \"\"),\n",
    "            \"high_level_pattern\": result.get(\"high_level_pattern\", \"\"),\n",
    "            \"sub_pattern\": result.get(\"sub_pattern\", \"\"),\n",
    "            \"confidence\": result.get(\"confidence\", \"unknown\"),\n",
    "            \"tokens_used\": response.usage.total_tokens,\n",
    "            \"error\": None\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e),\n",
    "            \"explanation\": None,\n",
    "            \"optimization_comparison\": None,\n",
    "            \"high_level_pattern\": None,\n",
    "            \"sub_pattern\": None,\n",
    "            \"confidence\": None,\n",
    "            \"tokens_used\": 0\n",
    "        }\n",
    "\n",
    "\n",
    "def batch_analyze_performance_prs(perf_prs, batch_size=10, delay=1.0,resume=False, checkpoint_prefix='perf_prs_checkpoint', output_file='perf_prs_with_gpt_analysis.csv'):\n",
    "    \"\"\"\n",
    "    Analyze all performance PRs in batches.\n",
    "\n",
    "    Parameters:\n",
    "    - perf_prs: DataFrame with performance PRs\n",
    "    - batch_size: Number of PRs to process before saving checkpoint\n",
    "    - delay: Delay between API calls in seconds\n",
    "    - resume: Continue from the last available checkpoint if True\n",
    "    - checkpoint_prefix: Filename prefix used for checkpoint files\n",
    "    - output_file: Final CSV filename for the aggregated results\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with analysis results added\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Starting GPT analysis of {len(perf_prs):,} performance PRs...\")\n",
    "\n",
    "    checkpoint_files = []\n",
    "    processed_count = 0\n",
    "\n",
    "    if resume:\n",
    "        checkpoint_files = sorted(Path('.').glob(f\"{checkpoint_prefix}_*.csv\"))\n",
    "        if checkpoint_files:\n",
    "            def _processed_from_path(path_obj):\n",
    "                suffix = path_obj.stem.rsplit('_', 1)[-1]\n",
    "                return int(suffix) if suffix.isdigit() else 0\n",
    "\n",
    "            latest_checkpoint = max(checkpoint_files, key=_processed_from_path)\n",
    "            checkpoint_progress = _processed_from_path(latest_checkpoint)\n",
    "            perf_prs = pd.read_csv(latest_checkpoint)\n",
    "            processed_count = min(checkpoint_progress, len(perf_prs))\n",
    "            print(f\"↻ Resuming from checkpoint {latest_checkpoint} ({processed_count} PRs processed)...\")\n",
    "        else:\n",
    "            print(\"↻ Resume requested but no checkpoint found. Starting from scratch.\")\n",
    "\n",
    "    result_defaults = {\n",
    "        'gpt_explanation': None,\n",
    "        'gpt_comparison': None,\n",
    "        'optimization_pattern': None,\n",
    "        'optimization_subpattern': None,\n",
    "        'pattern_confidence': None,\n",
    "        'gpt_success': False,\n",
    "        'gpt_error': None,\n",
    "        'gpt_tokens': 0\n",
    "    }\n",
    "\n",
    "    for column, default in result_defaults.items():\n",
    "        if resume and column in perf_prs.columns:\n",
    "            continue\n",
    "        perf_prs[column] = default\n",
    "\n",
    "    start_idx = processed_count if resume else 0\n",
    "    iterator = range(start_idx, len(perf_prs))\n",
    "    progress_bar = tqdm(iterator, total=len(perf_prs), desc=\"Analyzing PRs\", initial=start_idx)\n",
    "\n",
    "    for idx in progress_bar:\n",
    "        row = perf_prs.iloc[idx]\n",
    "        result = analyze_optimization_with_gpt(\n",
    "            title=row.get('title'),\n",
    "            body=row.get('body'),\n",
    "            patch=row.get('patch')\n",
    "        )\n",
    "\n",
    "        perf_prs.at[idx, 'gpt_success'] = result['success']\n",
    "        perf_prs.at[idx, 'gpt_tokens'] = result['tokens_used']\n",
    "\n",
    "        if result['success']:\n",
    "            perf_prs.at[idx, 'gpt_explanation'] = result['explanation']\n",
    "            perf_prs.at[idx, 'gpt_comparison'] = result['optimization_comparison']\n",
    "            perf_prs.at[idx, 'optimization_pattern'] = result['high_level_pattern']\n",
    "            perf_prs.at[idx, 'optimization_subpattern'] = result['sub_pattern']\n",
    "            perf_prs.at[idx, 'pattern_confidence'] = result['confidence']\n",
    "            perf_prs.at[idx, 'gpt_error'] = None\n",
    "        else:\n",
    "            perf_prs.at[idx, 'gpt_error'] = result['error']\n",
    "\n",
    "        time.sleep(delay)\n",
    "\n",
    "        if (idx + 1) % batch_size == 0:\n",
    "            checkpoint_file = f\"{checkpoint_prefix}_{idx+1}.csv\"\n",
    "            perf_prs.to_csv(checkpoint_file, index=False)\n",
    "            print(f\"✓ Checkpoint saved: {checkpoint_file}\")\n",
    "\n",
    "    perf_prs.to_csv(output_file, index=False)\n",
    "    print(f\"✓ Analysis complete! Saved to: {output_file}\")\n",
    "\n",
    "    success_series = perf_prs['gpt_success'].fillna(False)\n",
    "    success_count = success_series.sum()\n",
    "    success_rate = (success_count / len(perf_prs) * 100) if len(perf_prs) else 0\n",
    "    failure_count = success_series.eq(False).sum()\n",
    "    total_tokens = perf_prs['gpt_tokens'].sum()\n",
    "\n",
    "    print(f\"{'='*80}\")\n",
    "    print(\"ANALYSIS SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total PRs analyzed: {len(perf_prs):,}\")\n",
    "    print(f\"Successful: {success_count:,} ({success_rate:.1f}%)\")\n",
    "    print(f\"Failed: {failure_count:,}\")\n",
    "    print(f\"Total tokens used: {total_tokens:,}\")\n",
    "\n",
    "    if success_count > 0:\n",
    "        print(f\"{'='*80}\")\n",
    "        print(\"OPTIMIZATION PATTERN DISTRIBUTION\")\n",
    "        print(f\"{'='*80}\")\n",
    "        pattern_counts = perf_prs[perf_prs['gpt_success'] == True]['optimization_pattern'].value_counts()\n",
    "        for pattern, count in pattern_counts.items():\n",
    "            pct = count / success_count * 100\n",
    "            print(f\"  {pattern:50s} {count:4d} ({pct:5.1f}%)\")\n",
    "\n",
    "    return perf_prs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing GPT analysis on 340 AI PRs\n",
      "Starting GPT analysis of 340 performance PRs...\n",
      "↻ Resuming from checkpoint ai_perf_prs_checkpoint_250.csv (250 PRs processed)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing PRs:  76%|███████▋  | 260/340 [03:46<26:29, 19.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint saved: ai_perf_prs_checkpoint_260.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing PRs:  79%|███████▉  | 270/340 [07:27<27:26, 23.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint saved: ai_perf_prs_checkpoint_270.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing PRs:  81%|████████  | 274/340 [08:54<23:51, 21.68s/it]"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Usage\n",
    "# ============================================================================\n",
    "\n",
    "# run ai and human pr analysis separately\n",
    "\n",
    "# ai pr analysis\n",
    "ai_sample = perf_prs[perf_prs['author_type'] == 'AI Agent']\n",
    "print(f\"Testing GPT analysis on {len(ai_sample)} AI PRs\")\n",
    "\n",
    "# Run the analysis\n",
    "perf_prs_analyzed = batch_analyze_performance_prs(\n",
    "    ai_sample,\n",
    "    batch_size=10,    # Save checkpoint every 10 PRs\n",
    "    delay=0.5,        # 0.5 second delay between API calls\n",
    "    resume=True,      # Continue from the last saved checkpoint if available\n",
    "    checkpoint_prefix='ai_perf_prs_checkpoint',\n",
    "    output_file='ai_perf_prs_with_gpt_analysis.csv'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# human pr analysis\n",
    "human_sample = perf_prs[perf_prs['author_type'] == 'Human']\n",
    "print(f\"Testing GPT analysis on {len(human_sample)} Human PRs\")\n",
    "\n",
    "# Run the analysis\n",
    "perf_prs_analyzed = batch_analyze_performance_prs(\n",
    "    human_sample,\n",
    "    batch_size=10,    # Save checkpoint every 10 PRs\n",
    "    delay=0.5,        # 0.5 second delay between API calls\n",
    "    resume=True,      # Continue from the last saved checkpoint if available\n",
    "    checkpoint_prefix='human_perf_prs_checkpoint',\n",
    "    output_file='human_perf_prs_with_gpt_analysis.csv'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze results by author type\n",
    "print(\"=\"*80)\n",
    "print(\"PATTERN COMPARISON: AI AGENTS VS HUMANS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for author_type in ['AI Agent', 'Human']:\n",
    "    subset = perf_prs_analyzed[\n",
    "        (perf_prs_analyzed['author_type'] == author_type) & \n",
    "        (perf_prs_analyzed['gpt_success'] == True)\n",
    "    ]\n",
    "    \n",
    "    if len(subset) > 0:\n",
    "        print(f\"{author_type} (n={len(subset):,}):\")\n",
    "        pattern_dist = subset['optimization_pattern'].value_counts().head(5)\n",
    "        for pattern, count in pattern_dist.items():\n",
    "            pct = count / len(subset) * 100\n",
    "            print(f\"  {pattern:50s} {count:4d} ({pct:5.1f}%)\")\n",
    "\n",
    "# Compare sub-patterns\n",
    "print(\"=\"*80)\n",
    "print(\"TOP SUB-PATTERNS BY AUTHOR TYPE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for author_type in ['AI Agent', 'Human']:\n",
    "    subset = perf_prs_analyzed[\n",
    "        (perf_prs_analyzed['author_type'] == author_type) & \n",
    "        (perf_prs_analyzed['gpt_success'] == True)\n",
    "    ]\n",
    "    \n",
    "    if len(subset) > 0:\n",
    "        print(f\"{author_type}:\")\n",
    "        subpattern_dist = subset['optimization_subpattern'].value_counts().head(5)\n",
    "        for subpattern, count in subpattern_dist.items():\n",
    "            pct = count / len(subset) * 100\n",
    "            print(f\"  {subpattern:50s} {count:4d} ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Visualization Script: Optimization Pattern Analysis (Revised)\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import textwrap\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data from perf_prs_with_gpt_analysis.csv...\")\n",
    "perf_prs_analyzed = pd.read_csv('perf_prs_with_gpt_analysis.csv')\n",
    "\n",
    "# Filter successful analyses\n",
    "analyzed = perf_prs_analyzed[perf_prs_analyzed['gpt_success'] == True].copy()\n",
    "print(f\"Loaded {len(analyzed):,} successfully analyzed PRs\")\n",
    "print(f\"  AI Agents: {(analyzed['author_type'] == 'AI Agent').sum():,}\")\n",
    "print(f\"  Humans: {(analyzed['author_type'] == 'Human').sum():,}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Helper Functions\n",
    "# ============================================================================\n",
    "\n",
    "def wrap_labels(labels, width=30):\n",
    "    \"\"\"Wrap long labels for better readability\"\"\"\n",
    "    return [textwrap.fill(label, width) for label in labels]\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Pattern and Sub-Pattern Distribution (Single Figure per Author Type)\n",
    "# Using the same style as the provided script\n",
    "# ============================================================================\n",
    "\n",
    "def plot_pattern_subpattern_stacked(analyzed, author_type):\n",
    "    \"\"\"\n",
    "    Create stacked horizontal bar chart showing patterns and sub-patterns\n",
    "    Similar style to the provided script\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter by author type\n",
    "    subset = analyzed[analyzed['author_type'] == author_type].copy()\n",
    "    \n",
    "    # Get pattern and sub-pattern data\n",
    "    pattern_subpattern = subset.groupby(['optimization_pattern', 'optimization_subpattern']).size()\n",
    "    \n",
    "    # Build data dictionary\n",
    "    data = {}\n",
    "    for (pattern, subpattern), count in pattern_subpattern.items():\n",
    "        if pd.notna(pattern) and pd.notna(subpattern):\n",
    "            if pattern not in data:\n",
    "                data[pattern] = {}\n",
    "            data[pattern][subpattern] = count\n",
    "    \n",
    "    if not data:\n",
    "        print(f\"No data available for {author_type}\")\n",
    "        return\n",
    "    \n",
    "    # Sort patterns by total count\n",
    "    pattern_totals = {k: sum(v.values()) for k, v in data.items()}\n",
    "    sorted_patterns = sorted(pattern_totals.keys(), key=lambda x: pattern_totals[x], reverse=True)\n",
    "    data = {k: data[k] for k in sorted_patterns if k in data}\n",
    "    \n",
    "    # Short labels for patterns (y-axis)\n",
    "    short_labels = {}\n",
    "    for pattern in data.keys():\n",
    "        # Truncate long pattern names\n",
    "        if len(pattern) > 35:\n",
    "            short_labels[pattern] = pattern[:32] + '...'\n",
    "        else:\n",
    "            short_labels[pattern] = pattern\n",
    "    \n",
    "    # Short labels for sub-patterns (legend)\n",
    "    sub_short = {}\n",
    "    for pattern, subs in data.items():\n",
    "        for sub in subs.keys():\n",
    "            if len(sub) > 30:\n",
    "                sub_short[sub] = sub[:27] + '...'\n",
    "            else:\n",
    "                sub_short[sub] = sub\n",
    "    \n",
    "    # Color maps for each category (cycling through different colormaps)\n",
    "    available_colormaps = ['Blues', 'Greens', 'Oranges', 'Reds', 'Purples', 'YlOrBr', 'PuBu', 'RdPu']\n",
    "    colormaps = {}\n",
    "    for idx, pattern in enumerate(data.keys()):\n",
    "        num_subs = len(data[pattern])\n",
    "        colormaps[pattern] = (available_colormaps[idx % len(available_colormaps)], num_subs)\n",
    "    \n",
    "    # Flatten subpatterns\n",
    "    subpatterns = []\n",
    "    for cat, sub in data.items():\n",
    "        for sp in sub.keys():\n",
    "            subpatterns.append((cat, sp))\n",
    "    \n",
    "    # Build DataFrame\n",
    "    rows = []\n",
    "    for cat in data.keys():\n",
    "        row = [data[cat].get(sp, 0) for _, sp in subpatterns]\n",
    "        rows.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(rows,\n",
    "                      index=list(data.keys()),\n",
    "                      columns=[sp for _, sp in subpatterns])\n",
    "    \n",
    "    # Generate colors by category\n",
    "    colors = []\n",
    "    for cat, sp in subpatterns:\n",
    "        cmap_name, count = colormaps[cat]\n",
    "        cmap = plt.get_cmap(cmap_name)\n",
    "        idx = list(data[cat].keys()).index(sp)\n",
    "        colors.append(cmap((idx + 0.5) / max(count, 1)))\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(14, max(8, len(data) * 0.6)))\n",
    "    left = np.zeros(len(df))\n",
    "    y_labels = [short_labels[cat] for cat in df.index]\n",
    "    \n",
    "    for i, (_, sp) in enumerate(subpatterns):\n",
    "        ax.barh(y_labels, df.iloc[:, i], left=left,\n",
    "                label=sub_short[sp], color=colors[i])\n",
    "        left += df.iloc[:, i].values\n",
    "    \n",
    "    # Add total counts\n",
    "    totals = df.sum(axis=1).values\n",
    "    for i, total in enumerate(totals):\n",
    "        ax.text(total + max(totals)*0.01, i, str(int(total)), \n",
    "                va='center', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Final styling\n",
    "    ax.set_xlabel('Count', fontsize=17)\n",
    "    ax.set_title(f'Optimization Patterns: {author_type}', fontsize=18, fontweight='bold', pad=20)\n",
    "    ax.tick_params(axis='y', labelsize=16)\n",
    "    ax.tick_params(axis='x', labelsize=15)\n",
    "    ax.set_xlim(0, max(totals) * 1.15)\n",
    "    \n",
    "    # Legend at top center\n",
    "    ncol = min(4, (len(subpatterns) + 2) // 3)\n",
    "    legend = ax.legend(title='Sub-Patterns',\n",
    "                       loc='upper center',\n",
    "                       bbox_to_anchor=(0.5, 1.0),\n",
    "                       ncol=ncol,\n",
    "                       framealpha=0.9,\n",
    "                       fontsize=11,\n",
    "                       title_fontsize=13)\n",
    "    legend.get_frame().set_edgecolor('gray')\n",
    "    legend.get_frame().set_linewidth(0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save\n",
    "    filename = f'optimization_pattern_{author_type.lower().replace(\" \", \"_\")}.png'\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"✓ Saved: {filename}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Pattern Distribution Heatmap by Author Type\n",
    "# ============================================================================\n",
    "\n",
    "def plot_pattern_heatmap(analyzed):\n",
    "    \"\"\"Create heatmap showing pattern frequency by author type (percentages)\"\"\"\n",
    "    \n",
    "    # Create pivot table with percentages\n",
    "    pattern_pivot = pd.crosstab(\n",
    "        analyzed['optimization_pattern'],\n",
    "        analyzed['author_type'],\n",
    "        normalize='columns'\n",
    "    ) * 100\n",
    "    \n",
    "    # Sort by total frequency\n",
    "    pattern_totals = analyzed['optimization_pattern'].value_counts()\n",
    "    pattern_pivot = pattern_pivot.loc[pattern_totals.index]\n",
    "    \n",
    "    # Wrap labels\n",
    "    wrapped_labels = wrap_labels(pattern_pivot.index, width=40)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(10, max(8, len(pattern_pivot) * 0.4)))\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(\n",
    "        pattern_pivot,\n",
    "        annot=True,\n",
    "        fmt='.1f',\n",
    "        cmap='YlOrRd',\n",
    "        cbar_kws={'label': 'Percentage (%)'},\n",
    "        ax=ax,\n",
    "        linewidths=0.5,\n",
    "        linecolor='white'\n",
    "    )\n",
    "    \n",
    "    ax.set_title('Optimization Pattern Distribution by Author Type (%)', \n",
    "                 fontweight='bold', fontsize=14, pad=15)\n",
    "    ax.set_xlabel('Author Type', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Optimization Pattern', fontsize=12, fontweight='bold')\n",
    "    ax.set_yticklabels(wrapped_labels, rotation=0, fontsize=10)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=0, fontsize=11)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('pattern_heatmap_by_author.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"✓ Saved: pattern_heatmap_by_author.png\")\n",
    "    \n",
    "    return pattern_pivot\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Statistical Comparison Table\n",
    "# ============================================================================\n",
    "\n",
    "def create_statistical_comparison_table(analyzed):\n",
    "    \"\"\"Create detailed statistical comparison table\"\"\"\n",
    "    \n",
    "    # Get all unique patterns\n",
    "    all_patterns = analyzed['optimization_pattern'].value_counts().index\n",
    "    \n",
    "    comparison_data = []\n",
    "    \n",
    "    for pattern in all_patterns:\n",
    "        ai_count = len(analyzed[(analyzed['author_type'] == 'AI Agent') & \n",
    "                                (analyzed['optimization_pattern'] == pattern)])\n",
    "        human_count = len(analyzed[(analyzed['author_type'] == 'Human') & \n",
    "                                   (analyzed['optimization_pattern'] == pattern)])\n",
    "        \n",
    "        total_ai = len(analyzed[analyzed['author_type'] == 'AI Agent'])\n",
    "        total_human = len(analyzed[analyzed['author_type'] == 'Human'])\n",
    "        \n",
    "        ai_pct = (ai_count / total_ai * 100) if total_ai > 0 else 0\n",
    "        human_pct = (human_count / total_human * 100) if total_human > 0 else 0\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'Pattern': pattern,\n",
    "            'AI Count': ai_count,\n",
    "            'AI %': ai_pct,\n",
    "            'Human Count': human_count,\n",
    "            'Human %': human_pct,\n",
    "            'Difference (AI - Human) %': ai_pct - human_pct\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    comparison_df = comparison_df.sort_values('AI Count', ascending=False)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, ax = plt.subplots(figsize=(16, max(10, len(comparison_df) * 0.5)))\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Wrap pattern names for table\n",
    "    comparison_df_display = comparison_df.copy()\n",
    "    comparison_df_display['Pattern'] = [textwrap.fill(p, width=50) for p in comparison_df_display['Pattern']]\n",
    "    \n",
    "    # Create table\n",
    "    table_data = comparison_df_display.round(1).values\n",
    "    table = ax.table(cellText=table_data,\n",
    "                    colLabels=comparison_df_display.columns,\n",
    "                    cellLoc='center',\n",
    "                    loc='center',\n",
    "                    colWidths=[0.4, 0.1, 0.1, 0.12, 0.1, 0.15])\n",
    "    \n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(9)\n",
    "    table.scale(1, 2.5)\n",
    "    \n",
    "    # Color code the difference column\n",
    "    for i in range(len(comparison_df)):\n",
    "        cell = table[(i+1, 5)]  # Difference column (0-indexed in data, +1 for header)\n",
    "        diff_val = comparison_df.iloc[i]['Difference (AI - Human) %']\n",
    "        if diff_val > 5:\n",
    "            cell.set_facecolor('#d4e6f1')  # Light blue for AI advantage\n",
    "        elif diff_val < -5:\n",
    "            cell.set_facecolor('#fadbd8')  # Light red for Human advantage\n",
    "        else:\n",
    "            cell.set_facecolor('#f8f9f9')  # Gray for similar\n",
    "    \n",
    "    # Header styling\n",
    "    for j in range(len(comparison_df_display.columns)):\n",
    "        cell = table[(0, j)]\n",
    "        cell.set_facecolor('#34495e')\n",
    "        cell.set_text_props(weight='bold', color='white')\n",
    "    \n",
    "    plt.title('Statistical Comparison: Optimization Patterns by Author Type', \n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "    \n",
    "    plt.savefig('statistical_comparison_table.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"✓ Saved: statistical_comparison_table.png\")\n",
    "    \n",
    "    # Also save as CSV\n",
    "    comparison_df.to_csv('pattern_comparison_statistics.csv', index=False)\n",
    "    print(\"✓ Saved: pattern_comparison_statistics.csv\")\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Summary Statistics\n",
    "# ============================================================================\n",
    "\n",
    "def print_summary_statistics(analyzed):\n",
    "    \"\"\"Print summary statistics\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"OPTIMIZATION PATTERN ANALYSIS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nTotal PRs Analyzed: {len(analyzed):,}\")\n",
    "    print(f\"  AI Agents: {(analyzed['author_type'] == 'AI Agent').sum():,}\")\n",
    "    print(f\"  Humans: {(analyzed['author_type'] == 'Human').sum():,}\")\n",
    "    \n",
    "    print(f\"\\nUnique Patterns: {analyzed['optimization_pattern'].nunique()}\")\n",
    "    print(f\"Unique Sub-Patterns: {analyzed['optimization_subpattern'].nunique()}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"TOP 5 PATTERNS (OVERALL)\")\n",
    "    print(\"-\"*80)\n",
    "    top_patterns = analyzed['optimization_pattern'].value_counts().head(5)\n",
    "    for pattern, count in top_patterns.items():\n",
    "        pct = count / len(analyzed) * 100\n",
    "        print(f\"  {pattern[:60]:60s} {count:4d} ({pct:5.1f}%)\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"TOP 5 PATTERNS (AI AGENTS)\")\n",
    "    print(\"-\"*80)\n",
    "    ai_subset = analyzed[analyzed['author_type'] == 'AI Agent']\n",
    "    top_ai = ai_subset['optimization_pattern'].value_counts().head(5)\n",
    "    for pattern, count in top_ai.items():\n",
    "        pct = count / len(ai_subset) * 100\n",
    "        print(f\"  {pattern[:60]:60s} {count:4d} ({pct:5.1f}%)\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"TOP 5 PATTERNS (HUMANS)\")\n",
    "    print(\"-\"*80)\n",
    "    human_subset = analyzed[analyzed['author_type'] == 'Human']\n",
    "    top_human = human_subset['optimization_pattern'].value_counts().head(5)\n",
    "    for pattern, count in top_human.items():\n",
    "        pct = count / len(human_subset) * 100\n",
    "        print(f\"  {pattern[:60]:60s} {count:4d} ({pct:5.1f}%)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Main Execution\n",
    "# ============================================================================\n",
    "\n",
    "def generate_all_visualizations():\n",
    "    \"\"\"Generate all requested visualizations\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GENERATING VISUALIZATIONS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print_summary_statistics(analyzed)\n",
    "    \n",
    "    print(\"\\n1. Creating pattern-subpattern distribution for AI Agents...\")\n",
    "    plot_pattern_subpattern_stacked(analyzed, 'AI Agent')\n",
    "    \n",
    "    print(\"\\n2. Creating pattern-subpattern distribution for Humans...\")\n",
    "    plot_pattern_subpattern_stacked(analyzed, 'Human')\n",
    "    \n",
    "    print(\"\\n3. Creating pattern distribution heatmap...\")\n",
    "    pattern_pivot = plot_pattern_heatmap(analyzed)\n",
    "    \n",
    "    print(\"\\n4. Creating statistical comparison table...\")\n",
    "    comparison_df = create_statistical_comparison_table(analyzed)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"✓ ALL VISUALIZATIONS COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nGenerated files:\")\n",
    "    print(\"  1. optimization_pattern_ai_agent.png - Stacked pattern/sub-pattern for AI\")\n",
    "    print(\"  2. optimization_pattern_human.png - Stacked pattern/sub-pattern for Human\")\n",
    "    print(\"  3. pattern_heatmap_by_author.png - Pattern distribution heatmap\")\n",
    "    print(\"  4. statistical_comparison_table.png - Detailed comparison table\")\n",
    "    print(\"  5. pattern_comparison_statistics.csv - Data in CSV format\")\n",
    "    \n",
    "    return {\n",
    "        'pattern_pivot': pattern_pivot,\n",
    "        'comparison_df': comparison_df\n",
    "    }\n",
    "\n",
    "\n",
    "# Run all visualizations\n",
    "results = generate_all_visualizations()\n",
    "\n",
    "# Display comparison summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PATTERN DISTRIBUTION HEATMAP\")\n",
    "print(\"=\"*80)\n",
    "print(results['pattern_pivot'].round(1))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STATISTICAL COMPARISON (Top 10)\")\n",
    "print(\"=\"*80)\n",
    "print(results['comparison_df'].head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append pattern columns to perf_prs for future reference\n",
    "# Load data\n",
    "print(\"Loading data from perf_prs_with_gpt_analysis.csv...\")\n",
    "perf_prs_analyzed = pd.read_csv('perf_prs_with_gpt_analysis.csv')\n",
    "\n",
    "# Filter successful analyses\n",
    "analyzed = perf_prs_analyzed[perf_prs_analyzed['gpt_success'] == True].copy()\n",
    "print(f\"Loaded {len(analyzed):,} successfully analyzed PRs\")\n",
    "print(f\"  AI Agents: {(analyzed['author_type'] == 'AI Agent').sum():,}\")\n",
    "print(f\"  Humans: {(analyzed['author_type'] == 'Human').sum():,}\")\n",
    "\n",
    "# Append GPT analysis columns to perf_prs for future reference\n",
    "print(\"\\nMerging GPT analysis back to perf_prs...\")\n",
    "\n",
    "# Define GPT analysis columns to merge\n",
    "gpt_columns = [\n",
    "    'id',  # Key for merging\n",
    "    'gpt_explanation',\n",
    "    'gpt_comparison',\n",
    "    'optimization_pattern',\n",
    "    'optimization_subpattern'\n",
    "]\n",
    "\n",
    "# Select only the columns that exist in perf_prs_analyzed\n",
    "available_columns = [col for col in gpt_columns if col in perf_prs_analyzed.columns]\n",
    "\n",
    "# Merge on 'id' column\n",
    "perf_prs = perf_prs.merge(\n",
    "    perf_prs_analyzed[available_columns],\n",
    "    on='id',\n",
    "    how='left',\n",
    "    suffixes=('', '_gpt')\n",
    ")\n",
    "\n",
    "print(f\"✓ GPT analysis merged to perf_prs\")\n",
    "print(f\"  Total PRs in perf_prs: {len(perf_prs):,}\")\n",
    "print(f\"  PRs with patterns: {perf_prs['optimization_pattern'].notna().sum():,}\")\n",
    "\n",
    "# Verify merge\n",
    "if 'optimization_pattern' in perf_prs.columns:\n",
    "    print(f\"\\n✓ Available GPT columns in perf_prs:\")\n",
    "    for col in available_columns:\n",
    "        if col != 'id' and col in perf_prs.columns:\n",
    "            non_null = perf_prs[col].notna().sum()\n",
    "            print(f\"    {col:30s} {non_null:4d} non-null values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and Evaluation Behavior [TODO]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description Quality [Needs to double check and find a better way to do it]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Description quality scored\n",
      "  Avg quality score: 2.06/4\n"
     ]
    }
   ],
   "source": [
    "def assess_description_quality(row):\n",
    "    \"\"\"Score description quality (0-5)\"\"\"\n",
    "    body = str(row['body']).lower()\n",
    "    \n",
    "    if pd.isna(row['body']) or body == 'nan' or len(body.strip()) < 50:\n",
    "        return 0\n",
    "    \n",
    "    score = 0\n",
    "    \n",
    "    # Problem statement\n",
    "    if any(word in body for word in ['problem', 'issue', 'slow', 'bottleneck', 'inefficient']):\n",
    "        score += 1\n",
    "    \n",
    "    # Solution description\n",
    "    if any(word in body for word in ['solution', 'implement', 'change', 'optimize', 'improve']):\n",
    "        score += 1\n",
    "    \n",
    "    # Measurements\n",
    "    if any(word in body for word in ['benchmark', 'test', 'measure', 'result']):\n",
    "        score += 1\n",
    "    \n",
    "    # Before/after comparison\n",
    "    if any(word in body for word in ['before', 'after', 'was', 'now', 'reduced', 'improved']):\n",
    "        score += 1\n",
    "    \n",
    "    return score\n",
    "\n",
    "perf_prs['description_quality_score'] = perf_prs.apply(assess_description_quality, axis=1)\n",
    "\n",
    "print(\"✓ Description quality scored\")\n",
    "print(f\"  Avg quality score: {perf_prs['description_quality_score'].mean():.2f}/4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview: AI Agents vs Humans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Column(s) ['optimization_pattern'] do not exist\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# High-level comparison \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m comparison_stats \u001b[38;5;241m=\u001b[39m \u001b[43mperf_prs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauthor_type\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcount\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mis_merged\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtime_to_merge_hours\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmedian\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhas_body\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbody_length\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdescription_quality_score\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moptimization_pattern\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnunique\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Number of unique patterns used\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprimary_language\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnunique\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     11\u001b[0m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mround(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     13\u001b[0m comparison_stats\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal PRs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMerged PRs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMerge Rate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAvg Time to Merge (hrs)\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMedian Time to Merge (hrs)\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBody Rate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAvg Body Length\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAvg Quality Score\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnique Patterns\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnique Languages\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     17\u001b[0m ]\n\u001b[1;32m     19\u001b[0m comparison_stats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMerge Rate (\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (comparison_stats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMerge Rate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m)\u001b[38;5;241m.\u001b[39mround(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/pandas/core/groupby/generic.py:1432\u001b[0m, in \u001b[0;36mDataFrameGroupBy.aggregate\u001b[0;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1429\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m engine_kwargs\n\u001b[1;32m   1431\u001b[0m op \u001b[38;5;241m=\u001b[39m GroupByApply(\u001b[38;5;28mself\u001b[39m, func, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m-> 1432\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dict_like(func) \u001b[38;5;129;01mand\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1434\u001b[0m     \u001b[38;5;66;03m# GH #52849\u001b[39;00m\n\u001b[1;32m   1435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mas_index \u001b[38;5;129;01mand\u001b[39;00m is_list_like(func):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/pandas/core/apply.py:190\u001b[0m, in \u001b[0;36mApply.agg\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_dict_like(func):\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg_dict_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(func):\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# we require a list, but not a 'str'\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magg_list_like()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/pandas/core/apply.py:423\u001b[0m, in \u001b[0;36mApply.agg_dict_like\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21magg_dict_like\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m    416\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;124;03m    Compute aggregation in the case of a dict-like argument.\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;124;03m    Result of aggregation.\u001b[39;00m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg_or_apply_dict_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43magg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/pandas/core/apply.py:1603\u001b[0m, in \u001b[0;36mGroupByApply.agg_or_apply_dict_like\u001b[0;34m(self, op_name)\u001b[0m\n\u001b[1;32m   1598\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine\u001b[39m\u001b[38;5;124m\"\u001b[39m: engine, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: engine_kwargs})\n\u001b[1;32m   1600\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m com\u001b[38;5;241m.\u001b[39mtemp_setattr(\n\u001b[1;32m   1601\u001b[0m     obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas_index\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m, condition\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas_index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1602\u001b[0m ):\n\u001b[0;32m-> 1603\u001b[0m     result_index, result_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_dict_like\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1605\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1606\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results_dict_like(selected_obj, result_index, result_data)\n\u001b[1;32m   1607\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/pandas/core/apply.py:462\u001b[0m, in \u001b[0;36mApply.compute_dict_like\u001b[0;34m(self, op_name, selected_obj, selection, kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m is_groupby \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(obj, (DataFrameGroupBy, SeriesGroupBy))\n\u001b[1;32m    461\u001b[0m func \u001b[38;5;241m=\u001b[39m cast(AggFuncTypeDict, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc)\n\u001b[0;32m--> 462\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_dictlike_arg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    464\u001b[0m is_non_unique_col \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    465\u001b[0m     selected_obj\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m selected_obj\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnunique() \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(selected_obj\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m    467\u001b[0m )\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m selected_obj\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;66;03m# key only used for output\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/pandas/core/apply.py:663\u001b[0m, in \u001b[0;36mApply.normalize_dictlike_arg\u001b[0;34m(self, how, obj, func)\u001b[0m\n\u001b[1;32m    661\u001b[0m     cols \u001b[38;5;241m=\u001b[39m Index(\u001b[38;5;28mlist\u001b[39m(func\u001b[38;5;241m.\u001b[39mkeys()))\u001b[38;5;241m.\u001b[39mdifference(obj\u001b[38;5;241m.\u001b[39mcolumns, sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    662\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cols) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 663\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(cols)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m do not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    665\u001b[0m aggregator_types \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mdict\u001b[39m)\n\u001b[1;32m    667\u001b[0m \u001b[38;5;66;03m# if we have a dict of any non-scalars\u001b[39;00m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;66;03m# eg. {'A' : ['mean']}, normalize all to\u001b[39;00m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;66;03m# be list-likes\u001b[39;00m\n\u001b[1;32m    670\u001b[0m \u001b[38;5;66;03m# Cannot use func.values() because arg may be a Series\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Column(s) ['optimization_pattern'] do not exist\""
     ]
    }
   ],
   "source": [
    "# High-level comparison \n",
    "comparison_stats = perf_prs.groupby('author_type').agg({\n",
    "    'id': 'count',\n",
    "    'is_merged': ['sum', 'mean'],\n",
    "    'time_to_merge_hours': ['mean', 'median'],\n",
    "    'has_body': 'mean',\n",
    "    'body_length': 'mean',\n",
    "    'description_quality_score': 'mean',\n",
    "    'optimization_pattern': 'nunique',  # Number of unique patterns used\n",
    "    'primary_language': 'nunique'\n",
    "}).round(2)\n",
    "\n",
    "comparison_stats.columns = [\n",
    "    'Total PRs', 'Merged PRs', 'Merge Rate', 'Avg Time to Merge (hrs)', \n",
    "    'Median Time to Merge (hrs)', 'Body Rate', 'Avg Body Length',\n",
    "    'Avg Quality Score', 'Unique Patterns', 'Unique Languages'\n",
    "]\n",
    "\n",
    "comparison_stats['Merge Rate (%)'] = (comparison_stats['Merge Rate'] * 100).round(1)\n",
    "comparison_stats['Body Rate (%)'] = (comparison_stats['Body Rate'] * 100).round(1)\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"PERFORMANCE PR COMPARISON: AI AGENTS VS HUMANS\")\n",
    "print(\"=\"*100)\n",
    "print(comparison_stats[[\n",
    "    'Total PRs', 'Merge Rate (%)', 'Median Time to Merge (hrs)',\n",
    "    'Body Rate (%)', 'Avg Body Length', 'Avg Quality Score',\n",
    "    'Unique Patterns', 'Unique Languages'\n",
    "]])\n",
    "\n",
    "# Statistical test for merge rate difference\n",
    "ai_merged = perf_prs[perf_prs['author_type'] == 'AI Agent']['is_merged']\n",
    "human_merged = perf_prs[perf_prs['author_type'] == 'Human']['is_merged']\n",
    "\n",
    "chi2, p_value = stats.chi2_contingency(\n",
    "    pd.crosstab(perf_prs['author_type'], perf_prs['is_merged'])\n",
    ")[:2]\n",
    "\n",
    "print(f\"\\n📊 Statistical Test (Merge Rate Difference):\")\n",
    "print(f\"  Chi-square: {chi2:.4f}\")\n",
    "print(f\"  P-value: {p_value:.6f}\")\n",
    "print(f\"  Significant: {'✓ Yes' if p_value < 0.05 else '✗ No'} (α=0.05)\")\n",
    "\n",
    "diff = comparison_stats.loc['AI Agent', 'Merge Rate (%)'] - comparison_stats.loc['Human', 'Merge Rate (%)']\n",
    "print(f\"  Effect: AI agents {'+' if diff > 0 else ''}{diff:.1f}% merge rate vs humans\")\n",
    "\n",
    "# Additional pattern-based insights\n",
    "print(f\"\\n🎯 Optimization Pattern Insights:\")\n",
    "\n",
    "# Filter successful GPT analyses\n",
    "analyzed = perf_prs[perf_prs['gpt_success'] == True]\n",
    "\n",
    "if len(analyzed) > 0:\n",
    "    # Most common patterns by author type\n",
    "    print(f\"\\nTop 3 Patterns by Author Type:\")\n",
    "    for author_type in ['AI Agent', 'Human']:\n",
    "        subset = analyzed[analyzed['author_type'] == author_type]\n",
    "        if len(subset) > 0:\n",
    "            top_patterns = subset['optimization_pattern'].value_counts().head(3)\n",
    "            print(f\"\\n  {author_type}:\")\n",
    "            for pattern, count in top_patterns.items():\n",
    "                pct = count / len(subset) * 100\n",
    "                # Truncate long pattern names\n",
    "                pattern_short = pattern[:50] + '...' if len(pattern) > 50 else pattern\n",
    "                print(f\"    {pattern_short:52s} {count:3d} ({pct:5.1f}%)\")\n",
    "    \n",
    "    # Pattern diversity comparison\n",
    "    ai_pattern_diversity = analyzed[analyzed['author_type'] == 'AI Agent']['optimization_pattern'].nunique()\n",
    "    human_pattern_diversity = analyzed[analyzed['author_type'] == 'Human']['optimization_pattern'].nunique()\n",
    "    \n",
    "    print(f\"\\nPattern Diversity:\")\n",
    "    print(f\"  AI Agents use {ai_pattern_diversity} unique patterns\")\n",
    "    print(f\"  Humans use {human_pattern_diversity} unique patterns\")\n",
    "    \n",
    "    # Success rate by pattern (top 5)\n",
    "    print(f\"\\nMerge Rate by Top 5 Optimization Patterns:\")\n",
    "    top_5_patterns = analyzed['optimization_pattern'].value_counts().head(5).index\n",
    "    \n",
    "    for pattern in top_5_patterns:\n",
    "        pattern_prs = analyzed[analyzed['optimization_pattern'] == pattern]\n",
    "        if len(pattern_prs) > 0:\n",
    "            merge_rate = pattern_prs['is_merged'].mean() * 100\n",
    "            pattern_short = pattern[:45] + '...' if len(pattern) > 45 else pattern\n",
    "            print(f\"  {pattern_short:47s} {merge_rate:5.1f}% ({len(pattern_prs):3d} PRs)\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n⚠ No successful GPT analyses available for pattern insights\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize high-level comparison\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# 1. Sample size\n",
    "perf_prs.groupby('author_type').size().plot(kind='bar', ax=axes[0,0], color=['#3498db', '#e74c3c'])\n",
    "axes[0,0].set_title('Number of Performance PRs', fontweight='bold', fontsize=12)\n",
    "axes[0,0].set_xlabel('')\n",
    "axes[0,0].set_ylabel('Count')\n",
    "axes[0,0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# 2. Merge rate\n",
    "merge_rates = perf_prs.groupby('author_type')['is_merged'].mean() * 100\n",
    "merge_rates.plot(kind='bar', ax=axes[0,1], color=['#3498db', '#e74c3c'])\n",
    "axes[0,1].set_title('Merge Rate', fontweight='bold', fontsize=12)\n",
    "axes[0,1].set_xlabel('')\n",
    "axes[0,1].set_ylabel('Percentage')\n",
    "axes[0,1].set_ylim(0, 100)\n",
    "axes[0,1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# 3. Time to merge\n",
    "merged_prs = perf_prs[perf_prs['is_merged']]\n",
    "merged_prs.boxplot(column='time_to_merge_days', by='author_type', ax=axes[0,2])\n",
    "axes[0,2].set_title('Time to Merge Distribution', fontweight='bold', fontsize=12)\n",
    "axes[0,2].set_xlabel('')\n",
    "axes[0,2].set_ylabel('Days')\n",
    "plt.sca(axes[0,2])\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "# 4. Description quality\n",
    "quality_scores = perf_prs.groupby('author_type')['description_quality_score'].mean()\n",
    "quality_scores.plot(kind='bar', ax=axes[1,0], color=['#3498db', '#e74c3c'])\n",
    "axes[1,0].set_title('Avg Description Quality Score', fontweight='bold', fontsize=12)\n",
    "axes[1,0].set_xlabel('')\n",
    "axes[1,0].set_ylabel('Score (0-5)')\n",
    "axes[1,0].set_ylim(0, 5)\n",
    "axes[1,0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ai_vs_human_overview.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Saved: ai_vs_human_overview.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ1: Adoption and Practices\n",
    "\n",
    "Analyzing PR size, description quality, and language usage patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ1.1: PR Size and Success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize by size\n",
    "perf_prs['pr_size_category'] = pd.cut(\n",
    "    perf_prs['body_length'],\n",
    "    bins=[0, 500, 1500, 5000, float('inf')],\n",
    "    labels=['Small (<500)', 'Medium (500-1500)', 'Large (1500-5000)', 'Very Large (>5000)']\n",
    ")\n",
    "\n",
    "# Analyze by author type and size\n",
    "size_analysis = perf_prs.groupby(['author_type', 'pr_size_category'], observed=True).agg({\n",
    "    'id': 'count',\n",
    "    'is_merged': 'mean',\n",
    "    'time_to_merge_hours': 'median'\n",
    "}).round(2)\n",
    "\n",
    "size_analysis.columns = ['Count', 'Merge Rate', 'Median Time (hrs)']\n",
    "size_analysis['Merge Rate (%)'] = (size_analysis['Merge Rate'] * 100).round(1)\n",
    "\n",
    "print(\"PR Size vs Success (AI vs Humans):\")\n",
    "print(\"=\"*80)\n",
    "print(size_analysis[['Count', 'Merge Rate (%)', 'Median Time (hrs)']])\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Size distribution\n",
    "size_dist = pd.crosstab(perf_prs['author_type'], perf_prs['pr_size_category'], normalize='index') * 100\n",
    "size_dist.T.plot(kind='bar', ax=axes[0], color=['#3498db', '#e74c3c'])\n",
    "axes[0].set_title('PR Size Distribution', fontweight='bold')\n",
    "axes[0].set_xlabel('Size Category')\n",
    "axes[0].set_ylabel('Percentage')\n",
    "axes[0].legend(title='Author Type')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Merge rate by size\n",
    "merge_by_size = perf_prs.groupby(['author_type', 'pr_size_category'], observed=True)['is_merged'].mean() * 100\n",
    "merge_by_size.unstack().T.plot(kind='bar', ax=axes[1], color=['#3498db', '#e74c3c'])\n",
    "axes[1].set_title('Merge Rate by PR Size', fontweight='bold')\n",
    "axes[1].set_xlabel('Size Category')\n",
    "axes[1].set_ylabel('Merge Rate (%)')\n",
    "axes[1].set_ylim(0, 100)\n",
    "axes[1].legend(title='Author Type')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('rq1_pr_size_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ1.2: Description Quality Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality score distribution\n",
    "quality_comparison = perf_prs.groupby(['author_type', 'description_quality_score']).agg({\n",
    "    'id': 'count',\n",
    "    'is_merged': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "quality_comparison.columns = ['Count', 'Merge Rate']\n",
    "quality_comparison['Merge Rate (%)'] = (quality_comparison['Merge Rate'] * 100).round(1)\n",
    "\n",
    "print(\"Description Quality Score vs Merge Rate:\")\n",
    "print(\"=\"*80)\n",
    "print(quality_comparison[['Count', 'Merge Rate (%)']])\n",
    "\n",
    "# Statistical test\n",
    "for author_type in ['AI Agent', 'Human']:\n",
    "    subset = perf_prs[perf_prs['author_type'] == author_type]\n",
    "    corr = subset[['description_quality_score', 'is_merged']].corr().iloc[0, 1]\n",
    "    print(f\"\\n{author_type} - Quality ↔ Merge correlation: {corr:.3f}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Quality score distribution\n",
    "perf_prs.groupby(['author_type', 'description_quality_score']).size().unstack(fill_value=0).T.plot(\n",
    "    kind='bar', ax=axes[0], color=['#3498db', '#e74c3c']\n",
    ")\n",
    "axes[0].set_title('Description Quality Distribution', fontweight='bold')\n",
    "axes[0].set_xlabel('Quality Score')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].legend(title='Author Type')\n",
    "\n",
    "# Merge rate by quality\n",
    "(perf_prs.groupby(['author_type', 'description_quality_score'])['is_merged'].mean().unstack() * 100).T.plot(\n",
    "    kind='line', marker='o', ax=axes[1], color=['#3498db', '#e74c3c'], linewidth=2, markersize=8\n",
    ")\n",
    "axes[1].set_title('Merge Rate by Quality Score', fontweight='bold')\n",
    "axes[1].set_xlabel('Quality Score')\n",
    "axes[1].set_ylabel('Merge Rate (%)')\n",
    "axes[1].set_ylim(0, 100)\n",
    "axes[1].legend(title='Author Type')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('rq1_quality_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ1.3: Programming Language Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language frequency by author type\n",
    "print(\"Top 10 Languages by Author Type:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for author_type in ['AI Agent', 'Human']:\n",
    "    subset = perf_prs[perf_prs['author_type'] == author_type]\n",
    "    lang_counts = subset['primary_language'].value_counts().head(10)\n",
    "    \n",
    "    print(f\"\\n{author_type}:\")\n",
    "    for lang, count in lang_counts.items():\n",
    "        pct = count / len(subset) * 100\n",
    "        merge_rate = subset[subset['primary_language'] == lang]['is_merged'].mean() * 100\n",
    "        print(f\"  {lang:15s} {count:4d} ({pct:5.1f}%) - Merge: {merge_rate:.1f}%\")\n",
    "\n",
    "# Compare top languages\n",
    "all_top_langs = list(set(\n",
    "    perf_prs[perf_prs['author_type'] == 'AI Agent']['primary_language'].value_counts().head(8).index.tolist() +\n",
    "    perf_prs[perf_prs['author_type'] == 'Human']['primary_language'].value_counts().head(8).index.tolist()\n",
    "))\n",
    "\n",
    "lang_comparison = []\n",
    "for lang in all_top_langs:\n",
    "    for author_type in ['AI Agent', 'Human']:\n",
    "        subset = perf_prs[(perf_prs['author_type'] == author_type) & (perf_prs['primary_language'] == lang)]\n",
    "        if len(subset) > 0:\n",
    "            lang_comparison.append({\n",
    "                'Language': lang,\n",
    "                'Author Type': author_type,\n",
    "                'Count': len(subset),\n",
    "                'Merge Rate (%)': subset['is_merged'].mean() * 100\n",
    "            })\n",
    "\n",
    "lang_df = pd.DataFrame(lang_comparison)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Count comparison\n",
    "lang_pivot_count = lang_df.pivot(index='Language', columns='Author Type', values='Count').fillna(0)\n",
    "lang_pivot_count.plot(kind='barh', ax=axes[0], color=['#3498db', '#e74c3c'])\n",
    "axes[0].set_title('Language Usage: AI vs Human', fontweight='bold')\n",
    "axes[0].set_xlabel('Number of PRs')\n",
    "axes[0].legend(title='Author Type')\n",
    "\n",
    "# Merge rate comparison\n",
    "lang_pivot_merge = lang_df.pivot(index='Language', columns='Author Type', values='Merge Rate (%)')\n",
    "lang_pivot_merge.plot(kind='barh', ax=axes[1], color=['#3498db', '#e74c3c'])\n",
    "axes[1].set_title('Merge Rate by Language: AI vs Human', fontweight='bold')\n",
    "axes[1].set_xlabel('Merge Rate (%)')\n",
    "axes[1].set_xlim(0, 100)\n",
    "axes[1].legend(title='Author Type')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('rq1_language_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ2: Optimization Patch Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RQ2: Optimization Patch Characteristics\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"OPTIMIZATION PATTERN ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Filter PRs with successful GPT analysis\n",
    "analyzed = perf_prs[perf_prs['gpt_success'] == True].copy()\n",
    "\n",
    "print(f\"\\nAnalyzing {len(analyzed):,} PRs with GPT pattern classification\")\n",
    "print(f\"  AI Agents: {(analyzed['author_type'] == 'AI Agent').sum():,}\")\n",
    "print(f\"  Humans: {(analyzed['author_type'] == 'Human').sum():,}\")\n",
    "\n",
    "# Most common optimization patterns\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Most Common Optimization Patterns:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for author_type in ['AI Agent', 'Human']:\n",
    "    subset = analyzed[analyzed['author_type'] == author_type]\n",
    "    pattern_counts = subset['optimization_pattern'].value_counts()\n",
    "    \n",
    "    print(f\"\\n{author_type} (top 10):\")\n",
    "    for pattern, count in pattern_counts.head(10).items():\n",
    "        pct = count / len(subset) * 100\n",
    "        pattern_short = pattern[:50] + '...' if len(pattern) > 50 else pattern\n",
    "        print(f\"  {pattern_short:52s} {count:4d} ({pct:5.1f}%)\")\n",
    "\n",
    "# Most common sub-patterns\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Most Common Sub-Patterns:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for author_type in ['AI Agent', 'Human']:\n",
    "    subset = analyzed[analyzed['author_type'] == author_type]\n",
    "    subpattern_counts = subset['optimization_subpattern'].value_counts()\n",
    "    \n",
    "    print(f\"\\n{author_type} (top 10):\")\n",
    "    for subpattern, count in subpattern_counts.head(10).items():\n",
    "        pct = count / len(subset) * 100\n",
    "        subpattern_short = subpattern[:50] + '...' if len(subpattern) > 50 else subpattern\n",
    "        print(f\"  {subpattern_short:52s} {count:4d} ({pct:5.1f}%)\")\n",
    "\n",
    "# Pattern diversity comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Pattern Diversity Metrics:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "diversity_stats = analyzed.groupby('author_type').agg({\n",
    "    'optimization_pattern': ['nunique', 'count'],\n",
    "    'optimization_subpattern': 'nunique'\n",
    "}).round(2)\n",
    "\n",
    "diversity_stats.columns = ['Unique Patterns', 'Total PRs', 'Unique Sub-Patterns']\n",
    "diversity_stats['Patterns per PR'] = (diversity_stats['Unique Patterns'] / diversity_stats['Total PRs']).round(3)\n",
    "\n",
    "print(diversity_stats)\n",
    "\n",
    "# Statistical test for pattern diversity\n",
    "ai_patterns = analyzed[analyzed['author_type'] == 'AI Agent']['optimization_pattern']\n",
    "human_patterns = analyzed[analyzed['author_type'] == 'Human']['optimization_pattern']\n",
    "\n",
    "# Pattern distribution comparison using Chi-square test\n",
    "if len(ai_patterns) > 0 and len(human_patterns) > 0:\n",
    "    # Get common patterns that appear in both groups\n",
    "    common_patterns = set(ai_patterns.unique()) & set(human_patterns.unique())\n",
    "    \n",
    "    if len(common_patterns) > 1:\n",
    "        # Filter to common patterns only\n",
    "        ai_filtered = ai_patterns[ai_patterns.isin(common_patterns)]\n",
    "        human_filtered = human_patterns[human_patterns.isin(common_patterns)]\n",
    "        \n",
    "        # Create contingency table\n",
    "        pattern_crosstab = pd.crosstab(\n",
    "            analyzed[analyzed['optimization_pattern'].isin(common_patterns)]['author_type'],\n",
    "            analyzed[analyzed['optimization_pattern'].isin(common_patterns)]['optimization_pattern']\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            chi2, p_value = stats.chi2_contingency(pattern_crosstab)[:2]\n",
    "            print(f\"\\nChi-square test (pattern distribution):\")\n",
    "            print(f\"  Chi-square: {chi2:.4f}\")\n",
    "            print(f\"  P-value: {p_value:.6f}\")\n",
    "            print(f\"  Significant: {'✓ Yes' if p_value < 0.05 else '✗ No'} (α=0.05)\")\n",
    "            \n",
    "            if p_value < 0.05:\n",
    "                print(f\"  → AI agents and humans show significantly different pattern preferences\")\n",
    "            else:\n",
    "                print(f\"  → AI agents and humans show similar pattern distributions\")\n",
    "        except:\n",
    "            print(\"\\n⚠ Unable to perform chi-square test (insufficient data)\")\n",
    "\n",
    "# Pattern co-occurrence analysis\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Key Insights:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for author_type in ['AI Agent', 'Human']:\n",
    "    subset = analyzed[analyzed['author_type'] == author_type]\n",
    "    \n",
    "    print(f\"\\n{author_type}:\")\n",
    "    \n",
    "    # Most common pattern\n",
    "    top_pattern = subset['optimization_pattern'].value_counts().index[0]\n",
    "    top_count = subset['optimization_pattern'].value_counts().values[0]\n",
    "    print(f\"  Most common pattern: {top_pattern}\")\n",
    "    print(f\"    {top_count} PRs ({top_count/len(subset)*100:.1f}%)\")\n",
    "    \n",
    "    # Most common sub-pattern\n",
    "    top_subpattern = subset['optimization_subpattern'].value_counts().index[0]\n",
    "    top_sub_count = subset['optimization_subpattern'].value_counts().values[0]\n",
    "    print(f\"  Most common sub-pattern: {top_subpattern}\")\n",
    "    print(f\"    {top_sub_count} PRs ({top_sub_count/len(subset)*100:.1f}%)\")\n",
    "    \n",
    "    # Pattern diversity\n",
    "    unique_patterns = subset['optimization_pattern'].nunique()\n",
    "    unique_subpatterns = subset['optimization_subpattern'].nunique()\n",
    "    print(f\"  Pattern diversity: {unique_patterns} patterns, {unique_subpatterns} sub-patterns\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ3: Testing and Evaluation Behavior [TODO]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ4: Review Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review time analysis\n",
    "merged_prs = perf_prs[perf_prs['is_merged']].copy()\n",
    "\n",
    "review_stats = merged_prs.groupby('author_type')['time_to_merge_hours'].describe()\n",
    "print(\"Review Time Statistics (Merged PRs):\")\n",
    "print(\"=\"*80)\n",
    "print(review_stats)\n",
    "\n",
    "# Statistical test\n",
    "ai_times = merged_prs[merged_prs['author_type'] == 'AI Agent']['time_to_merge_hours'].dropna()\n",
    "human_times = merged_prs[merged_prs['author_type'] == 'Human']['time_to_merge_hours'].dropna()\n",
    "\n",
    "if len(ai_times) > 0 and len(human_times) > 0:\n",
    "    statistic, p_value = stats.mannwhitneyu(ai_times, human_times, alternative='two-sided')\n",
    "    print(f\"\\nMann-Whitney U test (time to merge):\")\n",
    "    print(f\"  AI Agent median: {ai_times.median():.1f} hours ({ai_times.median()/24:.1f} days)\")\n",
    "    print(f\"  Human median: {human_times.median():.1f} hours ({human_times.median()/24:.1f} days)\")\n",
    "    print(f\"  P-value: {p_value:.6f}\")\n",
    "    print(f\"  Significant: {'Yes' if p_value < 0.05 else 'No'} (α=0.05)\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "merged_prs.boxplot(column='time_to_merge_days', by='author_type', ax=ax)\n",
    "ax.set_title('Time to Merge Distribution: AI vs Human', fontweight='bold', fontsize=14)\n",
    "ax.set_xlabel('Author Type', fontsize=12)\n",
    "ax.set_ylabel('Days to Merge', fontsize=12)\n",
    "ax.set_ylim(0, merged_prs['time_to_merge_days'].quantile(0.95))\n",
    "plt.suptitle('')\n",
    "plt.tight_layout()\n",
    "plt.savefig('rq4_review_time_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ5: Failure Patterns [Needs to update]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Failure analysis\n",
    "failed_prs = perf_prs[(perf_prs['state'] == 'closed') & (~perf_prs['is_merged'])].copy()\n",
    "successful_prs = perf_prs[perf_prs['is_merged']].copy()\n",
    "\n",
    "print(\"Failure Analysis by Author Type:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "failure_stats = perf_prs.groupby('author_type').agg({\n",
    "    'id': 'count',\n",
    "    'is_merged': 'mean'\n",
    "}).round(3)\n",
    "failure_stats.columns = ['Total', 'Success Rate']\n",
    "failure_stats['Failure Rate (%)'] = ((1 - failure_stats['Success Rate']) * 100).round(1)\n",
    "\n",
    "print(failure_stats[['Total', 'Failure Rate (%)']])\n",
    "\n",
    "# Characteristic comparison\n",
    "comparison = []\n",
    "for author_type in ['AI Agent', 'Human']:\n",
    "    success = successful_prs[successful_prs['author_type'] == author_type]\n",
    "    fail = failed_prs[failed_prs['author_type'] == author_type]\n",
    "    \n",
    "    comparison.append({\n",
    "        'Author Type': author_type,\n",
    "        'Outcome': 'Successful',\n",
    "        'Avg Body Length': success['body_length'].mean(),\n",
    "        'Has Body (%)': success['has_body'].mean() * 100,\n",
    "        'Avg Quality Score': success['description_quality_score'].mean(),\n",
    "    })\n",
    "    \n",
    "    if len(fail) > 0:\n",
    "        comparison.append({\n",
    "            'Author Type': author_type,\n",
    "            'Outcome': 'Failed',\n",
    "            'Avg Body Length': fail['body_length'].mean(),\n",
    "            'Has Body (%)': fail['has_body'].mean() * 100,\n",
    "            'Avg Quality Score': fail['description_quality_score'].mean(),\n",
    "        })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison).round(2)\n",
    "\n",
    "print(\"\\nCharacteristic Comparison: Successful vs Failed PRs\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df)\n",
    "\n",
    "# Key risk factors\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY RISK FACTORS:\")\n",
    "\n",
    "for author_type in ['AI Agent', 'Human']:\n",
    "    subset = perf_prs[perf_prs['author_type'] == author_type]\n",
    "    print(f\"\\n{author_type}:\")\n",
    "    \n",
    "    # Missing description\n",
    "    no_body_fail = 1 - subset[~subset['has_body']]['is_merged'].mean()\n",
    "    has_body_fail = 1 - subset[subset['has_body']]['is_merged'].mean()\n",
    "    if not pd.isna(no_body_fail) and not pd.isna(has_body_fail):\n",
    "        print(f\"  Missing description: +{(no_body_fail - has_body_fail)*100:.1f}% failure rate\")\n",
    "    \n",
    "    # Low quality\n",
    "    low_qual = subset[subset['description_quality_score'] < 2]\n",
    "    high_qual = subset[subset['description_quality_score'] >= 3]\n",
    "    if len(low_qual) > 0 and len(high_qual) > 0:\n",
    "        low_fail = 1 - low_qual['is_merged'].mean()\n",
    "        high_fail = 1 - high_qual['is_merged'].mean()\n",
    "        print(f\"  Low quality (score <2): +{(low_fail - high_fail)*100:.1f}% failure rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"PERFORMANCE PR ANALYSIS: KEY FINDINGS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Calculate key metrics\n",
    "ai_stats = perf_prs[perf_prs['author_type'] == 'AI Agent']\n",
    "human_stats = perf_prs[perf_prs['author_type'] == 'Human']\n",
    "\n",
    "print(f\"\\n1. OVERALL SUCCESS\")\n",
    "print(f\"   AI Agents: {ai_stats['is_merged'].mean()*100:.1f}% merge rate ({ai_stats['is_merged'].sum():,}/{len(ai_stats):,})\")\n",
    "print(f\"   Humans: {human_stats['is_merged'].mean()*100:.1f}% merge rate ({human_stats['is_merged'].sum():,}/{len(human_stats):,})\")\n",
    "diff = ai_stats['is_merged'].mean() - human_stats['is_merged'].mean()\n",
    "print(f\"   Difference: {'+' if diff > 0 else ''}{diff*100:.1f}% points\")\n",
    "\n",
    "print(f\"\\n2. DESCRIPTION QUALITY\")\n",
    "print(f\"   AI Agents: {ai_stats['description_quality_score'].mean():.2f}/5 avg quality\")\n",
    "print(f\"   Humans: {human_stats['description_quality_score'].mean():.2f}/5 avg quality\")\n",
    "print(f\"   AI with detailed descriptions: {ai_stats['has_body'].mean()*100:.1f}%\")\n",
    "print(f\"   Humans with detailed descriptions: {human_stats['has_body'].mean()*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n3. TESTING & MEASUREMENT\")\n",
    "print(f\"TODO: Add testing & measurement statistics here\")\n",
    "\n",
    "print(f\"\\n4. REVIEW TIME\")\n",
    "ai_merged = ai_stats[ai_stats['is_merged']]\n",
    "human_merged = human_stats[human_stats['is_merged']]\n",
    "if len(ai_merged) > 0:\n",
    "    print(f\"   AI Agents median: {ai_merged['time_to_merge_hours'].median():.1f} hours ({ai_merged['time_to_merge_days'].median():.1f} days)\")\n",
    "if len(human_merged) > 0:\n",
    "    print(f\"   Humans median: {human_merged['time_to_merge_hours'].median():.1f} hours ({human_merged['time_to_merge_days'].median():.1f} days)\")\n",
    "\n",
    "print(f\"\\n5. TOP LANGUAGES\")\n",
    "print(f\"   AI Agents: {', '.join(ai_stats['primary_language'].value_counts().head(3).index.tolist())}\")\n",
    "print(f\"   Humans: {', '.join(human_stats['primary_language'].value_counts().head(3).index.tolist())}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export datasets\n",
    "raw_export_cols = [\n",
    "    'id', 'number', 'title', 'body', 'classification_reason', 'agent', 'author_type',\n",
    "    'state', 'created_at', 'merged_at', 'is_merged', 'primary_language', 'repo_id', 'html_url'\n",
    "]\n",
    "\n",
    "# Ground-truth subset (no engineered annotations)\n",
    "perf_prs[raw_export_cols].to_csv('performance_prs_ai_vs_human_raw.csv', index=False)\n",
    "print(\"✓ Exported: performance_prs_ai_vs_human_raw.csv (ground-truth subset)\")\n",
    "\n",
    "# Annotated dataset with engineered features\n",
    "export_cols = [\n",
    "    'id', 'number', 'title', 'body', 'classification_reason', 'agent', 'author_type', 'state',\n",
    "    'created_at', 'merged_at', 'is_merged', 'time_to_merge_days',\n",
    "    'primary_language', 'repo_id', 'html_url',\n",
    "    'has_body', 'body_length', 'description_quality_score',\n",
    "    'optimization_pattern', 'optimization_subpattern',\n",
    "    'gpt_explanation', 'gpt_comparison'\n",
    "]\n",
    "\n",
    "perf_prs[export_cols].to_csv('performance_prs_ai_vs_human.csv', index=False)\n",
    "print(\"✓ Exported: performance_prs_ai_vs_human.csv (annotated dataset)\")\n",
    "\n",
    "# Export summary statistics\n",
    "comparison_stats.to_csv('summary_ai_vs_human.csv')\n",
    "print(\"✓ Exported: summary_ai_vs_human.csv\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Generated files:\")\n",
    "print(f\"  • performance_prs_ai_vs_human_raw.csv - Ground-truth subset\")\n",
    "print(f\"  • performance_prs_ai_vs_human.csv - Annotated dataset\")\n",
    "print(f\"  • summary_ai_vs_human.csv - Summary statistics\")\n",
    "print(f\"  • ai_vs_human_overview.png - Overview comparison\")\n",
    "print(f\"  • rq1_pr_size_comparison.png - PR size analysis\")\n",
    "print(f\"  • rq1_quality_comparison.png - Quality analysis\")\n",
    "print(f\"  • rq1_language_comparison.png - Language analysis\")\n",
    "print(f\"  • rq3_testing_comparison.png - Testing practices\")\n",
    "print(f\"  • rq4_review_time_comparison.png - Review dynamics\")\n",
    "print(f\"Total performance PRs analyzed: {len(perf_prs):,}\")\n",
    "print(f\"  AI Agents: {len(ai_perf_prs):,}\")\n",
    "print(f\"  Humans: {len(human_perf_prs):,}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
