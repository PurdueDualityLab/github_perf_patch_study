{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "109b3abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading AIDev datasets...\n",
      "\n",
      "✓ Performance PR IDs to process: 428\n",
      "\n",
      "Processing commit details (filtering to performance PRs only)...\n",
      "  Total commit records in dataset: 712,528\n",
      "  Filtered to performance PRs: 8,015 commit records\n",
      "  Unique performance PRs with commits: 423\n",
      "  Filtered out null filenames: 46 records removed\n",
      "  Remaining after filename filter: 7,969 commit records\n",
      "  Filtered out config-only commits: 39 records removed\n",
      "  Remaining after config filter: 7,930 commit records\n",
      "  Filtered out merge commits: 2,945 records removed\n",
      "  Remaining after merge filter: 4,985 commit records\n",
      "  Removed deleted repo PR commits: 74 records removed\n",
      "  Remaining after deleted repo filter: 4,911 commit records\n",
      "  Unique performance PRs after all filters: 407\n",
      "  ✓ Aggregated to 407 unique performance PRs\n",
      "  Avg commits per PR: 12.1\n",
      "  AI Agent PRs with commit data: 324 / 340 (95.3%)\n",
      "  Human PRs with commit data: 83 / 88 (94.3%)\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "Original Performance PRs:\n",
      "  AI Agent: 340\n",
      "  Human: 88\n",
      "  Total: 428\n",
      "\n",
      "After Commit Filtering:\n",
      "✓ AI Agent Performance PRs: 324\n",
      "✓ Human Performance PRs: 83\n",
      "✓ Total Performance PRs: 407\n",
      "\n",
      "AI Agent Distribution:\n",
      "  OpenAI_Codex           205 ( 63.3%)\n",
      "  Devin                   59 ( 18.2%)\n",
      "  Copilot                 37 ( 11.4%)\n",
      "  Cursor                  20 (  6.2%)\n",
      "  Claude_Code              3 (  0.9%)\n",
      "\n",
      "================================================================================\n",
      "COMMIT STATISTICS\n",
      "================================================================================\n",
      "\n",
      "AI Agent:\n",
      "  PRs with commit data: 324\n",
      "  Avg commits per PR: 13.3\n",
      "  Median commits per PR: 4.0\n",
      "  Avg additions: 368 lines\n",
      "  Median additions: 61 lines\n",
      "  Avg deletions: 282 lines\n",
      "  Median deletions: 25 lines\n",
      "\n",
      "Human:\n",
      "  PRs with commit data: 83\n",
      "  Avg commits per PR: 7.3\n",
      "  Median commits per PR: 2.0\n",
      "  Avg additions: 141 lines\n",
      "  Median additions: 29 lines\n",
      "  Avg deletions: 75 lines\n",
      "  Median deletions: 19 lines\n",
      "\n",
      "Saved 407 valid performance PR IDs to valid_perf_pr_ids.csv\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Load datasets\n",
    "print(\"Loading AIDev datasets...\")\n",
    "\n",
    "# AI Agent PRs\n",
    "pr_df = pd.read_parquet(\"hf://datasets/hao-li/AIDev/pull_request.parquet\")\n",
    "pr_task_type_df = pd.read_parquet(\"hf://datasets/hao-li/AIDev/pr_task_type.parquet\")\n",
    "ai_perf_prs = (\n",
    "    pr_df\n",
    "    .merge(\n",
    "        pr_task_type_df[[\"id\", \"type\", \"reason\"]],\n",
    "        on=\"id\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    .query(\"type == 'perf'\")\n",
    "    .copy()\n",
    ")\n",
    "ai_perf_prs['classification_reason'] = ai_perf_prs['reason']\n",
    "ai_perf_prs['author_type'] = 'AI Agent'\n",
    "\n",
    "# Human PRs\n",
    "human_pr_df = pd.read_parquet(\"hf://datasets/hao-li/AIDev/human_pull_request.parquet\")\n",
    "human_pr_task_type_df = pd.read_parquet(\"hf://datasets/hao-li/AIDev/human_pr_task_type.parquet\")\n",
    "human_perf_prs = (\n",
    "    human_pr_df\n",
    "    .merge(\n",
    "        human_pr_task_type_df[[\"id\", \"type\", \"reason\"]],\n",
    "        on=\"id\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    .query(\"type == 'perf'\")\n",
    "    .copy()\n",
    ")\n",
    "human_perf_prs['classification_reason'] = human_perf_prs['reason']\n",
    "human_perf_prs['author_type'] = 'Human'\n",
    "human_perf_prs['agent'] = 'Human'\n",
    "\n",
    "# Store original counts\n",
    "original_ai_count = len(ai_perf_prs)\n",
    "original_human_count = len(human_perf_prs)\n",
    "\n",
    "# Repository data for language info\n",
    "all_repo_df = pd.read_parquet(\"hf://datasets/hao-li/AIDev/all_repository.parquet\")\n",
    "\n",
    "# Get list of performance PR IDs we care about\n",
    "perf_pr_ids = set(ai_perf_prs['id'].tolist() + human_perf_prs['id'].tolist())\n",
    "print(f\"\\n✓ Performance PR IDs to process: {len(perf_pr_ids):,}\")\n",
    "\n",
    "# PR commits details - FILTER FIRST, then aggregate\n",
    "print(\"\\nProcessing commit details (filtering to performance PRs only)...\")\n",
    "pr_commits_details = pd.read_parquet(\"hf://datasets/hao-li/AIDev/pr_commit_details.parquet\")\n",
    "\n",
    "# Pr commit details for human PRs\n",
    "human_pr_commit_details = pd.read_parquet(\"./datasets/human_pr/human_pr_commit_details.parquet\")\n",
    "\n",
    "\n",
    "pr_commits_details = pd.concat(\n",
    "    [pr_commits_details, human_pr_commit_details],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "if 'pr_id' in pr_commits_details.columns:\n",
    "    print(f\"  Total commit records in dataset: {len(pr_commits_details):,}\")\n",
    "    \n",
    "    # FILTER: Keep only commits for performance PRs\n",
    "    pr_commits_filtered = pr_commits_details[pr_commits_details['pr_id'].isin(perf_pr_ids)].copy()\n",
    "    print(f\"  Filtered to performance PRs: {len(pr_commits_filtered):,} commit records\")\n",
    "    print(f\"  Unique performance PRs with commits: {pr_commits_filtered['pr_id'].nunique():,}\")\n",
    "    \n",
    "    # ADDITIONAL FILTERING: Remove commits with null filename\n",
    "    if 'filename' in pr_commits_filtered.columns:\n",
    "        before_filename_filter = len(pr_commits_filtered)\n",
    "        pr_commits_filtered = pr_commits_filtered[pr_commits_filtered['filename'].notna()].copy()\n",
    "        print(f\"  Filtered out null filenames: {before_filename_filter - len(pr_commits_filtered):,} records removed\")\n",
    "        print(f\"  Remaining after filename filter: {len(pr_commits_filtered):,} commit records\")\n",
    "        \n",
    "    # ADDITIONAL FILTERING: Remove config/metadata-only files\n",
    "    if 'filename' in pr_commits_filtered.columns:\n",
    "        before_config_filter = len(pr_commits_filtered)\n",
    "        \n",
    "        # Define patterns for non-code files to exclude\n",
    "        config_patterns = [\n",
    "            r'^\\.mvn/',                          # Maven wrapper configs\n",
    "            r'^\\.gradle/',                       # Gradle configs\n",
    "            r'^\\.idea/',                         # IntelliJ configs\n",
    "            r'^\\.vscode/',                       # VSCode configs\n",
    "            r'^\\.github/workflows/',             # GitHub Actions (unless it's code)\n",
    "            r'\\.properties$',                    # Properties files\n",
    "            r'\\.xml$',                           # XML config files (pom.xml, etc.)\n",
    "            r'\\.yml$',                           # YAML configs\n",
    "            r'\\.yaml$',                          # YAML configs\n",
    "            r'\\.json$',                          # JSON configs (package.json, etc.)\n",
    "            r'\\.md$',                            # Markdown docs\n",
    "            r'\\.txt$',                           # Text files\n",
    "            r'\\.gitignore$',                     # Git configs\n",
    "            r'\\.dockerignore$',                  # Docker ignore files\n",
    "            r'/Dockerfile$',                     # Dockerfiles (anywhere in path)\n",
    "            r'^Dockerfile$',                     # Dockerfile at root\n",
    "            r'/docker-compose',                  # Docker compose (anywhere)\n",
    "            r'^docker-compose',                  # Docker compose at root\n",
    "            r'\\.lock$',                          # Lock files (package-lock, yarn.lock)\n",
    "            r'^LICENSE',                         # License files\n",
    "            r'^README',                          # README files\n",
    "        ]\n",
    "        \n",
    "        config_pattern = '|'.join(config_patterns)\n",
    "        \n",
    "        # Mark config files\n",
    "        pr_commits_filtered['is_config_file'] = pr_commits_filtered['filename'].str.contains(\n",
    "            config_pattern, case=False, na=False, regex=True\n",
    "        )\n",
    "        \n",
    "        # Keep track of which files are code files per PR\n",
    "        pr_commits_filtered['is_code_file'] = ~pr_commits_filtered['is_config_file']\n",
    "        \n",
    "        # For each PR, check if it has ANY code files\n",
    "        pr_has_code = pr_commits_filtered.groupby('pr_id')['is_code_file'].any().reset_index()\n",
    "        pr_has_code.columns = ['pr_id', 'has_code_files']\n",
    "        \n",
    "        # Filter to keep only PRs that have at least one code file\n",
    "        pr_commits_filtered = pr_commits_filtered.merge(pr_has_code, on='pr_id', how='left')\n",
    "        pr_commits_filtered = pr_commits_filtered[pr_commits_filtered['has_code_files']].copy()\n",
    "        \n",
    "        # Clean up temporary columns\n",
    "        pr_commits_filtered = pr_commits_filtered.drop(columns=['is_config_file', 'is_code_file', 'has_code_files'])\n",
    "        \n",
    "        print(f\"  Filtered out config-only commits: {before_config_filter - len(pr_commits_filtered):,} records removed\")\n",
    "        print(f\"  Remaining after config filter: {len(pr_commits_filtered):,} commit records\")\n",
    "    \n",
    "    # ADDITIONAL FILTERING: Remove merge commits\n",
    "    if 'message' in pr_commits_filtered.columns:\n",
    "        before_merge_filter = len(pr_commits_filtered)\n",
    "        # Common merge commit patterns\n",
    "        merge_patterns = [\n",
    "            r'^Merge\\s+branch',\n",
    "            r'^Merge\\s+pull\\s+request',\n",
    "            r'^Merge\\s+remote-tracking\\s+branch',\n",
    "            r'^Merge\\s+.*\\s+into\\s+',\n",
    "            r\"^Merged\\s+in\\s+\",\n",
    "        ]\n",
    "        merge_pattern = '|'.join(merge_patterns)\n",
    "        pr_commits_filtered = pr_commits_filtered[\n",
    "            ~pr_commits_filtered['message'].str.match(merge_pattern, case=False, na=False)\n",
    "        ].copy()\n",
    "        print(f\"  Filtered out merge commits: {before_merge_filter - len(pr_commits_filtered):,} records removed\")\n",
    "        print(f\"  Remaining after merge filter: {len(pr_commits_filtered):,} commit records\")\n",
    "\n",
    "        # FINAL FILTER: drop PRs whose repositories were deleted\n",
    "        deleted_repo_pr_ids = {3271610326, 3209206554}\n",
    "        before_deleted_repo_filter = len(pr_commits_filtered)\n",
    "        pr_commits_filtered = pr_commits_filtered[~pr_commits_filtered['pr_id'].isin(deleted_repo_pr_ids)].copy()\n",
    "        removed_deleted_repo_commits = before_deleted_repo_filter - len(pr_commits_filtered)\n",
    "        if removed_deleted_repo_commits > 0:\n",
    "            print(f\"  Removed deleted repo PR commits: {removed_deleted_repo_commits:,} records removed\")\n",
    "        print(f\"  Remaining after deleted repo filter: {len(pr_commits_filtered):,} commit records\")\n",
    "    \n",
    "    print(f\"  Unique performance PRs after all filters: {pr_commits_filtered['pr_id'].nunique():,}\")\n",
    "    \n",
    "    if len(pr_commits_filtered) > 0:\n",
    "        # AGGREGATE: Now aggregate only the filtered commits\n",
    "        commit_aggregated = pr_commits_filtered.groupby('pr_id').agg({\n",
    "            'additions': 'sum',      # Total lines added across all commits\n",
    "            'deletions': 'sum',      # Total lines deleted across all commits\n",
    "            'patch': lambda x: '\\n\\n'.join([str(p) for p in x if pd.notna(p)])  # Concatenate all patches\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Add derived metrics\n",
    "        commit_aggregated['num_commits'] = pr_commits_filtered.groupby('pr_id').size().values\n",
    "        \n",
    "        # Calculate patch length (for analysis)\n",
    "        commit_aggregated['patch_length'] = commit_aggregated['patch'].str.len()\n",
    "        \n",
    "        print(f\"  ✓ Aggregated to {len(commit_aggregated):,} unique performance PRs\")\n",
    "        print(f\"  Avg commits per PR: {commit_aggregated['num_commits'].mean():.1f}\")\n",
    "        \n",
    "        # Merge commit stats into AI Agent PR table\n",
    "        ai_perf_prs = ai_perf_prs.merge(\n",
    "            commit_aggregated,\n",
    "            left_on='id',\n",
    "            right_on='pr_id',\n",
    "            how='left'\n",
    "        )\n",
    "        if 'pr_id' in ai_perf_prs.columns:\n",
    "            ai_perf_prs = ai_perf_prs.drop(columns=['pr_id'])\n",
    "        \n",
    "        # Filter to keep only PRs with commit data\n",
    "        ai_before_filter = len(ai_perf_prs)\n",
    "        ai_with_commits = ai_perf_prs[ai_perf_prs['additions'].notna()].copy()\n",
    "        print(f\"  AI Agent PRs with commit data: {len(ai_with_commits):,} / {ai_before_filter:,} ({len(ai_with_commits)/ai_before_filter*100:.1f}%)\")\n",
    "        \n",
    "        # Merge commit stats into Human PR table\n",
    "        human_perf_prs = human_perf_prs.merge(\n",
    "            commit_aggregated,\n",
    "            left_on='id',\n",
    "            right_on='pr_id',\n",
    "            how='left'\n",
    "        )\n",
    "        if 'pr_id' in human_perf_prs.columns:\n",
    "            human_perf_prs = human_perf_prs.drop(columns=['pr_id'])\n",
    "        \n",
    "        # Filter to keep only PRs with commit data\n",
    "        human_before_filter = len(human_perf_prs)\n",
    "        human_with_commits = human_perf_prs[human_perf_prs['additions'].notna()].copy()\n",
    "        print(f\"  Human PRs with commit data: {len(human_with_commits):,} / {human_before_filter:,} ({len(human_with_commits)/human_before_filter*100:.1f}%)\")\n",
    "    else:\n",
    "        print(\"  ⚠ No commits found for performance PRs after filtering\")\n",
    "        # Create empty dataframes with same structure\n",
    "        ai_with_commits = ai_perf_prs.iloc[0:0].copy()\n",
    "        human_with_commits = human_perf_prs.iloc[0:0].copy()\n",
    "    \n",
    "else:\n",
    "    print('⚠ pr_commit_details missing pr_id column; skipping commit merges.')\n",
    "    # Create empty dataframes\n",
    "    ai_with_commits = ai_perf_prs.iloc[0:0].copy()\n",
    "    human_with_commits = human_perf_prs.iloc[0:0].copy()\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Original Performance PRs:\")\n",
    "print(f\"  AI Agent: {original_ai_count:,}\")\n",
    "print(f\"  Human: {original_human_count:,}\")\n",
    "print(f\"  Total: {original_ai_count + original_human_count:,}\")\n",
    "print(f\"\\nAfter Commit Filtering:\")\n",
    "print(f\"✓ AI Agent Performance PRs: {len(ai_with_commits):,}\")\n",
    "print(f\"✓ Human Performance PRs: {len(human_with_commits):,}\")\n",
    "print(f\"✓ Total Performance PRs: {len(ai_with_commits) + len(human_with_commits):,}\")\n",
    "\n",
    "# Distribution by AI agent\n",
    "if len(ai_with_commits) > 0:\n",
    "    print(f\"\\nAI Agent Distribution:\")\n",
    "    for agent, count in ai_with_commits['agent'].value_counts().items():\n",
    "        pct = count / len(ai_with_commits) * 100\n",
    "        print(f\"  {agent:20s} {count:5,d} ({pct:5.1f}%)\")\n",
    "\n",
    "# Commit statistics summary\n",
    "if len(ai_with_commits) > 0 or len(human_with_commits) > 0:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"COMMIT STATISTICS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    for author_type, df in [('AI Agent', ai_with_commits), ('Human', human_with_commits)]:\n",
    "        if len(df) > 0:\n",
    "            print(f\"\\n{author_type}:\")\n",
    "            print(f\"  PRs with commit data: {len(df):,}\")\n",
    "            print(f\"  Avg commits per PR: {df['num_commits'].mean():.1f}\")\n",
    "            print(f\"  Median commits per PR: {df['num_commits'].median():.1f}\")\n",
    "            print(f\"  Avg additions: {df['additions'].mean():.0f} lines\")\n",
    "            print(f\"  Median additions: {df['additions'].median():.0f} lines\")\n",
    "            print(f\"  Avg deletions: {df['deletions'].mean():.0f} lines\")\n",
    "            print(f\"  Median deletions: {df['deletions'].median():.0f} lines\")\n",
    "\n",
    "# Save valid PR IDs for reuse\n",
    "valid_ids_output_path = 'valid_perf_pr_ids.csv'\n",
    "valid_pr_ids_series = pd.concat([\n",
    "    ai_with_commits['id'],\n",
    "    human_with_commits['id'],\n",
    "], ignore_index=True).dropna().drop_duplicates()\n",
    "valid_pr_ids_df = (\n",
    "    valid_pr_ids_series.to_frame(name='id')\n",
    "    .sort_values('id')\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "valid_pr_ids_df.to_csv(valid_ids_output_path, index=False)\n",
    "print(f\"\\nSaved {len(valid_pr_ids_df):,} valid performance PR IDs to {valid_ids_output_path}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
