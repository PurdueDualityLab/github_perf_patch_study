{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "975a3c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment ready!\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "!pip install pandas numpy matplotlib seaborn scipy wordcloud pyarrow datasets PyGithub python-dotenv --quiet\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import re\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlparse\n",
    "import time\n",
    "from github import Github\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "# Plot settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Environment ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f4e3e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compatibility shim: some versions of fsspec don't expose url_to_fs at top-level.\n",
    "# This ensures code that expects fsspec.url_to_fs (used by some IO backends) continues to work.\n",
    "try:\n",
    "    import fsspec\n",
    "    if not hasattr(fsspec, \"url_to_fs\"):\n",
    "        try:\n",
    "            from fsspec.core import url_to_fs as _url_to_fs\n",
    "        except Exception:\n",
    "            try:\n",
    "                import fsspec.core as _core\n",
    "                _url_to_fs = _core.url_to_fs\n",
    "            except Exception:\n",
    "                # Fallback shim: create a minimal url_to_fs that returns a filesystem and the path.\n",
    "                def _url_to_fs(url, **kwargs):\n",
    "                    protocol = url.split(\"://\")[0] if \"://\" in url else \"file\"\n",
    "                    fs = fsspec.filesystem(protocol)\n",
    "                    return fs, url\n",
    "        fsspec.url_to_fs = _url_to_fs\n",
    "except Exception:\n",
    "    # If anything goes wrong, continue without failing here; subsequent IO calls will raise their own errors.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f91e93a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9l/dq9r8b6n1bx49mhvn82gzl180000gn/T/ipykernel_23319/4070930473.py:6: DeprecationWarning: Argument login_or_token is deprecated, please use auth=github.Auth.Token(...) instead\n",
      "  gh = Github(GITHUB_API_TOKEN)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() \n",
    "GITHUB_API_TOKEN = os.environ.get(\"GITHUB_TOKEN\")\n",
    "gh = Github(GITHUB_API_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ce11e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_owner_repo(repo_url: str, html_url: str):\n",
    "    for url in (repo_url, html_url):\n",
    "        if not isinstance(url, str):\n",
    "            continue\n",
    "        try:\n",
    "            path = urlparse(url).path.strip(\"/\")\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        parts = path.split(\"/\")\n",
    "\n",
    "        # API: /repos/OWNER/REPO\n",
    "        if \"repos\" in parts:\n",
    "            idx = parts.index(\"repos\")\n",
    "            if len(parts) >= idx + 3:\n",
    "                owner = parts[idx + 1]\n",
    "                repo = parts[idx + 2]\n",
    "                return owner, repo\n",
    "\n",
    "        # Web: /OWNER/REPO/pull/123\n",
    "        if len(parts) >= 2:\n",
    "            owner = parts[0]\n",
    "            repo = parts[1]\n",
    "            return owner, repo\n",
    "\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab3bb2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading AIDev datasets...\n",
      "len = 88\n",
      "Missing repos: 0\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "print(\"Loading AIDev datasets...\")\n",
    "\n",
    "\n",
    "# Human PRs\n",
    "human_pr_df = pd.read_parquet(\"hf://datasets/hao-li/AIDev/human_pull_request.parquet\")\n",
    "human_pr_task_type_df = pd.read_parquet(\"hf://datasets/hao-li/AIDev/human_pr_task_type.parquet\")\n",
    "\n",
    "perf_hummans = (\n",
    "    human_pr_df\n",
    "    .merge(\n",
    "        human_pr_task_type_df[[\"id\", \"type\"]], on=\"id\", \n",
    "    )\n",
    "    .query(\"type == 'perf'\")\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "perf_hummans[[\"repo_owner\", \"repo_name\"]] = perf_hummans.apply(\n",
    "    lambda row: pd.Series(extract_owner_repo(row['repo_url'], row['html_url'])), axis=1\n",
    ")\n",
    "\n",
    "print(f\"len = {len(perf_hummans)}\")\n",
    "\n",
    "missing_repos = perf_hummans[perf_hummans['repo_owner'].isna() | perf_hummans['repo_name'].isna()]\n",
    "print(f\"Missing repos: {len(missing_repos)}\")\n",
    "\n",
    "#Reconstruct pr_commit_df from human_pr_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "527183b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>number</th>\n",
       "      <th>title</th>\n",
       "      <th>user</th>\n",
       "      <th>user_id</th>\n",
       "      <th>state</th>\n",
       "      <th>created_at</th>\n",
       "      <th>closed_at</th>\n",
       "      <th>merged_at</th>\n",
       "      <th>repo_url</th>\n",
       "      <th>html_url</th>\n",
       "      <th>body</th>\n",
       "      <th>agent</th>\n",
       "      <th>type</th>\n",
       "      <th>repo_owner</th>\n",
       "      <th>repo_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2486573779</td>\n",
       "      <td>90516</td>\n",
       "      <td>ref(perf-issues): Consolidate File IO override option</td>\n",
       "      <td>leeandher</td>\n",
       "      <td>35509934</td>\n",
       "      <td>closed</td>\n",
       "      <td>2025-04-28T18:17:36Z</td>\n",
       "      <td>2025-04-28T19:22:01Z</td>\n",
       "      <td>2025-04-28T19:22:01Z</td>\n",
       "      <td>https://api.github.com/repos/getsentry/sentry</td>\n",
       "      <td>https://github.com/getsentry/sentry/pull/90516</td>\n",
       "      <td>This PR removes the `performance_issues.file_io_main_thread.disabled` override option for the Fi...</td>\n",
       "      <td>Human</td>\n",
       "      <td>perf</td>\n",
       "      <td>getsentry</td>\n",
       "      <td>sentry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2419106029</td>\n",
       "      <td>87963</td>\n",
       "      <td>ref(span-buffer): Move from sets to arrays</td>\n",
       "      <td>untitaker</td>\n",
       "      <td>837573</td>\n",
       "      <td>closed</td>\n",
       "      <td>2025-03-26T11:30:27Z</td>\n",
       "      <td>2025-04-04T10:20:54Z</td>\n",
       "      <td>None</td>\n",
       "      <td>https://api.github.com/repos/getsentry/sentry</td>\n",
       "      <td>https://github.com/getsentry/sentry/pull/87963</td>\n",
       "      <td>Arrays might be faster as they might not run comparisons on payloads to\\r\\ndetermine whether the...</td>\n",
       "      <td>Human</td>\n",
       "      <td>perf</td>\n",
       "      <td>getsentry</td>\n",
       "      <td>sentry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>2425248848</td>\n",
       "      <td>18585</td>\n",
       "      <td>avoid encoding as double in `napi_create_double` if possible</td>\n",
       "      <td>dylan-conway</td>\n",
       "      <td>35280289</td>\n",
       "      <td>closed</td>\n",
       "      <td>2025-03-28T13:01:23Z</td>\n",
       "      <td>2025-03-28T22:16:32Z</td>\n",
       "      <td>2025-03-28T22:16:32Z</td>\n",
       "      <td>https://api.github.com/repos/oven-sh/bun</td>\n",
       "      <td>https://github.com/oven-sh/bun/pull/18585</td>\n",
       "      <td>### What does this PR do?\\r\\nArithmetic on numbers encoded as doubles in JSC seems to hit more s...</td>\n",
       "      <td>Human</td>\n",
       "      <td>perf</td>\n",
       "      <td>oven-sh</td>\n",
       "      <td>bun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>2613893429</td>\n",
       "      <td>20612</td>\n",
       "      <td>Optimize  `napi_get_value_string_utf8` `napi_get_value_string_latin1`  `napi_get_value_string_ut...</td>\n",
       "      <td>Jarred-Sumner</td>\n",
       "      <td>709451</td>\n",
       "      <td>closed</td>\n",
       "      <td>2025-06-24T07:24:20Z</td>\n",
       "      <td>2025-06-25T00:39:33Z</td>\n",
       "      <td>2025-06-25T00:39:33Z</td>\n",
       "      <td>https://api.github.com/repos/oven-sh/bun</td>\n",
       "      <td>https://github.com/oven-sh/bun/pull/20612</td>\n",
       "      <td>\\r\\n\\r\\n### What does this PR do?\\r\\n\\r\\nAvoid resolving string slices\\r\\n\\r\\nCheck for exceptio...</td>\n",
       "      <td>Human</td>\n",
       "      <td>perf</td>\n",
       "      <td>oven-sh</td>\n",
       "      <td>bun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>2303501996</td>\n",
       "      <td>16857</td>\n",
       "      <td>Drain microtasks again after deferred tasks run</td>\n",
       "      <td>Jarred-Sumner</td>\n",
       "      <td>709451</td>\n",
       "      <td>closed</td>\n",
       "      <td>2025-01-29T01:31:39Z</td>\n",
       "      <td>2025-03-08T04:09:53Z</td>\n",
       "      <td>None</td>\n",
       "      <td>https://api.github.com/repos/oven-sh/bun</td>\n",
       "      <td>https://github.com/oven-sh/bun/pull/16857</td>\n",
       "      <td>### What does this PR do?\\r\\n\\r\\nDeferred tasks might call callbacks to JavaScript. We need to d...</td>\n",
       "      <td>Human</td>\n",
       "      <td>perf</td>\n",
       "      <td>oven-sh</td>\n",
       "      <td>bun</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  number  \\\n",
       "5    2486573779   90516   \n",
       "12   2419106029   87963   \n",
       "115  2425248848   18585   \n",
       "140  2613893429   20612   \n",
       "142  2303501996   16857   \n",
       "\n",
       "                                                                                                   title  \\\n",
       "5                                                  ref(perf-issues): Consolidate File IO override option   \n",
       "12                                                            ref(span-buffer): Move from sets to arrays   \n",
       "115                                         avoid encoding as double in `napi_create_double` if possible   \n",
       "140  Optimize  `napi_get_value_string_utf8` `napi_get_value_string_latin1`  `napi_get_value_string_ut...   \n",
       "142                                                      Drain microtasks again after deferred tasks run   \n",
       "\n",
       "              user   user_id   state            created_at  \\\n",
       "5        leeandher  35509934  closed  2025-04-28T18:17:36Z   \n",
       "12       untitaker    837573  closed  2025-03-26T11:30:27Z   \n",
       "115   dylan-conway  35280289  closed  2025-03-28T13:01:23Z   \n",
       "140  Jarred-Sumner    709451  closed  2025-06-24T07:24:20Z   \n",
       "142  Jarred-Sumner    709451  closed  2025-01-29T01:31:39Z   \n",
       "\n",
       "                closed_at             merged_at  \\\n",
       "5    2025-04-28T19:22:01Z  2025-04-28T19:22:01Z   \n",
       "12   2025-04-04T10:20:54Z                  None   \n",
       "115  2025-03-28T22:16:32Z  2025-03-28T22:16:32Z   \n",
       "140  2025-06-25T00:39:33Z  2025-06-25T00:39:33Z   \n",
       "142  2025-03-08T04:09:53Z                  None   \n",
       "\n",
       "                                          repo_url  \\\n",
       "5    https://api.github.com/repos/getsentry/sentry   \n",
       "12   https://api.github.com/repos/getsentry/sentry   \n",
       "115       https://api.github.com/repos/oven-sh/bun   \n",
       "140       https://api.github.com/repos/oven-sh/bun   \n",
       "142       https://api.github.com/repos/oven-sh/bun   \n",
       "\n",
       "                                           html_url  \\\n",
       "5    https://github.com/getsentry/sentry/pull/90516   \n",
       "12   https://github.com/getsentry/sentry/pull/87963   \n",
       "115       https://github.com/oven-sh/bun/pull/18585   \n",
       "140       https://github.com/oven-sh/bun/pull/20612   \n",
       "142       https://github.com/oven-sh/bun/pull/16857   \n",
       "\n",
       "                                                                                                    body  \\\n",
       "5    This PR removes the `performance_issues.file_io_main_thread.disabled` override option for the Fi...   \n",
       "12   Arrays might be faster as they might not run comparisons on payloads to\\r\\ndetermine whether the...   \n",
       "115  ### What does this PR do?\\r\\nArithmetic on numbers encoded as doubles in JSC seems to hit more s...   \n",
       "140  \\r\\n\\r\\n### What does this PR do?\\r\\n\\r\\nAvoid resolving string slices\\r\\n\\r\\nCheck for exceptio...   \n",
       "142  ### What does this PR do?\\r\\n\\r\\nDeferred tasks might call callbacks to JavaScript. We need to d...   \n",
       "\n",
       "     agent  type repo_owner repo_name  \n",
       "5    Human  perf  getsentry    sentry  \n",
       "12   Human  perf  getsentry    sentry  \n",
       "115  Human  perf    oven-sh       bun  \n",
       "140  Human  perf    oven-sh       bun  \n",
       "142  Human  perf    oven-sh       bun  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perf_hummans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fda7408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "➡ Fetching commits, pipelines & comments for getsentry/sentry PR #90516 (dataset id=2486573779)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for getsentry/sentry PR #87963 (dataset id=2419106029)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for oven-sh/bun PR #18585 (dataset id=2425248848)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for oven-sh/bun PR #20612 (dataset id=2613893429)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for oven-sh/bun PR #16857 (dataset id=2303501996)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Following Github server redirection from /repos/RockChinQ/LangBot to /repositories/575321313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "➡ Fetching commits, pipelines & comments for RockChinQ/LangBot PR #1256 (dataset id=2427616889)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for TracecatHQ/tracecat PR #1213 (dataset id=2609611207)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for bruin-data/bruin PR #470 (dataset id=2356985296)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for tokens-studio/figma-plugin PR #3402 (dataset id=2564432253)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for liam-hq/liam PR #1994 (dataset id=2588963649)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for vllm-project/vllm PR #16440 (dataset id=2452623588)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for OpenHFT/Chronicle-Core PR #684 (dataset id=2260441374)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for janbjorge/pgqueuer PR #390 (dataset id=2556468457)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for seasonedcc/remix-forms PR #272 (dataset id=2269202548)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for herbie-fp/herbie PR #1231 (dataset id=2537690761)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for herbie-fp/herbie PR #1182 (dataset id=2443864788)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for roboflow/inference PR #1092 (dataset id=2408616836)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for roboflow/inference PR #1385 (dataset id=2616290996)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for roboflow/inference PR #1280 (dataset id=2519831355)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for gofiber/fiber PR #3329 (dataset id=2356811134)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for gofiber/fiber PR #3479 (dataset id=2544691147)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for gofiber/fiber PR #3532 (dataset id=2607579182)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for netket/netket PR #2040 (dataset id=2519312120)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for tinygrad/tinygrad PR #8994 (dataset id=2324987642)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for EricLBuehler/mistral.rs PR #1379 (dataset id=2545078467)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for bionic-gpt/bionic-gpt PR #776 (dataset id=2558083620)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for bruin-data/ingestr PR #264 (dataset id=2615702170)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for commaai/opendbc PR #1823 (dataset id=2352318434)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for joamag/boytacean PR #10 (dataset id=2542615571)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for OpenHFT/Chronicle-Wire PR #988 (dataset id=2316356365)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for OpenHFT/Chronicle-Wire PR #985 (dataset id=2277950711)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for OpenHFT/Chronicle-Wire PR #984 (dataset id=2260678480)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for modular/modular PR #4511 (dataset id=2504407177)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for Kanaries/graphic-walker PR #443 (dataset id=2527565003)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for mangiucugna/json_repair PR #112 (dataset id=2524180167)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for mangiucugna/json_repair PR #114 (dataset id=2524300649)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for mangiucugna/json_repair PR #115 (dataset id=2524313861)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for wandb/weave PR #4192 (dataset id=2469218203)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for onyx-dot-app/onyx PR #4127 (dataset id=2358030784)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for lollipopkit/flutter_server_box PR #752 (dataset id=2517537659)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for joshyattridge/smart-money-concepts PR #74 (dataset id=2369238232)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for joshyattridge/smart-money-concepts PR #75 (dataset id=2369253951)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for AFASSoftware/maquette PR #193 (dataset id=2354104157)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for appsmithorg/appsmith PR #41033 (dataset id=2617294066)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for appsmithorg/appsmith PR #39757 (dataset id=2398828721)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for bitcoin/bitcoin PR #31868 (dataset id=2336649960)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for beyond-all-reason/Beyond-All-Reason PR #4336 (dataset id=2336988355)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for microsoft/vscode PR #251382 (dataset id=2590261382)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for microsoft/typescript-go PR #218 (dataset id=2269709704)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for microsoft/typescript-go PR #405 (dataset id=2369320781)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for dotnet/msbuild PR #11934 (dataset id=2555753483)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for dotnet/runtime PR #114517 (dataset id=2452691617)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for dotnet/runtime PR #117071 (dataset id=2622581875)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for dotnet/runtime PR #112047 (dataset id=2309904375)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for dotnet/efcore PR #35835 (dataset id=2412640161)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for dotnet/fsharp PR #18377 (dataset id=2386158448)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for dotnet/fsharp PR #18509 (dataset id=2483117033)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for microsoft/TypeScript PR #61822 (dataset id=2573225924)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Following Github server redirection from /repos/microsoft/qsharp to /repositories/593773927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "➡ Fetching commits, pipelines & comments for microsoft/qsharp PR #2530 (dataset id=2596620305)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for stack-auth/stack-auth PR #546 (dataset id=2394225726)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for crewAIInc/crewAI PR #2397 (dataset id=2400016065)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for crewAIInc/crewAI PR #2136 (dataset id=2337334370)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for crewAIInc/crewAI PR #2137 (dataset id=2337335339)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for calcom/cal.com PR #19021 (dataset id=2308221415)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for calcom/cal.com PR #20034 (dataset id=2389511160)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for calcom/cal.com PR #20496 (dataset id=2432868443)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for calcom/cal.com PR #20080 (dataset id=2392888093)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for calcom/cal.com PR #21855 (dataset id=2597070258)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for calcom/cal.com PR #19491 (dataset id=2353668916)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for calcom/cal.com PR #20545 (dataset id=2439339242)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for calcom/cal.com PR #21923 (dataset id=2604024784)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for GlareDB/glaredb PR #3762 (dataset id=2496617006)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for GlareDB/glaredb PR #3750 (dataset id=2492416622)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for GlareDB/glaredb PR #3774 (dataset id=2497503442)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for GlareDB/glaredb PR #3756 (dataset id=2495944314)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for GlareDB/glaredb PR #3793 (dataset id=2512247973)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for antiwork/gumroad PR #471 (dataset id=2623769975)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for antiwork/gumroad PR #361 (dataset id=2604162624)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for antiwork/gumroad PR #397 (dataset id=2608906245)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for antiwork/gumroad PR #307 (dataset id=2577421996)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for antiwork/gumroad PR #289 (dataset id=2560305820)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for antiwork/gumroad PR #56 (dataset id=2441809617)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for yamadashy/repomix PR #309 (dataset id=2297969098)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for box/box-ui-elements PR #4109 (dataset id=2531991252)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for wolfSSL/wolfssl PR #8412 (dataset id=2311607019)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for jina-ai/node-DeepResearch PR #32 (dataset id=2319710038)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for medplum/medplum PR #6182 (dataset id=2398994327)\n",
      "\n",
      "➡ Fetching commits, pipelines & comments for noneback/go-taskflow PR #89 (dataset id=2503287360)\n",
      "\n",
      "Total commit rows (human_pr_commits): 277\n",
      "Total detail rows (human_pr_commit_details): 7874\n",
      "Total run rows (human_pr_workflow_runs): 213\n",
      "Total issue comment rows (human_pr_issue_comments): 209\n",
      "Total review comment rows (human_pr_review_comments): 126\n",
      "\n",
      "Applying filters to commit data...\n",
      "  Starting detail rows: 7,874\n",
      "  Removed null filenames: 0\n",
      "  Removed config/metadata files: 969\n",
      "  Removed merge commits: 63 commit(s)\n",
      "  PRs remaining after filters: 83\n",
      "\n",
      "Totals after filtering:\n",
      "  Commit rows (human_pr_commits): 210\n",
      "  Detail rows (human_pr_commit_details): 615\n",
      "  Run rows (human_pr_workflow_runs): 207\n",
      "  Issue comment rows (human_pr_issue_comments): 202\n",
      "  Review comment rows (human_pr_review_comments): 126\n",
      "  Unique PRs retained: 83\n"
     ]
    }
   ],
   "source": [
    "rows_commits = []\n",
    "rows_details = []\n",
    "rows_runs = []\n",
    "rows_issue_comments = []\n",
    "rows_review_comments = []\n",
    "skipped_count = 0\n",
    "\n",
    "for idx, row in perf_hummans.iterrows():\n",
    "    pr_id = int(row[\"id\"])\n",
    "    owner = row[\"repo_owner\"]\n",
    "    repo_name = row[\"repo_name\"]\n",
    "    number = int(row[\"number\"])\n",
    "\n",
    "    full_repo = f\"{owner}/{repo_name}\"\n",
    "    print(f\"\\n➡ Fetching commits, pipelines & comments for {full_repo} PR #{number} (dataset id={pr_id})\")\n",
    "\n",
    "    if pd.isna(owner) or pd.isna(repo_name):\n",
    "        print(\"   Skipping due to missing owner/repo\")\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        repo = gh.get_repo(full_repo)\n",
    "        pr = repo.get_pull(number)\n",
    "\n",
    "        pr_title = pr.title\n",
    "        pr_description = pr.body\n",
    "        pr_comments_count = pr.comments  \n",
    "\n",
    "        # ===================== ISSUE COMMENTS  =====================\n",
    "        try:\n",
    "            issue_comments = pr.get_issue_comments()\n",
    "            for c in issue_comments:\n",
    "                rows_issue_comments.append({\n",
    "                    \"pr_id\": pr_id,\n",
    "                    \"pr_number\": number,\n",
    "                    \"comment_id\": c.id,\n",
    "                    \"user_login\": c.user.login if c.user else None,\n",
    "                    \"user_type\": c.user.type if c.user else None,\n",
    "                    \"body\": c.body,\n",
    "                    \"created_at\": c.created_at,\n",
    "                    \"updated_at\": c.updated_at,\n",
    "                    \"url\": c.html_url,\n",
    "                })\n",
    "        except Exception as e_ic:\n",
    "            print(f\"   Error fetching issue comments for PR #{number}: {e_ic}\")\n",
    "\n",
    "        # ===================== REVIEW COMMENTS  =====================\n",
    "        try:\n",
    "            review_comments = pr.get_review_comments()\n",
    "            for rc in review_comments:\n",
    "                rows_review_comments.append({\n",
    "                    \"pr_id\": pr_id,\n",
    "                    \"pr_number\": number,\n",
    "                    \"comment_id\": rc.id,\n",
    "                    \"user_login\": rc.user.login if rc.user else None,\n",
    "                    \"user_type\": rc.user.type if rc.user else None,\n",
    "                    \"body\": rc.body,\n",
    "                    \"created_at\": rc.created_at,\n",
    "                    \"updated_at\": rc.updated_at,\n",
    "                    \"path\": rc.path,\n",
    "                    \"position\": rc.position,\n",
    "                    \"original_position\": rc.original_position,\n",
    "                    \"commit_id\": rc.commit_id,\n",
    "                    \"original_commit_id\": rc.original_commit_id,\n",
    "                    \"in_reply_to_id\": getattr(rc, \"in_reply_to_id\", None),\n",
    "                    \"diff_hunk\": rc.diff_hunk,\n",
    "                    \"url\": rc.html_url,\n",
    "                })\n",
    "        except Exception as e_rc:\n",
    "            print(f\"   Error fetching review comments for PR #{number}: {e_rc}\")\n",
    "\n",
    "        # ===================== COMMITS =====================\n",
    "        commit_list = pr.get_commits()\n",
    "        for c in commit_list:\n",
    "            sha = c.sha\n",
    "            commit_obj = c.commit\n",
    "\n",
    "            author_name = None\n",
    "            committer_name = None\n",
    "            commit_message = None\n",
    "\n",
    "            if commit_obj is not None:\n",
    "                if commit_obj.author is not None:\n",
    "                    author_name = commit_obj.author.name\n",
    "                if commit_obj.committer is not None:\n",
    "                    committer_name = commit_obj.committer.name\n",
    "                commit_message = commit_obj.message\n",
    "\n",
    "            stats = c.stats\n",
    "            commit_stats_additions = getattr(stats, \"additions\", None)\n",
    "            commit_stats_deletions = getattr(stats, \"deletions\", None)\n",
    "            commit_stats_total = getattr(stats, \"total\", None)\n",
    "\n",
    "            # ---- table commits ----\n",
    "            rows_commits.append({\n",
    "                \"sha\": sha,\n",
    "                \"pr_id\": pr_id,\n",
    "                \"pr_number\": number,\n",
    "                \"repo_owner\": owner,\n",
    "                \"repo_name\": repo_name,\n",
    "                \"author\": author_name,\n",
    "                \"committer\": committer_name,\n",
    "                \"commit_message\": commit_message,\n",
    "                \"pr_title\": pr_title,\n",
    "                \"pr_description\": pr_description,\n",
    "                \"pr_comments_count\": pr_comments_count,\n",
    "            })\n",
    "\n",
    "            # ---- table pr_commit_details ----\n",
    "            for f in c.files:\n",
    "                rows_details.append({\n",
    "                    \"sha\": sha,\n",
    "                    \"pr_id\": pr_id,\n",
    "                    \"pr_number\": number,\n",
    "                    \"commit_stats_total\": commit_stats_total,\n",
    "                    \"commit_stats_additions\": commit_stats_additions,\n",
    "                    \"commit_stats_deletions\": commit_stats_deletions,\n",
    "                    \"filename\": f.filename,\n",
    "                    \"status\": f.status,\n",
    "                    \"additions\": f.additions,\n",
    "                    \"deletions\": f.deletions,\n",
    "                    \"changes\": f.changes,\n",
    "                    \"patch\": f.patch,\n",
    "                })\n",
    "\n",
    "        # ===================== PIPELINES / WORKFLOW RUNS =====================\n",
    "        head_sha = pr.head.sha\n",
    "        head_branch = pr.head.ref\n",
    "\n",
    "        for run in repo.get_workflow_runs(branch=head_branch, event=\"pull_request\"):\n",
    "            if run.head_sha != head_sha:\n",
    "                continue\n",
    "\n",
    "            rows_runs.append({\n",
    "                \"run_id\": run.id,\n",
    "                \"pr_id\": pr_id,\n",
    "                \"pr_number\": number,\n",
    "                \"workflow_id\": run.workflow_id,\n",
    "                \"workflow_name\": getattr(run, \"name\", None),\n",
    "                \"head_branch\": run.head_branch,\n",
    "                \"head_sha\": run.head_sha,\n",
    "                \"event\": run.event,\n",
    "                \"status\": run.status,\n",
    "                \"conclusion\": run.conclusion,\n",
    "                \"created_at\": run.created_at,\n",
    "                \"updated_at\": run.updated_at,\n",
    "                \"run_attempt\": getattr(run, \"run_attempt\", None),\n",
    "                \"url\": run.html_url,\n",
    "            })\n",
    "\n",
    "        time.sleep(0.7)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   Error fetching PR #{number} from {full_repo}: {e}\")\n",
    "        skipped_count += 1\n",
    "\n",
    "# ===================== DATAFRAMES =====================\n",
    "df_commits = pd.DataFrame(rows_commits)\n",
    "df_details = pd.DataFrame(rows_details)\n",
    "df_runs = pd.DataFrame(rows_runs)\n",
    "df_issue_comments = pd.DataFrame(rows_issue_comments)\n",
    "df_review_comments = pd.DataFrame(rows_review_comments)\n",
    "\n",
    "print(\"\\nTotal commit rows (human_pr_commits):\", len(df_commits))\n",
    "print(\"Total detail rows (human_pr_commit_details):\", len(df_details))\n",
    "print(\"Total run rows (human_pr_workflow_runs):\", len(df_runs))\n",
    "print(\"Total issue comment rows (human_pr_issue_comments):\", len(df_issue_comments))\n",
    "print(\"Total review comment rows (human_pr_review_comments):\", len(df_review_comments))\n",
    "\n",
    "# ===================== FILTERS =====================\n",
    "print(\"\\nApplying filters to commit data...\")\n",
    "\n",
    "if len(df_details) == 0:\n",
    "    print(\"  No commit detail rows; skipping filters.\")\n",
    "    filtered_pr_ids = set(df_commits.get('pr_id', pd.Series(dtype=int)).unique())\n",
    "else:\n",
    "    print(f\"  Starting detail rows: {len(df_details):,}\")\n",
    "\n",
    "\n",
    "    before_filename = len(df_details)\n",
    "    df_details = df_details[df_details['filename'].notna()].copy()\n",
    "    print(f\"  Filtered out null filenames: {before_filename - len(df_details):,} records removed\")\n",
    "    print(f\"  Remaining after filename filter: {len(df_details):,} detail rows\")\n",
    "\n",
    "    config_patterns = [\n",
    "        r'^\\.mvn/',\n",
    "        r'^\\.gradle/',\n",
    "        r'^\\.idea/',\n",
    "        r'^\\.vscode/',\n",
    "        r'^\\.github/workflows/',\n",
    "        r'\\.properties$',\n",
    "        r'\\.xml$',\n",
    "        r'\\.yml$',\n",
    "        r'\\.yaml$',\n",
    "        r'\\.json$',\n",
    "        r'\\.md$',          \n",
    "        r'\\.txt$',\n",
    "        r'\\.gitignore$',\n",
    "        r'\\.dockerignore$',\n",
    "        r'/Dockerfile$',\n",
    "        r'^Dockerfile$',\n",
    "        r'/docker-compose',\n",
    "        r'^docker-compose',\n",
    "        r'\\.lock$',\n",
    "        r'^LICENSE',\n",
    "        r'^README',\n",
    "    ]\n",
    "    config_pattern = '|'.join(config_patterns)\n",
    "\n",
    "    before_config_filter = len(df_details)\n",
    "\n",
    "    df_details['is_config_file'] = df_details['filename'].str.contains(\n",
    "        config_pattern, case=False, na=False, regex=True\n",
    "    )\n",
    "    df_details['is_code_file'] = ~df_details['is_config_file']\n",
    "\n",
    "    pr_has_code = df_details.groupby('pr_id')['is_code_file'].any().reset_index()\n",
    "    pr_has_code.columns = ['pr_id', 'has_code_files']\n",
    "\n",
    "    df_details = df_details.merge(pr_has_code, on='pr_id', how='left')\n",
    "    df_details = df_details[df_details['has_code_files']].copy()\n",
    "    df_details = df_details.drop(columns=['is_config_file', 'is_code_file', 'has_code_files'])\n",
    "\n",
    "    print(f\"  Filtered out config-only PR/file rows: {before_config_filter - len(df_details):,} records removed\")\n",
    "    print(f\"  Remaining after config filter: {len(df_details):,} detail rows\")\n",
    "\n",
    "    if 'commit_message' in df_commits.columns:\n",
    "        before_merge_filter = len(df_details)\n",
    "\n",
    "        merge_patterns = [\n",
    "            r'^Merge\\s+branch',\n",
    "            r'^Merge\\s+pull\\s+request',\n",
    "            r'^Merge\\s+remote-tracking\\s+branch',\n",
    "            r'^Merge\\s+.*\\s+into\\s+',\n",
    "            r\"^Merged\\s+in\\s+\",\n",
    "        ]\n",
    "        merge_pattern = '|'.join(merge_patterns)\n",
    "\n",
    "        merge_shas = set(\n",
    "            df_commits[\n",
    "                df_commits['commit_message'].str.match(merge_pattern, case=False, na=False)\n",
    "            ]['sha'].tolist()\n",
    "        )\n",
    "\n",
    "        if merge_shas:\n",
    "            df_commits = df_commits[~df_commits['sha'].isin(merge_shas)].copy()\n",
    "            df_details = df_details[~df_details['sha'].isin(merge_shas)].copy()\n",
    "\n",
    "        after_merge_filter = len(df_details)\n",
    "        print(f\"  Filtered out merge commits: {before_merge_filter - after_merge_filter:,} detail rows removed from {len(merge_shas)} merge commit(s)\")\n",
    "        print(f\"  Remaining after merge filter: {after_merge_filter:,} detail rows\")\n",
    "    else:\n",
    "        print(\"  ⚠ 'commit_message' column missing in df_commits; skipping merge filter.\")\n",
    "\n",
    "    # 4) PRs que quedan después de TODOS los filtros de detalles\n",
    "    filtered_pr_ids = set(df_details['pr_id'].unique())\n",
    "    print(f\"  PRs remaining after all detail-level filters: {len(filtered_pr_ids):,}\")\n",
    "\n",
    "# ===================== SINCRONIZAR df_commits / runs / comments =====================  # NEW\n",
    "if filtered_pr_ids:\n",
    "    df_commits = df_commits[df_commits['pr_id'].isin(filtered_pr_ids)].copy()\n",
    "    df_runs = df_runs[df_runs['pr_id'].isin(filtered_pr_ids)].copy()\n",
    "    df_issue_comments = df_issue_comments[df_issue_comments['pr_id'].isin(filtered_pr_ids)].copy()\n",
    "    df_review_comments = df_review_comments[df_review_comments['pr_id'].isin(filtered_pr_ids)].copy()\n",
    "\n",
    "    print(\"\\nAfter syncing to filtered PRs:\")\n",
    "    print(\"  Commit rows:\", len(df_commits))\n",
    "    print(\"  Detail rows:\", len(df_details))\n",
    "    print(\"  Run rows:\", len(df_runs))\n",
    "    print(\"  Issue comment rows:\", len(df_issue_comments))\n",
    "    print(\"  Review comment rows:\", len(df_review_comments))\n",
    "else:\n",
    "    print(\"\\n⚠ No PRs left after filters; dataframes will be empty for RQ analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c757bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking for PRs with markdown-only changes...\n",
      "  PRs with only markdown files: 0\n",
      "  Unique PRs retained after markdown-only filter: 83\n",
      "\n",
      "Totals after markdown-only filter:\n",
      "  Commit rows (ai_pr_commits): 210\n",
      "  Detail rows (ai_pr_commit_details): 615\n",
      "  Run rows (ai_pr_workflow_runs): 207\n",
      "  Issue comment rows (ai_pr_issue_comments): 202\n",
      "  Review comment rows (ai_pr_review_comments): 126\n"
     ]
    }
   ],
   "source": [
    "# Remove PRs where commits only touch markdown files (no code changes)\n",
    "print(\"\\nChecking for PRs with markdown-only changes...\")\n",
    "\n",
    "if len(df_details) == 0:\n",
    "    print(\"  No commit detail rows; skipping markdown-only filter.\")\n",
    "    print(f\"  Unique PRs retained (unchanged): {len(filtered_pr_ids):,}\")\n",
    "else:\n",
    "    df_details[\"__is_markdown\"] = df_details[\"filename\"].str.lower().str.endswith((\".md\", \".markdown\"))\n",
    "\n",
    "    md_only_pr_ids = set(\n",
    "        df_details.groupby(\"pr_id\")[\"__is_markdown\"]\n",
    "        .agg(lambda s: bool(len(s)) and s.all())\n",
    "        .pipe(lambda s: s[s].index)\n",
    "    )\n",
    "\n",
    "    print(f\"  PRs with only markdown files: {len(md_only_pr_ids):,}\")\n",
    "    if md_only_pr_ids:\n",
    "        df_details = df_details[~df_details[\"pr_id\"].isin(md_only_pr_ids)].copy()\n",
    "        df_commits = df_commits[~df_commits[\"pr_id\"].isin(md_only_pr_ids)].copy()\n",
    "        df_runs = df_runs[~df_runs[\"pr_id\"].isin(md_only_pr_ids)].copy()\n",
    "        df_issue_comments = df_issue_comments[~df_issue_comments[\"pr_id\"].isin(md_only_pr_ids)].copy()\n",
    "        df_review_comments = df_review_comments[~df_review_comments[\"pr_id\"].isin(md_only_pr_ids)].copy()\n",
    "\n",
    "    filtered_pr_ids = set(df_details[\"pr_id\"].unique())\n",
    "    df_details = df_details.drop(columns=[\"__is_markdown\"])\n",
    "\n",
    "    print(f\"  Unique PRs retained after markdown-only filter: {len(filtered_pr_ids):,}\")\n",
    "    print(\"\\nTotals after markdown-only filter:\")\n",
    "    print(\"  Commit rows (ai_pr_commits):\", len(df_commits))\n",
    "    print(\"  Detail rows (ai_pr_commit_details):\", len(df_details))\n",
    "    print(\"  Run rows (ai_pr_workflow_runs):\", len(df_runs))\n",
    "    print(\"  Issue comment rows (ai_pr_issue_comments):\", len(df_issue_comments))\n",
    "    print(\"  Review comment rows (ai_pr_review_comments):\", len(df_review_comments))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a4d0244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved: human_pr_commits.parquet\n",
      "Saved: human_pr_commit_details.parquet\n",
      "Saved: human_pr_workflow_runs.parquet\n",
      "Saved: human_pr_issue_comments.parquet\n",
      "Saved: human_pr_review_comments.parquet\n",
      "\n",
      "Skipped 0 PRs due to errors or missing data.\n"
     ]
    }
   ],
   "source": [
    "# ===================== SAVE TO PARQUET =====================\n",
    "df_commits.to_parquet(\"./human_pr_commits.parquet\", index=False)\n",
    "df_details.to_parquet(\"./human_pr_commit_details.parquet\", index=False)\n",
    "df_runs.to_parquet(\"./human_pr_workflow_runs.parquet\", index=False)\n",
    "df_issue_comments.to_parquet(\"./human_pr_issue_comments.parquet\", index=False)\n",
    "df_review_comments.to_parquet(\"./human_pr_review_comments.parquet\", index=False)\n",
    "\n",
    "print(\"\\nSaved: human_pr_commits.parquet\")\n",
    "print(\"Saved: human_pr_commit_details.parquet\")\n",
    "print(\"Saved: human_pr_workflow_runs.parquet\")\n",
    "print(\"Saved: human_pr_issue_comments.parquet\")\n",
    "print(\"Saved: human_pr_review_comments.parquet\")\n",
    "print(f\"\\nSkipped {skipped_count} PRs due to errors or missing data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c01f4d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>pr_id</th>\n",
       "      <th>pr_number</th>\n",
       "      <th>workflow_id</th>\n",
       "      <th>workflow_name</th>\n",
       "      <th>head_branch</th>\n",
       "      <th>head_sha</th>\n",
       "      <th>event</th>\n",
       "      <th>status</th>\n",
       "      <th>conclusion</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>run_attempt</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14714884717</td>\n",
       "      <td>2486573779</td>\n",
       "      <td>90516</td>\n",
       "      <td>3020796</td>\n",
       "      <td>sentry pull request bot</td>\n",
       "      <td>leander/rollout-io</td>\n",
       "      <td>91d120e06968062bca53859a962c3b41456606fa</td>\n",
       "      <td>pull_request</td>\n",
       "      <td>completed</td>\n",
       "      <td>skipped</td>\n",
       "      <td>2025-04-28 18:17:41+00:00</td>\n",
       "      <td>2025-04-28 18:17:46+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>https://github.com/getsentry/sentry/actions/runs/14714884717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14714884704</td>\n",
       "      <td>2486573779</td>\n",
       "      <td>90516</td>\n",
       "      <td>24420232</td>\n",
       "      <td>CodeQL</td>\n",
       "      <td>leander/rollout-io</td>\n",
       "      <td>91d120e06968062bca53859a962c3b41456606fa</td>\n",
       "      <td>pull_request</td>\n",
       "      <td>completed</td>\n",
       "      <td>success</td>\n",
       "      <td>2025-04-28 18:17:41+00:00</td>\n",
       "      <td>2025-04-28 18:28:12+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>https://github.com/getsentry/sentry/actions/runs/14714884704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14714884711</td>\n",
       "      <td>2486573779</td>\n",
       "      <td>90516</td>\n",
       "      <td>33200914</td>\n",
       "      <td>self-hosted</td>\n",
       "      <td>leander/rollout-io</td>\n",
       "      <td>91d120e06968062bca53859a962c3b41456606fa</td>\n",
       "      <td>pull_request</td>\n",
       "      <td>completed</td>\n",
       "      <td>success</td>\n",
       "      <td>2025-04-28 18:17:41+00:00</td>\n",
       "      <td>2025-04-28 18:23:16+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>https://github.com/getsentry/sentry/actions/runs/14714884711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14714884686</td>\n",
       "      <td>2486573779</td>\n",
       "      <td>90516</td>\n",
       "      <td>1585211</td>\n",
       "      <td>migrations</td>\n",
       "      <td>leander/rollout-io</td>\n",
       "      <td>91d120e06968062bca53859a962c3b41456606fa</td>\n",
       "      <td>pull_request</td>\n",
       "      <td>completed</td>\n",
       "      <td>success</td>\n",
       "      <td>2025-04-28 18:17:41+00:00</td>\n",
       "      <td>2025-04-28 18:18:06+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>https://github.com/getsentry/sentry/actions/runs/14714884686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14714884710</td>\n",
       "      <td>2486573779</td>\n",
       "      <td>90516</td>\n",
       "      <td>23572706</td>\n",
       "      <td>Enforce License Compliance</td>\n",
       "      <td>leander/rollout-io</td>\n",
       "      <td>91d120e06968062bca53859a962c3b41456606fa</td>\n",
       "      <td>pull_request</td>\n",
       "      <td>completed</td>\n",
       "      <td>success</td>\n",
       "      <td>2025-04-28 18:17:41+00:00</td>\n",
       "      <td>2025-04-28 18:19:01+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>https://github.com/getsentry/sentry/actions/runs/14714884710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        run_id       pr_id  pr_number  workflow_id  \\\n",
       "0  14714884717  2486573779      90516      3020796   \n",
       "1  14714884704  2486573779      90516     24420232   \n",
       "2  14714884711  2486573779      90516     33200914   \n",
       "3  14714884686  2486573779      90516      1585211   \n",
       "4  14714884710  2486573779      90516     23572706   \n",
       "\n",
       "                workflow_name         head_branch  \\\n",
       "0     sentry pull request bot  leander/rollout-io   \n",
       "1                      CodeQL  leander/rollout-io   \n",
       "2                 self-hosted  leander/rollout-io   \n",
       "3                  migrations  leander/rollout-io   \n",
       "4  Enforce License Compliance  leander/rollout-io   \n",
       "\n",
       "                                   head_sha         event     status  \\\n",
       "0  91d120e06968062bca53859a962c3b41456606fa  pull_request  completed   \n",
       "1  91d120e06968062bca53859a962c3b41456606fa  pull_request  completed   \n",
       "2  91d120e06968062bca53859a962c3b41456606fa  pull_request  completed   \n",
       "3  91d120e06968062bca53859a962c3b41456606fa  pull_request  completed   \n",
       "4  91d120e06968062bca53859a962c3b41456606fa  pull_request  completed   \n",
       "\n",
       "  conclusion                created_at                updated_at  run_attempt  \\\n",
       "0    skipped 2025-04-28 18:17:41+00:00 2025-04-28 18:17:46+00:00            1   \n",
       "1    success 2025-04-28 18:17:41+00:00 2025-04-28 18:28:12+00:00            1   \n",
       "2    success 2025-04-28 18:17:41+00:00 2025-04-28 18:23:16+00:00            1   \n",
       "3    success 2025-04-28 18:17:41+00:00 2025-04-28 18:18:06+00:00            1   \n",
       "4    success 2025-04-28 18:17:41+00:00 2025-04-28 18:19:01+00:00            1   \n",
       "\n",
       "                                                            url  \n",
       "0  https://github.com/getsentry/sentry/actions/runs/14714884717  \n",
       "1  https://github.com/getsentry/sentry/actions/runs/14714884704  \n",
       "2  https://github.com/getsentry/sentry/actions/runs/14714884711  \n",
       "3  https://github.com/getsentry/sentry/actions/runs/14714884686  \n",
       "4  https://github.com/getsentry/sentry/actions/runs/14714884710  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pr commit details for human PRs\n",
    "human_pr_commit_details = pd.read_parquet(\"./human_pr_workflow_runs.parquet\")\n",
    "human_pr_commit_details.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
