id,number,title,body,agent,user_id,user,state,created_at,closed_at,merged_at,repo_id,repo_url,html_url,type,reason,classification_reason,author_type,additions,deletions,patch,num_commits,patch_length,primary_language,is_merged,is_closed,is_open,time_to_close_hours,time_to_merge_hours,time_to_close_days,time_to_merge_days,title_length,title_word_count,body_length,body_word_count,has_body,gpt_explanation,gpt_comparison,optimization_pattern,optimization_subpattern,pattern_confidence,gpt_success,gpt_error,gpt_tokens
2486573779,90516,ref(perf-issues): Consolidate File IO override option,"This PR removes the `performance_issues.file_io_main_thread.disabled` override option for the FileIOMainThread detector. There are already system options that were being checked after detection to stop issue creation, but instead, we will defer to the Issue Platform on whether or not an issue should be created.

I've documented this change on the base class's methods and changed `creation` to `detection` since it establishes the detector will not be run if the option is set to false.",Human,35509934,leeandher,closed,2025-04-28 18:17:36+00:00,2025-04-28 19:22:01+00:00,2025-04-28 19:22:01+00:00,,https://api.github.com/repos/getsentry/sentry,https://github.com/getsentry/sentry/pull/90516,perf,"The PR improves the performance issue detection by removing redundant override options and deferring issue creation decisions to the Issue Platform, which optimizes the detection process without adding new features or fixing bugs.","The PR improves the performance issue detection by removing redundant override options and deferring issue creation decisions to the Issue Platform, which optimizes the detection process without adding new features or fixing bugs.",Human,,,,,,Python,True,True,False,1.073611111111111,1.073611111111111,0.0447337962962962,0.0447337962962962,53,6,490.0,74.0,True,"The code manages performance issue detection related to file I/O on the main thread. It previously used a specific override option ('performance_issues.file_io_main_thread.disabled') to control whether the detector runs. This change removes that specific override and instead relies on existing system-wide options and the Issue Platform to determine if an issue should be created, streamlining configuration and centralizing control.","The original version used a dedicated override, introducing extra condition checking and configuration specific to file I/O on the main thread. The optimized version removes this redundant layer, consolidates logic to use existing platform/system-wide controls, and renames methods for clarity. This reduces unnecessary code paths, removes redundant configuration, and documents the intended control flow more clearly at the base class level.",Code Smells and Structural Simplification,Remove Redundant Method Calls,high,True,,1177
2419106029,87963,ref(span-buffer): Move from sets to arrays,"Arrays might be faster as they might not run comparisons on payloads to
determine whether they are unique. However, they might not be as each
item has to be copied individually in Lua.
",Human,837573,untitaker,closed,2025-03-26 11:30:27+00:00,2025-04-04 10:20:54+00:00,,,https://api.github.com/repos/getsentry/sentry,https://github.com/getsentry/sentry/pull/87963,perf,"The change involves modifying the data structure used in the code (from sets to arrays) to potentially improve performance, without adding new features or fixing bugs.","The change involves modifying the data structure used in the code (from sets to arrays) to potentially improve performance, without adding new features or fixing bugs.",Human,,,,,,Python,False,True,False,214.84083333333336,,8.951701388888889,,42,6,188.0,33.0,True,"The code manages a data buffer for span (range) objects, previously storing them in sets (which enforce uniqueness through comparisons) and now switching to arrays (lists), which do not enforce uniqueness but may be faster due to simpler data structure operations. The goal is to optimize performance when handling many span payloads.","Previously, the code used sets to store span buffers, likely relying on key-indexed tables to enforce uniqueness, which requires checking payloads for equality (incurring comparison costs for insertions and lookups). The optimized code replaces sets with arrays, eliminating uniqueness checks and comparison overhead, relying instead on raw sequential storage and copying. This can improve insertion and access speeds, especially if the uniqueness guarantee is not needed. The change trades off the benefit of uniqueness enforcement for potentially better speed. There is no apparent change in the algorithmic logic, just in the data structure used for storage.",Data Structure Selection and Adaptation,Darwinian Selection,high,True,,1145
2425248848,18585,avoid encoding as double in `napi_create_double` if possible,"### What does this PR do?
Arithmetic on numbers encoded as doubles in JSC seems to hit more slow paths compared to `NumberTag` numbers.

Fixes #9218

We might want to do this in other places. With this change in a debug build, fps goes from ~1 to ~100 on M4 max

<img width=""339"" alt=""Screenshot 2025-03-28 at 5 54 06â€¯AM"" src=""https://github.com/user-attachments/assets/2f4817a4-af4c-4d9e-a293-d98d478871be"" />
<!-- **Please explain what your changes do**, example: -->

<!--

This adds a new flag --bail to bun test. When set, it will stop running tests after the first failure. This is useful for CI environments where you want to fail fast.

-->

### How did you verify your code works?

<!-- **For code changes, please include automated tests**. Feel free to uncomment the line below -->

<!-- I wrote automated tests -->

<!-- If JavaScript/TypeScript modules or builtins changed:

- [ ] I included a test for the new code, or existing tests cover it
- [ ] I ran my tests locally and they pass (`bun-debug test test-file-name.test`)

-->

<!-- If Zig files changed:

- [ ] I checked the lifetime of memory allocated to verify it's (1) freed and (2) only freed when it should be
- [ ] I included a test for the new code, or an existing test covers it
- [ ] JSValue used outside of the stack is either wrapped in a JSC.Strong or is JSValueProtect'ed
- [ ] I wrote TypeScript/JavaScript tests and they pass locally (`bun-debug test test-file-name.test`)
-->

<!-- If new methods, getters, or setters were added to a publicly exposed class:

- [ ] I added TypeScript types for the new methods, getters, or setters
-->

<!-- If dependencies in tests changed:

- [ ] I made sure that specific versions of dependencies are used instead of ranged or tagged versions
-->

<!-- If a new builtin ESM/CJS module was added:

- [ ] I updated Aliases in `module_loader.zig` to include the new module
- [ ] I added a test that imports the module
- [ ] I added a test that require() the module
-->
",Human,35280289,dylan-conway,closed,2025-03-28 13:01:23+00:00,2025-03-28 22:16:32+00:00,2025-03-28 22:16:32+00:00,,https://api.github.com/repos/oven-sh/bun,https://github.com/oven-sh/bun/pull/18585,perf,"The PR improves performance by avoiding unnecessary encoding as double, resulting in a significant FPS increase. This is a performance optimization rather than a bug fix or new feature.","The PR improves performance by avoiding unnecessary encoding as double, resulting in a significant FPS increase. This is a performance optimization rather than a bug fix or new feature.",Human,,,,,,Zig,True,True,False,9.2525,9.2525,0.3855208333333333,0.3855208333333333,60,8,2039.0,351.0,True,"The code in question optimizes the process of creating number representations in the N-API for JavaScriptCore (JSC), particularly in the 'napi_create_double' function. Originally, all numbers were encoded as doubles regardless of whether a more efficient internal representation ('NumberTag') was possible. The updated code adds a check to use the more efficient 'NumberTag' encoding for numbers that can be represented this way, only falling back to double encoding when necessary. This improves arithmetic performance by enabling faster paths in JSC for commonly used integer and small number values.","The original code always encoded numeric values as doubles, which in JSC leads to hitting more slow paths during arithmetic operations. The optimized code first checks if the numeric value can be represented using the more efficient 'NumberTag' encoding, and chooses that over the double encoding when possible. This change reduces the number of slow-path executions in the JSC engine, as arithmetic on 'NumberTag' values is handled much more efficiently than on doubles. This leads to a dramatic frames-per-second improvement (from ~1 FPS to ~100 FPS as cited) due to reduced overhead and better use of JSC's fast paths. The redundant/unnecessary casting of values to double is also avoided, resulting in both faster and more memory-efficient execution paths for many typical numeric values.",Memory and Data Locality Optimizations,Access Data with Appropriate Type,high,True,,1684
2613893429,20612,Optimize  `napi_get_value_string_utf8` `napi_get_value_string_latin1`  `napi_get_value_string_utf16`,"

### What does this PR do?

Avoid resolving string slices

Check for exceptions when resolving rope strings

### How did you verify your code works?

It compiled",Human,709451,Jarred-Sumner,closed,2025-06-24 07:24:20+00:00,2025-06-25 00:39:33+00:00,2025-06-25 00:39:33+00:00,,https://api.github.com/repos/oven-sh/bun,https://github.com/oven-sh/bun/pull/20612,perf,"The PR description indicates improvements in the implementation to avoid unnecessary operations and handle exceptions better, which are performance-related enhancements rather than bug fixes or new features.","The PR description indicates improvements in the implementation to avoid unnecessary operations and handle exceptions better, which are performance-related enhancements rather than bug fixes or new features.",Human,,,,,,Zig,True,True,False,17.253611111111113,17.253611111111113,0.7189004629629631,0.7189004629629631,100,4,172.0,27.0,True,"The code involves optimization of three N-API string retrieval functions: 'napi_get_value_string_utf8', 'napi_get_value_string_latin1', and 'napi_get_value_string_utf16'. These functions are responsible for extracting string values from JavaScript values in various encodings through the Native Abstractions for Node.js (N-API) interface. The optimizations target internal handling when strings use rope data structures (i.e., concatenated or composite strings). Specifically, it avoids prematurely resolving or flattening 'rope' strings (which can be composed of smaller string slices) unless absolutely necessary, and ensures robust error handling if rope resolution throws exceptions.","The original implementation likely resolved 'rope' strings into contiguous string slices even when not strictly necessary, which could cause unnecessary heap allocations, CPU work, or cache thrashing if the full string was not needed immediately. The optimized version checks if resolution is required before flattening the string, thereby avoiding extra computation and memory usage in the common case. Additionally, it adds checks for exceptions during the rope resolution process, providing more robust and fail-safe behavior. This reduces redundant or costly logic that only sometimes needs to be executed.",Control-Flow and Branching Optimizations,Remove Branches by Doing Extra Work,high,True,,1223
2303501996,16857,Drain microtasks again after deferred tasks run,"### What does this PR do?

Deferred tasks might call callbacks to JavaScript. We need to drain microtasks when calling into JavaScript. Therefore, we should drain microtasks after draining deferred tasks. 

Whether or not we should recursively do this is another question.

### How did you verify your code works?

Let's see what CI says about this.",Human,709451,Jarred-Sumner,closed,2025-01-29 01:31:39+00:00,2025-03-08 04:09:53+00:00,,,https://api.github.com/repos/oven-sh/bun,https://github.com/oven-sh/bun/pull/16857,perf,"The change improves the runtime behavior by ensuring microtasks are drained after deferred tasks, which can be seen as a performance or correctness improvement in task scheduling.","The change improves the runtime behavior by ensuring microtasks are drained after deferred tasks, which can be seen as a performance or correctness improvement in task scheduling.",Human,,,,,,Zig,False,True,False,914.6372222222222,,38.10988425925926,,47,7,357.0,57.0,True,"The code is responsible for managing the correct sequencing and execution of microtasks and deferred tasks, particularly when interacting with JavaScript callbacks. By ensuring that microtasks are drained again after deferred tasks have run (because deferred tasks might schedule new microtasks via JavaScript callbacks), the code guarantees proper microtask scheduling and execution order as per the JavaScript event loop semantics.","The original version only drained microtasks before running deferred tasks. The optimized version adds an additional draining of microtasks after deferred tasks are run. This change does not alter the core algorithm but ensures that all microtasks scheduled during deferred task processing are executed immediately, improving event loop correctness and potentially reducing latency and avoiding redundant wakeups or event loop ticks. There is no redundant code removed, but the sequence of execution is structurally modified for more predictable runtime behavior, which may avoid edge-case bugs and inefficiency in microtask scheduling.",Control-Flow and Branching Optimizations,Make Conditional Branches More Predictable,medium,True,,1178
2427616889,1256,perf(chatcmpl): remove `space` from `base-url`,"## æ¦‚è¿°

- fix: #1248 

## æ£€æŸ¥æ¸…å•

### PR ä½œè€…å®Œæˆ

*è¯·åœ¨æ–¹æ‹¬å·é—´å†™`x`ä»¥æ‰“å‹¾

- [ ] é˜…è¯»ä»“åº“[è´¡çŒ®æŒ‡å¼•](https://github.com/RockChinQ/LangBot/blob/master/CONTRIBUTING.md)äº†å—ï¼Ÿ
- [ ] ä¸é¡¹ç›®æ‰€æœ‰è€…æ²Ÿé€šè¿‡äº†å—ï¼Ÿ
- [ ] æˆ‘ç¡®å®šå·²è‡ªè¡Œæµ‹è¯•æ‰€ä½œçš„æ›´æ”¹ï¼Œç¡®ä¿åŠŸèƒ½ç¬¦åˆé¢„æœŸã€‚

### é¡¹ç›®æ‰€æœ‰è€…å®Œæˆ

- [ ] ç›¸å…³ issues é“¾æ¥äº†å—ï¼Ÿ
- [ ] é…ç½®é¡¹å†™å¥½äº†å—ï¼Ÿè¿ç§»å†™å¥½äº†å—ï¼Ÿç”Ÿæ•ˆäº†å—ï¼Ÿ
- [ ] ä¾èµ–å†™åˆ° requirements.txt å’Œ core/bootutils/deps.py äº†å—
- [ ] æ–‡æ¡£ç¼–å†™äº†å—ï¼Ÿ",Human,45992437,RockChinQ,closed,2025-03-30 15:59:41+00:00,2025-03-30 15:59:55+00:00,2025-03-30 15:59:55+00:00,,https://api.github.com/repos/RockChinQ/LangBot,https://github.com/RockChinQ/LangBot/pull/1256,perf,"The title indicates a performance improvement by removing unnecessary spaces from a URL, which aligns with the 'perf' type for performance enhancements.","The title indicates a performance improvement by removing unnecessary spaces from a URL, which aligns with the 'perf' type for performance enhancements.",Human,,,,,,,True,True,False,0.0038888888888888,0.0038888888888888,0.000162037037037,0.000162037037037,46,5,346.0,47.0,True,"The code is responsible for constructing a 'base-url' string used in API calls or similar network operations. Previously, there was a possibility that the generated base URL had leading, trailing, or extra spaces, potentially causing incorrect HTTP requests. The commit removes space characters from the base URL to ensure that it is well-formed and does not contain any whitespace, fixing potential bugs and inconsistencies related to URL handling.","The original code may have either allowed whitespace in the base-url string or did not actively trim/remove extra spaces. The optimized code explicitly removes or trims out any spaces from the base-url (either via a .replace(), .strip(), or similar method). This not only corrects potential bugs but also avoids failed or misdirected network requests, reducing unnecessary error-handling or retries. Technically, this is a code simplification and minor correctness/performance improvement by avoiding invalid URL errors that may have resulted in wasted cycles.",Code Smells and Structural Simplification,Remove Redundant Method Calls,high,True,,1279
2609611207,1213,perf(engine): Disable worker eager execution to try distribute load,"
    
<!-- This is an auto-generated description by cubic. -->

## Summary by cubic
Disabled eager activity execution for Temporal workers to help distribute load more evenly.

- **Config**
  - Added TEMPORAL__DISABLE_EAGER_ACTIVITY_EXECUTION environment variable to control this behavior.

<!-- End of auto-generated description by cubic. -->

",Human,5508348,daryllimyt,closed,2025-06-22 09:46:03+00:00,2025-06-22 09:47:52+00:00,2025-06-22 09:47:52+00:00,,https://api.github.com/repos/TracecatHQ/tracecat,https://github.com/TracecatHQ/tracecat/pull/1213,perf,"The change disables eager execution to improve load distribution, which is a performance optimization.","The change disables eager execution to improve load distribution, which is a performance optimization.",Human,,,,,,Python,True,True,False,0.0302777777777777,0.0302777777777777,0.001261574074074,0.001261574074074,67,9,357.0,45.0,True,"The code introduces an environment variable (TEMPORAL__DISABLE_EAGER_ACTIVITY_EXECUTION) to allow disabling 'eager activity execution' in Temporal workers. Eager execution is a mode where a worker immediately executes activities instead of distributing them, potentially leading to uneven load. By disabling this, activities are more likely to be distributed among available workers, potentially achieving better load balancing and overall performance.","Originally, eager execution was always enabled, causing the worker to execute activities immediately as they are scheduled, which could lead to some workers being overburdened while others are idle. The optimization adds configurability to disable this feature, allowing for more even distribution of tasks among workers. This could reduce bottlenecks, improve throughput, and make better use of computing resources. No significant redundant code removal is reported, but there is an improvement in system-level algorithm/configuration for resource utilization and scalability.",Algorithm-Level Optimizations,Select Computationally Efficient Algorithms,high,True,,1157
2356985296,470,Patch/reduce response size for internal parse commands,"This PR removes unused elements from two internal commands, which were getting problematic in larger pipelines.

<img width=""406"" alt=""image"" src=""https://github.com/user-attachments/assets/54fd43c8-ede6-400a-8fa8-ce63544ebd24"" />
",Human,16530606,karakanb,closed,2025-02-25 15:05:26+00:00,2025-02-25 15:17:19+00:00,2025-02-25 15:17:19+00:00,,https://api.github.com/repos/bruin-data/bruin,https://github.com/bruin-data/bruin/pull/470,perf,"The PR reduces the response size by removing unused elements, which improves efficiency but does not add a new feature or fix a bug. This is a performance improvement.","The PR reduces the response size by removing unused elements, which improves efficiency but does not add a new feature or fix a bug. This is a performance improvement.",Human,,,,,,Go,True,True,False,0.1980555555555555,0.1980555555555555,0.0082523148148148,0.0082523148148148,54,7,234.0,21.0,True,"The code processes internal parse commands, likely within a software pipeline, and has been updated to reduce the response size by removing unused elements from the command responses. This change addresses performance issues that became apparent in larger pipelines, likely caused by unnecessarily large data payloads.","The original version included additional elements in the responses of certain internal commands, increasing the data transmitted or processed by downstream consumers. In the optimized version, these unused response fields have been eliminated. This change likely reduces both memory usage and network transfer overhead, leading to increased performance especially in cases with larger or more numerous pipelines. No change to core algorithmic logic or control flow is made; the modification is a tightening of response data structure, improving efficiency by not transmitting or handling data that isn't needed.",Code Smells and Structural Simplification,Remove Redundant Method Calls,high,True,,1139
2564432253,3402,Github Sync Optimization,"<!--
  Notes for authors:
  - Provide context with minimal words, keep it concise
  - Mark as a draft for work in progress PRs
  - Once ready for review, notify others in #code-reviews
  - Remember, the review process is a learning opportunity for both reviewers and authors, it's a way for us to share knowledge and avoid silos.
-->

### Why does this PR exist?

Resolves #3392 

<!--
  Describe the problem you're addressing and the rationale behind this PR.
-->

### What does this pull request do?

Currently, when syncing to GitHub(in multi file sync), we push all JSON files regardless of whether they've changed or not. This results in unnecessary writes, longer sync times, and potentially bloated commit histories.

This PR addresses it by creating a filtered Changeset when pushing to github, scanning for files only with a change, or potentially being deleted, and pushes only those changes in the commit API request. 
<!--
  Detailed summary of the changes, including any visual or interactive updates.
  For UI changes, add before/after screenshots. For interactive elements, consider including a video or an animated gif.
  Explain some of the choices you've made in the PR, if they're not obvious.
-->

### Testing this change

There is no direct way for a user to test this, but what can be done is that they can push a file with a lot of token sets, then make a small change and see how much time is it taking for them to push even a small change.

<!--
  Describe how this change can be tested. Are there steps required to get there? Explain what's required so a reviewer can test these changes locally.

  If you have a review link available, add it here.
-->

### Additional Notes (if any)

<!--
  Add any other context or screenshots about the pull request
-->
",Human,9948167,akshay-gupta7,open,2025-06-03 15:36:53+00:00,,,,https://api.github.com/repos/tokens-studio/figma-plugin,https://github.com/tokens-studio/figma-plugin/pull/3402,perf,"The PR introduces an optimization feature that reduces unnecessary file pushes during GitHub sync, improving efficiency and performance.","The PR introduces an optimization feature that reduces unnecessary file pushes during GitHub sync, improving efficiency and performance.",Human,,,,,,TypeScript,False,False,True,,,,,24,3,1824.0,305.0,True,"The code is for a multi-file sync system that integrates with GitHub. Its function is to push local JSON files to a GitHub repository as part of a synchronization process. Originally, the sync operation would push all JSON files to GitHub regardless of whether they had changed, potentially causing unnecessary network operations and verbose commit histories. The new code introduces logic to detect which files were changed or deleted and ensures that only those files are included in the GitHub commit API request.","Originally, the implementation always pushed all files to GitHub during a sync, which resulted in unnecessary data transfers, increased sync times, and larger-than-needed commit histories. The optimized version scans the set of files to be synced and adds to the list only those files which have changed or are being deleted. This change removes redundant uploads and optimizes data transfer by reducing the payload size and the number of Git operations. Algorithmically, this introduces a filtering step, which reduces the number of files processed in subsequent steps and API requests. This improves both time and network efficiency. There is also effectively a removal of unnecessary logic (e.g., iterating over unchanged files), and the commit history becomes cleaner with only actual changes tracked.",Algorithm-Level Optimizations,Select Computationally Efficient Algorithms,high,True,,1528
2588963649,1994,ğŸ”§(turbo): Configure outputLogs errors-only to reduce AI agent token usage,"## Issue

- resolve: Reduce token usage when AI agents run lint commands while maintaining error visibility

## Why is this change needed?

When AI agents execute build tools like lint, gen, and fmt through Turborepo, the verbose output consumes significant tokens. By configuring `outputLogs: ""errors-only""`, we maintain error visibility for debugging while dramatically reducing unnecessary output that agents don't need to process.

## What would you like reviewers to focus on?

- Verify that error output is still properly displayed for debugging
- Confirm that the configuration covers all relevant build tasks
- Check that this doesn't break any existing CI/CD workflows

## Testing Verification

### Success
```sh
â¯ pnpm lint:turbo --force

> liam-frontend@0.0.1 lint:turbo /Users/mh4gf/ghq/github.com/liam-hq/liam
> turbo lint --force

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

                  Update available v2.5.3 â‰« v2.5.4
 Changelog: https://github.com/vercel/turborepo/releases/tag/v2.5.4
       Run ""pnpm dlx @turbo/codemod@latest update"" to update

       Follow @turborepo for updates: https://x.com/turborepo
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
turbo 2.5.3

â€¢ Packages in scope: @liam-hq/agent, @liam-hq/app, @liam-hq/cli, @liam-hq/configs, @liam-hq/db, @liam-hq/db-structure, @liam-hq/docs, @liam-hq/e2e, @liam-hq/erd-core, @liam-hq/erd-sample, @liam-hq/figma-to-css-variables, @liam-hq/github, @liam-hq/jobs, @liam-hq/mcp-server, @liam-hq/pglite-server, @liam-hq/storybook, @liam-hq/ui
â€¢ Running lint in 17 packages
â€¢ Remote caching disabled

 Tasks:    24 successful, 24 total
Cached:    0 cached, 24 total
  Time:    19.754s
```

### Failure

```sh
â¯ pnpm lint:turbo

> liam-frontend@0.0.1 lint:turbo /Users/mh4gf/ghq/github.com/liam-hq/liam
> turbo lint

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

                  Update available v2.5.3 â‰« v2.5.4
 Changelog: https://github.com/vercel/turborepo/releases/tag/v2.5.4
       Run ""pnpm dlx @turbo/codemod@latest update"" to update

       Follow @turborepo for updates: https://x.com/turborepo
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
turbo 2.5.3

â€¢ Packages in scope: @liam-hq/agent, @liam-hq/app, @liam-hq/cli, @liam-hq/configs, @liam-hq/db, @liam-hq/db-structure, @liam-hq/docs, @liam-hq/e2e, @liam-hq/erd-core, @liam-hq/erd-sample, @liam-hq/figma-to-css-variables, @liam-hq/github, @liam-hq/jobs, @liam-hq/mcp-server, @liam-hq/pglite-server, @liam-hq/storybook, @liam-hq/ui
â€¢ Running lint in 17 packages
â€¢ Remote caching disabled
@liam-hq/agent:lint: cache miss, executing 74fc4e76a3a16063
@liam-hq/agent:lint:
@liam-hq/agent:lint:
@liam-hq/agent:lint: > @liam-hq/agent@0.1.0 lint /Users/mh4gf/ghq/github.com/liam-hq/liam/frontend/internal-packages/agent
@liam-hq/agent:lint: > concurrently ""pnpm:lint:*""
@liam-hq/agent:lint:
@liam-hq/agent:lint: [eslint]
@liam-hq/agent:lint: [eslint] > @liam-hq/agent@0.1.0 lint:eslint /Users/mh4gf/ghq/github.com/liam-hq/liam/frontend/internal-packages/agent
@liam-hq/agent:lint: [eslint] > eslint .
@liam-hq/agent:lint: [eslint]
@liam-hq/agent:lint: [biome]
@liam-hq/agent:lint: [biome] > @liam-hq/agent@0.1.0 lint:biome /Users/mh4gf/ghq/github.com/liam-hq/liam/frontend/internal-packages/agent
@liam-hq/agent:lint: [biome] > biome check .
@liam-hq/agent:lint: [biome]
@liam-hq/agent:lint: [tsc]
@liam-hq/agent:lint: [tsc] > @liam-hq/agent@0.1.0 lint:tsc /Users/mh4gf/ghq/github.com/liam-hq/liam/frontend/internal-packages/agent
@liam-hq/agent:lint: [tsc] > tsc --noEmit
@liam-hq/agent:lint: [tsc]
@liam-hq/agent:lint: [biome] ./src/chat/workflow/shared/stateManager.ts organizeImports â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
@liam-hq/agent:lint: [biome]
@liam-hq/agent:lint: [biome]   Ã— Import statements could be sorted:
@liam-hq/agent:lint: [biome]
@liam-hq/agent:lint: [biome]       1   1 â”‚   import { schemaSchema } from '@liam-hq/db-structure'
@liam-hq/agent:lint: [biome]       2     â”‚ - importÂ·typeÂ·{Â·WorkflowStateÂ·}Â·fromÂ·'../types'
@liam-hq/agent:lint: [biome]       3     â”‚ - importÂ·*Â·asÂ·vÂ·fromÂ·'valibot'
@liam-hq/agent:lint: [biome]           2 â”‚ + importÂ·*Â·asÂ·vÂ·fromÂ·'valibot'
@liam-hq/agent:lint: [biome]           3 â”‚ + importÂ·typeÂ·{Â·WorkflowStateÂ·}Â·fromÂ·'../types'
@liam-hq/agent:lint: [biome]       4   4 â”‚
@liam-hq/agent:lint: [biome]       5   5 â”‚   /**
@liam-hq/agent:lint: [biome]
@liam-hq/agent:lint: [biome]
@liam-hq/agent:lint: [biome] check â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
@liam-hq/agent:lint: [biome]
@liam-hq/agent:lint: [biome]   Ã— Some errors were emitted while running checks.
@liam-hq/agent:lint: [biome]
@liam-hq/agent:lint: [biome]
@liam-hq/agent:lint: [biome] Skipped 1 suggested fixes.
@liam-hq/agent:lint: [biome] If you wish to apply the suggested (unsafe) fixes, use the command biome check --fix --unsafe
@liam-hq/agent:lint: [biome]
@liam-hq/agent:lint: [biome] Checked 35 files in 8ms. No fixes applied.
@liam-hq/agent:lint: [biome] Found 1 error.
@liam-hq/agent:lint: [biome] â€‰ELIFECYCLEâ€‰ Command failed with exit code 1.
@liam-hq/agent:lint: [biome] pnpm run lint:biome exited with code 1
@liam-hq/agent:lint: [tsc] pnpm run lint:tsc exited with code 0
@liam-hq/agent:lint: [eslint] pnpm run lint:eslint exited with code 0
@liam-hq/agent:lint: â€‰ELIFECYCLEâ€‰ Command failed with exit code 1.
@liam-hq/agent:lint: ERROR: command finished with error: command (/Users/mh4gf/ghq/github.com/liam-hq/liam/frontend/internal-packages/agent) /Users/mh4gf/.asdf/installs/nodejs/22.16.0/bin/pnpm run lint exited (1)
@liam-hq/agent#lint: command (/Users/mh4gf/ghq/github.com/liam-hq/liam/frontend/internal-packages/agent) /Users/mh4gf/.asdf/installs/nodejs/22.16.0/bin/pnpm run lint exited (1)

 Tasks:    22 successful, 23 total
Cached:    21 cached, 23 total
  Time:    2.847s
Failed:    @liam-hq/agent#lint

 ERROR  run failed: command  exited (1)
â€‰ELIFECYCLEâ€‰ Command failed with exit code 1.
```

## What was done

### ğŸ¤– Generated by PR Agent at 4049ba47d9858d737e085712f2f6f03dae52795f

â€¢ Configure `outputLogs: ""errors-only""` for build tools to reduce AI agent token usage
â€¢ Apply setting to build, gen, lint, and fmt tasks in turbo.json
â€¢ Maintain error visibility while minimizing verbose output


## Detailed Changes

<table><thead><tr><th></th><th align=""left"">Relevant files</th></tr></thead><tbody><tr><td><strong>Configuration changes</strong></td><td><table>
<tr>
  <td>
    <details>
      <summary><strong>turbo.json</strong><dd><code>Configure outputLogs errors-only for build tasks</code>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </dd></summary>
<hr>

turbo.json

â€¢ Added <code>""outputLogs"": ""errors-only""</code> to build, gen, lint, and fmt tasks<br> <br>â€¢ Modified fmt task from empty object to explicit configuration<br> â€¢ <br>Maintained all existing dependencies and configurations


</details>


  </td>
  <td><a href=""https://github.com/liam-hq/liam/pull/1994/files#diff-f8de965273949793edc0fbfe249bb458c0becde39b2e141db087bcbf5d4ad5e3"">+9/-4</a>&nbsp; &nbsp; &nbsp; </td>

</tr>
</table></td></tr></tr></tbody></table>

## Additional Notes

This change specifically targets AI agent efficiency while preserving developer experience for error handling.

___

> <details> <summary>  Need help?</summary><li>Type <code>/help how to ...</code> in the comments thread for any questions about Qodo Merge usage.</li><li>Check out the <a href=""https://qodo-merge-docs.qodo.ai/usage-guide/"">documentation</a> for more information.</li></details>",Human,31152321,MH4GF,closed,2025-06-13 03:05:58+00:00,2025-06-13 07:44:18+00:00,2025-06-13 07:44:18+00:00,,https://api.github.com/repos/liam-hq/liam,https://github.com/liam-hq/liam/pull/1994,perf,"The change configures the build tool output to reduce token usage by limiting logs to errors only, which is a configuration improvement rather than a bug fix or new feature. It optimizes the output behavior for AI agents without changing functionality, fitting best under 'perf' for performance improvement.","The change configures the build tool output to reduce token usage by limiting logs to errors only, which is a configuration improvement rather than a bug fix or new feature. It optimizes the output behavior for AI agents without changing functionality, fitting best under 'perf' for performance improvement.",Human,,,,,,TypeScript,True,True,False,4.638888888888889,4.638888888888889,0.193287037037037,0.193287037037037,73,10,7765.0,693.0,True,"The code configures the Turborepo tool to only output error logs ('errors-only') for the build, lint, generation, and formatting tasks. This solves the problem of excessive token consumption by AI agents caused by verbose logs, while ensuring that error messages remain visible for debugging and developer workflows.","The original configuration allowed Turborepo tasks to output full logs, which included a large amount of non-essential information. The optimized version reduces the output to only show errors, significantly decreasing the volume of data processed or stored (token usage) by AI agents. There are no algorithmic changes, but the configuration eliminates unnecessary log output, improving resource efficiency. This change does not modify the execution or results of the build, lint, or format tasks; it streamlines the data pipeline and improves performance for downstream consumers that parse these logs (e.g., AI agents).",Code Smells and Structural Simplification,Remove Redundant Method Calls,high,True,,3664
2452623588,16440,[V1][Perf] Avoid mem duplication when aggregating MM tensors,"When stacking or concatenating, replace the original tensors with slices into the larger one.

cc @ywang96 @DarkLight1337 ",Human,16958488,njhill,open,2025-04-11 01:12:56+00:00,,,,https://api.github.com/repos/vllm-project/vllm,https://github.com/vllm-project/vllm/pull/16440,perf,"The change improves performance by avoiding memory duplication during tensor aggregation, which is a performance optimization.","The change improves performance by avoiding memory duplication during tensor aggregation, which is a performance optimization.",Human,,,,,,Python,False,False,True,,,,,60,8,124.0,17.0,True,"The code is responsible for aggregating 'MM tensors' via operations like stacking or concatenation. Previously, this process duplicated data in memory by creating new tensors containing copies of the originals. The optimization change instead replaces the original tensor references with slices into a newly created, larger aggregated tensor, so data for each original is not redundantly stored.","Originally, the code aggregated tensors by duplicating their memory into a new, larger tensor through stacking or concatenation. This could result in high memory usage when dealing with large or numerous tensors. In the optimized version, instead of copying data, the code assigns slices (views) of the new aggregated tensor back to replace the original tensors. This prevents memory duplication and ensures only one copy of the data exists in memory. The change likely improves both space efficiency and runtime performance by reducing memory allocation and copying overhead. There is also an implicit structural simplification as tensor references now accurately reflect slices, decreasing code redundancy.",Memory and Data Locality Optimizations,Optimize Object Use,high,True,,1147
2260441374,684,StringUtils.equalsCaseIgnore optimisation fixes #683,"StringUtils.equalsCaseIgnore can be optimised for the common case where both strings exactly match. See #683 

`equalsCaseIgnore` is used in wire during deserialisation - `net.openhft.chronicle.wire.TextWire#read(java.lang.CharSequence, int, java.lang.Object)`

Screenshot shows before and after:

<img width=""1141"" alt=""image"" src=""https://github.com/user-attachments/assets/3725654c-1c1a-47d4-8e13-f89c8799f281"" />
 ",Human,109181,tgd,closed,2025-01-04 15:39:11+00:00,2025-01-05 01:34:27+00:00,2025-01-05 01:34:27+00:00,,https://api.github.com/repos/OpenHFT/Chronicle-Core,https://github.com/OpenHFT/Chronicle-Core/pull/684,perf,"The PR title and description indicate an optimization to the equalsCaseIgnore method, improving performance for a common case without adding features or fixing bugs.","The PR title and description indicate an optimization to the equalsCaseIgnore method, improving performance for a common case without adding features or fixing bugs.",Human,,,,,,Java,True,True,False,9.921111111111111,9.921111111111111,0.4133796296296296,0.4133796296296296,52,4,425.0,36.0,True,"The code provides an implementation of a method, presumably named equalsCaseIgnore, which compares two strings for equality, ignoring case. The function aims to determine as efficiently as possible whether the string values are the same, without regard to character case. This is frequently used in text parsing and deserialization code paths.","In the original version, the equalsCaseIgnore method likely performed a character-by-character case-insensitive comparison for all inputs, regardless of whether the two strings matched exactly in case. In the optimized version, a fast-path check is added: if both strings are exactly equal (case-sensitive equality), this is detected immediately and the function returns true. Only if the fast-path fails does the code proceed to the slower character-by-character case-insensitive comparison. This reduces unnecessary computation for the common case where the strings are actually identicalâ€”including case. No changes to the comparison algorithm other than this short-circuit are mentioned. This reduces runtime in the majority of deserialization cases, avoids extra allocations, and improves branch prediction by favoring the most common path. No redundant code was added or removed aside from this fast-path.",Control-Flow and Branching Optimizations,Make Conditional Branches More Predictable,high,True,,1269
2556468457,390,Postgres tuning,,Human,16663421,janbjorge,closed,2025-05-31 08:41:19+00:00,2025-06-02 14:40:58+00:00,,,https://api.github.com/repos/janbjorge/pgqueuer,https://github.com/janbjorge/pgqueuer/pull/390,perf,"The title suggests changes related to tuning Postgres, which typically involves performance improvements rather than adding features or fixing bugs.","The title suggests changes related to tuning Postgres, which typically involves performance improvements rather than adding features or fixing bugs.",Human,,,,,,Python,False,True,False,53.994166666666665,,2.2497569444444445,,15,2,,,False,"The code is related to tuning the configuration or usage of a PostgreSQL database, likely to improve query performance, lower latency, or reduce resource consumption. This may involve changing settings, query structures, or resource management to make the system more efficient.","The original version likely used default or less-optimal PostgreSQL settings or routines, which may not have leveraged hardware or workload characteristics effectively. The optimized version introduces new settings or code paths that improve query performance, for example, through better indexing, modifying memory usage parameters, or by changing application interaction patterns with the database. These changes reduce query times and memory consumption, and may eliminate redundant configuration parameters or method calls.",Code Smells and Structural Simplification,Remove Redundant Method Calls,medium,True,,1032
2269202548,272,Improve performMutation and formAction,,Human,204765,danielweinmann,closed,2025-01-09 20:48:20+00:00,2025-01-09 21:03:32+00:00,2025-01-09 21:03:32+00:00,,https://api.github.com/repos/seasonedcc/remix-forms,https://github.com/seasonedcc/remix-forms/pull/272,perf,"The title indicates an improvement in the performance of existing functions, which aligns with performance enhancements rather than new features or bug fixes.","The title indicates an improvement in the performance of existing functions, which aligns with performance enhancements rather than new features or bug fixes.",Human,,,,,,TypeScript,True,True,False,0.2533333333333333,0.2533333333333333,0.0105555555555555,0.0105555555555555,38,4,,,False,"The code is responsible for managing mutations and form actionsâ€”likely within a web framework or form handling utility. It processes mutations (possibly state changes or updates) and handles form actions efficiently, possibly validating input, executing callbacks, and updating internal state.","The optimized version simplifies the mutation and action processing logic by reducing redundant method calls and condensing conditional checks. It possibly removes nested or duplicated processing steps, streamlining the flow such that each mutation or action is handled with fewer control structures or repeated computations. There may also be the removal of intermediary variables or methods that were not contributing meaningful logic, thus reducing call overhead and improving maintainability.",Code Smells and Structural Simplification,Remove Redundant Method Calls,high,True,,1030
2537690761,1231,Speed up duplicative printing in taylor-alts,"This is super minor and dumb and a mis-feature already but this PR makes colonnade way faster without any significant code change, just by caching a string.",Human,30707,pavpanchekha,closed,2025-05-22 16:21:04+00:00,2025-05-22 16:51:29+00:00,2025-05-22 16:51:29+00:00,,https://api.github.com/repos/herbie-fp/herbie,https://github.com/herbie-fp/herbie/pull/1231,perf,The change improves performance by caching a string to speed up printing without altering functionality or adding features.,The change improves performance by caching a string to speed up printing without altering functionality or adding features.,Human,,,,,,HTML,True,True,False,0.5069444444444444,0.5069444444444444,0.0211226851851851,0.0211226851851851,44,6,156.0,27.0,True,"The code is handling duplicative printing in a 'taylor-alts' context, likely involving output formatting (such as table columns) using the colonnade library. The core functionality is to efficiently print or render tabular/columnar data, which previously involved recalculating or re-generating a string related to the print output multiple times, causing unnecessary overhead.","In the optimized version, the code introduces a cache for a string value that was previously recomputed or reconstructed repeatedly during the printing process. Algorithmically, the logic is unchanged: the same string is ultimately used, but it's generated once and stored for subsequent reuse. This removes redundant operations and reduces unnecessary allocation or computation, thereby improving performance, particularly for large or frequent tabular outputs. There are no significant structural or stylistic changes beyond this caching, and the improvement is focused on runtime efficiency.",Memory and Data Locality Optimizations,"Caching, Buffering",high,True,,1115
2443864788,1182,Delete unneeded `expand-rules` function,"It is, I think, unnecessary, and it costs a bit of time (though not a lot). I also removed the cacheâ€”let's see if it pays for itself.",Human,30707,pavpanchekha,closed,2025-04-07 16:59:52+00:00,2025-04-08 19:32:24+00:00,,,https://api.github.com/repos/herbie-fp/herbie,https://github.com/herbie-fp/herbie/pull/1182,perf,"The change involves removing an unnecessary function and a cache to potentially improve performance, which aligns with performance improvements rather than fixing a bug or adding a feature.","The change involves removing an unnecessary function and a cache to potentially improve performance, which aligns with performance improvements rather than fixing a bug or adding a feature.",Human,,,,,,HTML,False,True,False,26.54222222222222,,1.105925925925926,,39,4,133.0,27.0,True,"The code originally contained an `expand-rules` function and an associated caching mechanism, likely intended to process or transform some rules and cache the results for performance. The commit removes this function and the cache because they are considered unnecessary and introduce an overhead, albeit small.","The original version introduced (a) an extra function call (`expand-rules`) in the code path, and (b) a cache structure to store results from this function. The optimized version eliminates both. There is a redundant code removal: by deleting the function and its cache, the codebase is simplified, reducing overhead from unnecessary logic and potential maintenance issues. There is also possibly a minor algorithmic improvement by reducing a layer of indirection and avoiding any unnecessary computation and storage.",Code Smells and Structural Simplification,Remove Redundant Method Calls,high,True,,1112
2408616836,1092,âš¡ï¸ Speed up function `select_top_confidence_detection` by 188%,"### ğŸ“„ 188% (1.88x) speedup for ***`select_top_confidence_detection` in `inference/core/workflows/core_steps/common/query_language/operations/detections/base.py`***

â±ï¸ Runtime :   **`1.02 millisecond`**  **â†’** **`355 microseconds`** (best of `491` runs)
<details>
<summary> ğŸ“ Explanation and details</summary>

Here is an optimized version of the program.

</details>

âœ… **Correctness verification report:**


| Test                        | Status            |
| --------------------------- | ----------------- |
| âš™ï¸ Existing Unit Tests | ğŸ”˜ **None Found** |
| ğŸŒ€ Generated Regression Tests | âœ… **20 Passed** |
| âª Replay Tests | ğŸ”˜ **None Found** |
| ğŸ” Concolic Coverage Tests | ğŸ”˜ **None Found** |
|ğŸ“Š Tests Coverage       | 100.0% |
<details>
<summary>ğŸŒ€ Generated Regression Tests Details</summary>

```python
from copy import deepcopy

import numpy as np
# imports
import pytest  # used for our unit tests
from inference.core.workflows.core_steps.common.query_language.operations.detections.base import \
    select_top_confidence_detection


# Mocking the sv.Detections class for testing purposes
class Detections:
    def __init__(self, confidence):
        self.confidence = np.array(confidence)
    
    def __len__(self):
        return len(self.confidence)
    
    def __getitem__(self, index):
        return Detections([self.confidence[index]])
from inference.core.workflows.core_steps.common.query_language.operations.detections.base import \
    select_top_confidence_detection

# unit tests

def test_single_detection():
    # Single detection
    detections = Detections([0.5])
    codeflash_output = select_top_confidence_detection(detections)

def test_multiple_unique_confidence_scores():
    # Multiple detections with unique confidence scores
    detections = Detections([0.1, 0.5, 0.9])
    codeflash_output = select_top_confidence_detection(detections)

def test_empty_detections():
    # Empty detections
    detections = Detections([])
    codeflash_output = select_top_confidence_detection(detections)

def test_all_zero_confidence_scores():
    # All zero confidence scores
    detections = Detections([0.0, 0.0, 0.0])
    codeflash_output = select_top_confidence_detection(detections)

def test_multiple_maximum_confidence_scores():
    # Multiple maximum confidence scores
    detections = Detections([0.9, 0.9, 0.8])
    codeflash_output = select_top_confidence_detection(detections)

def test_large_number_of_detections():
    # Large number of detections
    large_confidences = np.random.rand(1000)
    detections = Detections(large_confidences)
    codeflash_output = select_top_confidence_detection(detections)

def test_high_precision_confidence_scores():
    # High precision confidence scores
    detections = Detections([0.123456789, 0.987654321, 0.555555555])
    codeflash_output = select_top_confidence_detection(detections)

def test_negative_confidence_scores():
    # Negative confidence scores
    detections = Detections([-0.1, -0.5, -0.9])
    codeflash_output = select_top_confidence_detection(detections)


def test_minimum_and_maximum_float_values():
    # Minimum and maximum float values
    detections = Detections([np.finfo(float).min, np.finfo(float).max])
    codeflash_output = select_top_confidence_detection(detections)

def test_immutable_input():
    # Ensure the original detections object is not modified
    detections = Detections([0.5, 0.7, 0.9])
    original_confidences = deepcopy(detections.confidence)
    codeflash_output = select_top_confidence_detection(detections)

def test_non_numeric_confidence_scores():
    # Non-numeric confidence scores should raise an error
    detections = Detections([0.5, 'high', None])
    with pytest.raises(TypeError):
        select_top_confidence_detection(detections)

# Run the tests
if __name__ == ""__main__"":
    pytest.main()
# codeflash_output is used to check that the output of the original code is the same as that of the optimized code.

from copy import deepcopy

import numpy as np
# imports
import pytest  # used for our unit tests
from inference.core.workflows.core_steps.common.query_language.operations.detections.base import \
    select_top_confidence_detection


# Mocking the sv.Detections class for testing purposes
class MockDetections:
    def __init__(self, confidence):
        self.confidence = np.array(confidence)
    
    def __len__(self):
        return len(self.confidence)
    
    def __getitem__(self, index):
        return MockDetections([self.confidence[index]])
from inference.core.workflows.core_steps.common.query_language.operations.detections.base import \
    select_top_confidence_detection


# unit tests
def test_empty_detections():
    # Test with empty detections
    detections = MockDetections([])
    codeflash_output = select_top_confidence_detection(detections)

def test_single_detection():
    # Test with a single detection
    detections = MockDetections([0.5])
    codeflash_output = select_top_confidence_detection(detections)

def test_multiple_unique_confidences():
    # Test with multiple detections with unique confidence scores
    detections = MockDetections([0.1, 0.5, 0.9])
    codeflash_output = select_top_confidence_detection(detections)

def test_multiple_duplicate_highest_confidences():
    # Test with multiple detections with duplicate highest confidence scores
    detections = MockDetections([0.5, 0.9, 0.9])
    codeflash_output = select_top_confidence_detection(detections)

def test_all_equal_confidences():
    # Test with multiple detections with all equal confidence scores
    detections = MockDetections([0.5, 0.5, 0.5])
    codeflash_output = select_top_confidence_detection(detections)

def test_non_numeric_confidences():
    # Test with non-numeric confidence scores (if applicable)
    detections = MockDetections(['high', 'medium', 'low'])
    with pytest.raises(TypeError):
        select_top_confidence_detection(detections)


def test_large_number_of_detections():
    # Test with a large number of detections
    detections = MockDetections(np.random.rand(10000))
    codeflash_output = select_top_confidence_detection(detections)

def test_performance_large_scale():
    # Performance test with large scale dataset
    detections = MockDetections(np.random.rand(1000000))
    codeflash_output = select_top_confidence_detection(detections)

# Run the tests
if __name__ == ""__main__"":
    pytest.main()
# codeflash_output is used to check that the output of the original code is the same as that of the optimized code.
```

</details>



[![Codeflash](https://img.shields.io/badge/Optimized%20with-Codeflash-yellow?style=flat&color=%23ffc428&logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iNDgwIiBoZWlnaHQ9ImF1dG8iIHZpZXdCb3g9IjAgMCA0ODAgMjgwIiBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgo8cGF0aCBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTI4Ni43IDAuMzc4NDE4SDIwMS43NTFMNTAuOTAxIDE0OC45MTFIMTM1Ljg1MUwwLjk2MDkzOCAyODEuOTk5SDk1LjQzNTJMMjgyLjMyNCA4OS45NjE2SDE5Ni4zNDVMMjg2LjcgMC4zNzg0MThaIiBmaWxsPSIjRkZDMDQzIi8+CjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMzExLjYwNyAwLjM3ODkwNkwyNTguNTc4IDU0Ljk1MjZIMzc5LjU2N0w0MzIuMzM5IDAuMzc4OTA2SDMxMS42MDdaIiBmaWxsPSIjMEIwQTBBIi8+CjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMzA5LjU0NyA4OS45NjAxTDI1Ni41MTggMTQ0LjI3NkgzNzcuNTA2TDQzMC4wMjEgODkuNzAyNkgzMDkuNTQ3Vjg5Ljk2MDFaIiBmaWxsPSIjMEIwQTBBIi8+CjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMjQyLjg3MyAxNjQuNjZMMTg5Ljg0NCAyMTkuMjM0SDMxMC44MzNMMzYzLjM0NyAxNjQuNjZIMjQyLjg3M1oiIGZpbGw9IiMwQjBBMEEiLz4KPC9zdmc+Cg==)](https://codeflash.ai)
",Human,1271289,misrasaurabh1,closed,2025-03-21 06:00:21+00:00,2025-03-26 13:40:42+00:00,2025-03-26 13:40:42+00:00,,https://api.github.com/repos/roboflow/inference,https://github.com/roboflow/inference/pull/1092,perf,"The PR title and body describe a significant speedup optimization for a function without adding new features or fixing bugs. The focus is on improving performance, as indicated by the 188% speedup and runtime reduction.","The PR title and body describe a significant speedup optimization for a function without adding new features or fixing bugs. The focus is on improving performance, as indicated by the 188% speedup and runtime reduction.",Human,,,,,,Python,True,True,False,127.6725,127.6725,5.3196875,5.3196875,62,7,7817.0,583.0,True,"The function `select_top_confidence_detection` selects the detection with the highest confidence score from a collection of detections. It takes as input an object with a `confidence` attribute (likely a NumPy array) and returns the detection (or detections) corresponding to the highest confidence value. This is useful in computer vision tasks (e.g., object detection) where one often wishes to identify the most confident prediction for further processing.","The optimized code achieves a 1.88x speedup over the original. While the source code is not shown, common slow patterns in such functions include using Python loops to scan for the highest value, which is outperformed by vectorized NumPy operations (such as `np.argmax`). The optimized version likely replaces Python-level iteration and branching with a direct, vectorized maximum search and indexing over the NumPy array, minimizing Python interpreter overhead and taking advantage of efficient C-level implementations. This eliminates unnecessary control flow and method calls that would have previously scanned or filtered the detections one-by-one.",Algorithm-Level Optimizations,Select Computationally Efficient Algorithms,high,True,,3362
2616290996,1385,âš¡ï¸ Speed up method `WithFixedSizeCache.add_model` by 50% in PR #1373 (`feat/pass-countinference-to-serverless-getweights`),"## âš¡ï¸ This pull request contains optimizations for PR #1373
If you approve this dependent PR, these changes will be merged into the original PR branch `feat/pass-countinference-to-serverless-getweights`.
>This PR will be automatically closed if the original PR is merged.
----
### ğŸ“„ 50% (0.50x) speedup for ***`WithFixedSizeCache.add_model` in `inference/core/managers/decorators/fixed_size_cache.py`***

â±ï¸ Runtime :   **`1.08 seconds`**  **â†’** **`722 milliseconds`** (best of `12` runs)
### ğŸ“ Explanation and details

Here's an optimized rewrite of your program, addressing profiling hot spots and general efficiency improvements.

**Optimization Summary:**

1. **Avoid Redundant Method Calls:** 
   - Minimize repeated lookups and calculations.
   - Cache computations/results when possible within function scope.
2. **Lazy Imports:** 
   - Move GC and optional torch imports where needed (they are only used upon eviction).
3. **Deque Optimizations:** 
   - In `WithFixedSizeCache.add_model`, avoid repeated `self._key_queue.remove(queue_id)` by checking position or maintaining a set for fast checks (no need, since only called if known present, and block is rare). Still, code can be reduced for clarity.
4. **Reduce logging** in the hot add logic (unless DEBUG mode; logging is a major time sink during profiling).
5. **Batch Removals:** 
   - Accumulate models to remove and do a single `gc.collect()` call after, instead of per-iteration. 
6. **Data structure** choices are left unchanged (deque is still best for explicit ordering here).
7. **General Logic**: Use local variables for lookups on attributes used multiple times (minor, but helps).

---




**Key Runtime Optimizations:**
- Only call `gc.collect()` after all removals in a batch, not after every single model eviction.
- Reduced logging in hot code paths (this was responsible for noticeable time in profiling).
- Use local variables when repeatedly accessing class attributes.
- Use direct inlining for `_resolve_queue_id` for this use case.
- Defensive handling if queue/model state falls out of syncâ€”never throws unnecessarily.

**Performance Note:**
If you profile again after these changes, most of the time will now be in actual model loading and removal. That is, this code will not be a noticeable bottleneck anymore in the workflow. If LRU cache size is much larger, consider further data structure optimizations such as a dict for constant-time eviction and presence checking, but for N ~ 8 this is not needed.


âœ… **Correctness verification report:**

| Test                        | Status            |
| --------------------------- | ----------------- |
| âª Replay Tests | ğŸ”˜ **None Found** |
| âš™ï¸ Existing Unit Tests | ğŸ”˜ **None Found** |
| ğŸ” Concolic Coverage Tests | ğŸ”˜ **None Found** |
| ğŸŒ€ Generated Regression Tests | âœ… **476 Passed** |
|ğŸ“Š Tests Coverage       | 85.2% |
<details>
<summary>ğŸŒ€ Generated Regression Tests and Runtime</summary>

```python
import sys
from collections import deque

# imports
import pytest
from inference.core.managers.decorators.fixed_size_cache import \
    WithFixedSizeCache

# function to test and minimal stubs/mocks

class DummyModel:
    """"""Minimal dummy model for testing.""""""
    def __init__(self, model_id, api_key):
        self.model_id = model_id
        self.api_key = api_key
        self.has_model_metadata = False

    def clear_cache(self, delete_from_disk=True):
        pass

class DummyModelRegistry:
    """"""Minimal dummy registry that returns DummyModel.""""""
    def get_model(self, resolved_identifier, api_key, countinference=None, service_secret=None):
        return DummyModel
class InferenceModelNotFound(Exception): pass
class InvalidModelIDError(Exception): pass

# Enum stub
class ModelEndpointType:
    ORT = type(""ORT"", (), {""value"": ""ort""})()
    value = ""ort""

# ModelManager and WithFixedSizeCache as in prompt, but minimal
class ModelManager:
    def __init__(self, model_registry, models=None):
        self.model_registry = model_registry
        self._models = models if models is not None else {}

    def add_model(self, model_id, api_key, model_id_alias=None, endpoint_type=ModelEndpointType.ORT, countinference=None, service_secret=None):
        resolved_identifier = model_id if model_id_alias is None else model_id_alias
        if resolved_identifier in self._models:
            return
        model_class = self.model_registry.get_model(resolved_identifier, api_key, countinference=countinference, service_secret=service_secret)
        model = model_class(model_id=model_id, api_key=api_key)
        self._models[resolved_identifier] = model

    def remove(self, model_id, delete_from_disk=True):
        if model_id not in self._models:
            raise InferenceModelNotFound()
        self._models[model_id].clear_cache(delete_from_disk=delete_from_disk)
        del self._models[model_id]

    def __contains__(self, model_id):
        return model_id in self._models

    def __getitem__(self, key):
        if key not in self._models:
            raise InferenceModelNotFound()
        return self._models[key]

    def __len__(self):
        return len(self._models)

    def keys(self):
        return self._models.keys()

# ========== UNIT TESTS BELOW ==========

@pytest.fixture
def cache_manager():
    """"""Returns a WithFixedSizeCache with max_size=3 for testing.""""""
    registry = DummyModelRegistry()
    base_manager = ModelManager(registry)
    return WithFixedSizeCache(base_manager, max_size=3)

@pytest.fixture
def unique_model_id():
    """"""Returns a function to generate unique model_ids for tests.""""""
    counter = [0]
    def _gen():
        counter[0] += 1
        return f""dataset{counter[0]}/1""
    return _gen

# 1. BASIC TEST CASES

def test_add_single_model(cache_manager, unique_model_id):
    """"""Test adding a single model works and is present.""""""
    model_id = unique_model_id()
    cache_manager.add_model(model_id, api_key=""key"")

def test_add_duplicate_model_noop(cache_manager, unique_model_id):
    """"""Adding the same model twice does not increase count.""""""
    model_id = unique_model_id()
    cache_manager.add_model(model_id, api_key=""key"")
    cache_manager.add_model(model_id, api_key=""key"")

def test_add_model_with_alias(cache_manager, unique_model_id):
    """"""Adding with an alias stores under the alias, not model_id.""""""
    model_id = unique_model_id()
    alias = ""alias1""
    cache_manager.add_model(model_id, api_key=""key"", model_id_alias=alias)

def test_add_multiple_models_up_to_capacity(cache_manager, unique_model_id):
    """"""Add up to max_size models, all should be present.""""""
    ids = [unique_model_id() for _ in range(3)]
    for mid in ids:
        cache_manager.add_model(mid, api_key=""key"")
    for mid in ids:
        pass

# 2. EDGE TEST CASES

def test_eviction_on_capacity(cache_manager, unique_model_id):
    """"""Adding more than max_size evicts least recently used.""""""
    ids = [unique_model_id() for _ in range(4)]
    for mid in ids[:3]:
        cache_manager.add_model(mid, api_key=""key"")
    # Now add a 4th, should evict ids[0]
    cache_manager.add_model(ids[3], api_key=""key"")

def test_eviction_marks_mru(cache_manager, unique_model_id):
    """"""Adding a model again marks it as most recently used (no eviction).""""""
    ids = [unique_model_id() for _ in range(3)]
    for mid in ids:
        cache_manager.add_model(mid, api_key=""key"")
    # Access ids[0] to mark it as MRU
    cache_manager.add_model(ids[0], api_key=""key"")
    # Add new model, should evict ids[1] now (was LRU)
    new_id = unique_model_id()
    cache_manager.add_model(new_id, api_key=""key"")

def test_add_model_with_alias_then_same_id(cache_manager, unique_model_id):
    """"""Adding with alias, then with same model_id, both can exist.""""""
    model_id = unique_model_id()
    alias = ""alias2""
    cache_manager.add_model(model_id, api_key=""key"", model_id_alias=alias)
    cache_manager.add_model(model_id, api_key=""key"")

def test_add_model_eviction_multiple_rounds(cache_manager, unique_model_id):
    """"""Eviction removes 3 at a time if possible when over threshold.""""""
    # Fill up to 3
    ids = [unique_model_id() for _ in range(3)]
    for mid in ids:
        cache_manager.add_model(mid, api_key=""key"")
    # Add 4th, should evict 1st
    cache_manager.add_model(""dataset999/1"", api_key=""key"")
    # Add 5th, should evict 3 more (but only 3 in cache, so only possible to evict all)
    cache_manager.add_model(""dataset1000/1"", api_key=""key"")

def test_remove_model(cache_manager, unique_model_id):
    """"""Test removing a model actually removes it.""""""
    model_id = unique_model_id()
    cache_manager.add_model(model_id, api_key=""key"")
    cache_manager.remove(model_id)

def test_remove_nonexistent_model_raises(cache_manager):
    """"""Removing a model not present raises InferenceModelNotFound.""""""
    with pytest.raises(InferenceModelNotFound):
        cache_manager.remove(""not-present/1"")


def test_add_model_with_alias_eviction(cache_manager, unique_model_id):
    """"""Eviction works when models are added by alias.""""""
    ids = [unique_model_id() for _ in range(2)]
    alias = ""alias3""
    cache_manager.add_model(ids[0], api_key=""key"", model_id_alias=alias)
    cache_manager.add_model(ids[1], api_key=""key"")
    cache_manager.add_model(""dataset888/1"", api_key=""key"")
    # Now add another to force eviction
    cache_manager.add_model(""dataset889/1"", api_key=""key"")
    # At least one of the first 3 should be evicted
    count = sum(mid in cache_manager for mid in [alias, ids[1], ""dataset888/1""])

def test_lru_eviction_order(cache_manager, unique_model_id):
    """"""Eviction order is LRU, not FIFO.""""""
    ids = [unique_model_id() for _ in range(3)]
    for mid in ids:
        cache_manager.add_model(mid, api_key=""key"")
    # Access ids[1] to make it MRU
    cache_manager.add_model(ids[1], api_key=""key"")
    # Add new model, should evict ids[0]
    new_id = unique_model_id()
    cache_manager.add_model(new_id, api_key=""key"")

def test_add_model_memory_pressure(monkeypatch, cache_manager, unique_model_id):
    """"""If memory_pressure_detected returns True, eviction is triggered.""""""
    monkeypatch.setattr(cache_manager, ""memory_pressure_detected"", lambda: True)
    # Fill up cache
    ids = [unique_model_id() for _ in range(3)]
    for mid in ids:
        cache_manager.add_model(mid, api_key=""key"")
    # Add another, should evict 3 at once
    cache_manager.add_model(""dataset2000/1"", api_key=""key"")

def test_add_model_exception_removes_from_queue(cache_manager, monkeypatch):
    """"""If add_model raises, queue is cleaned up.""""""
    # Patch model_manager.add_model to raise
    def raise_exc(*a, **kw): raise RuntimeError(""fail!"")
    monkeypatch.setattr(cache_manager.model_manager, ""add_model"", raise_exc)
    before_len = len(cache_manager._key_queue)
    with pytest.raises(RuntimeError):
        cache_manager.add_model(""dataset/1"", api_key=""key"")

# 3. LARGE SCALE TEST CASES

def test_large_number_of_models_eviction():
    """"""Add 10 models to a cache of size 5, only last 5 remain.""""""
    registry = DummyModelRegistry()
    base_manager = ModelManager(registry)
    cache_manager = WithFixedSizeCache(base_manager, max_size=5)
    ids = [f""ds{i}/1"" for i in range(10)]
    for mid in ids:
        cache_manager.add_model(mid, api_key=""key"")
    # Only last 5 should remain
    for mid in ids[:5]:
        pass
    for mid in ids[5:]:
        pass

def test_stress_add_and_access():
    """"""Add 20 models, repeatedly access some to keep them in cache.""""""
    registry = DummyModelRegistry()
    base_manager = ModelManager(registry)
    cache_manager = WithFixedSizeCache(base_manager, max_size=10)
    ids = [f""ds{i}/1"" for i in range(20)]
    for mid in ids[:10]:
        cache_manager.add_model(mid, api_key=""key"")
    # Repeatedly access first 5 to keep them MRU
    for _ in range(5):
        for mid in ids[:5]:
            cache_manager.add_model(mid, api_key=""key"")
    # Add next 10
    for mid in ids[10:]:
        cache_manager.add_model(mid, api_key=""key"")
    # The first 5 should still be in cache, next 5 should have been evicted
    for mid in ids[:5]:
        pass
    for mid in ids[5:10]:
        pass
    for mid in ids[10:]:
        pass

def test_add_models_with_aliases_large_scale():
    """"""Add 50 models with unique aliases, only last 10 remain in cache.""""""
    registry = DummyModelRegistry()
    base_manager = ModelManager(registry)
    cache_manager = WithFixedSizeCache(base_manager, max_size=10)
    for i in range(50):
        model_id = f""dataset{i}/1""
        alias = f""alias{i}""
        cache_manager.add_model(model_id, api_key=""key"", model_id_alias=alias)
    # Only last 10 aliases should be present
    for i in range(40):
        pass
    for i in range(40, 50):
        pass

def test_eviction_never_exceeds_max_size():
    """"""After many operations, cache never exceeds max_size.""""""
    registry = DummyModelRegistry()
    base_manager = ModelManager(registry)
    cache_manager = WithFixedSizeCache(base_manager, max_size=7)
    for i in range(30):
        cache_manager.add_model(f""ds{i}/1"", api_key=""key"")

def test_eviction_when_queue_empty_does_not_crash():
    """"""Eviction with empty queue does not raise.""""""
    registry = DummyModelRegistry()
    base_manager = ModelManager(registry)
    cache_manager = WithFixedSizeCache(base_manager, max_size=1)
    # Remove all models to empty queue
    cache_manager._key_queue.clear()
    try:
        cache_manager.add_model(""ds1/1"", api_key=""key"")
    except Exception:
        pytest.fail(""add_model should not raise when queue is empty"")
# codeflash_output is used to check that the output of the original code is the same as that of the optimized code.

from collections import deque

# imports
import pytest
from inference.core.managers.decorators.fixed_size_cache import \
    WithFixedSizeCache

# --- Minimal stubs and mocks for dependencies ---

# Exception classes
class RoboflowAPINotAuthorizedError(Exception):
    pass

class InferenceModelNotFound(Exception):
    pass

# ModelEndpointType enum stub
class ModelEndpointType:
    ORT = ""ort""

# Model stub
class DummyModel:
    def __init__(self, model_id, api_key):
        self.model_id = model_id
        self.api_key = api_key
        self.cleared = False

    def clear_cache(self, delete_from_disk=True):
        self.cleared = True

# ModelRegistry stub
class DummyModelRegistry:
    def get_model(self, resolved_identifier, api_key, countinference=None, service_secret=None):
        # Always returns DummyModel constructor
        return DummyModel

# --- The ModelManager, ModelManagerDecorator, and WithFixedSizeCache implementations ---

class ModelManager:
    def __init__(self, model_registry, models=None):
        self.model_registry = model_registry
        self._models = {} if models is None else models

    def add_model(
        self,
        model_id,
        api_key,
        model_id_alias=None,
        endpoint_type=ModelEndpointType.ORT,
        countinference=None,
        service_secret=None,
    ):
        resolved_identifier = model_id if model_id_alias is None else model_id_alias
        if resolved_identifier in self._models:
            return
        model_class = self.model_registry.get_model(
            resolved_identifier, api_key, countinference=countinference, service_secret=service_secret
        )
        model = model_class(model_id=model_id, api_key=api_key)
        self._models[resolved_identifier] = model

    def remove(self, model_id, delete_from_disk=True):
        if model_id not in self._models:
            raise InferenceModelNotFound(f""Model {model_id} not found"")
        self._models[model_id].clear_cache(delete_from_disk=delete_from_disk)
        del self._models[model_id]

    def __contains__(self, model_id):
        return model_id in self._models

    def __getitem__(self, key):
        if key not in self._models:
            raise InferenceModelNotFound(f""Model {key} not found"")
        return self._models[key]

    def __len__(self):
        return len(self._models)

    def keys(self):
        return self._models.keys()

# Global flag for API key check
MODELS_CACHE_AUTH_ENABLED = False

# --- UNIT TESTS ---

@pytest.fixture
def model_manager():
    # Returns a fresh ModelManager with DummyModelRegistry
    return ModelManager(DummyModelRegistry())

@pytest.fixture
def cache_manager(model_manager):
    # Returns a WithFixedSizeCache wrapping the above
    return WithFixedSizeCache(model_manager, max_size=4)

# 1. BASIC TEST CASES

def test_add_single_model_basic(cache_manager):
    """"""Test adding a single model to an empty cache.""""""
    cache_manager.add_model(""modelA/1"", ""KEY"")

def test_add_duplicate_model_noop(cache_manager):
    """"""Test that adding the same model twice does not increase cache size.""""""
    cache_manager.add_model(""modelA/1"", ""KEY"")
    cache_manager.add_model(""modelA/1"", ""KEY"")

def test_add_model_with_alias(cache_manager):
    """"""Test adding a model with an alias as queue id.""""""
    cache_manager.add_model(""modelA/1"", ""KEY"", model_id_alias=""aliasA"")

def test_add_model_with_different_aliases(cache_manager):
    """"""Test that different aliases are treated as different cache entries.""""""
    cache_manager.add_model(""modelA/1"", ""KEY"", model_id_alias=""aliasA"")
    cache_manager.add_model(""modelA/1"", ""KEY"", model_id_alias=""aliasB"")

def test_add_multiple_models_basic(cache_manager):
    """"""Test adding multiple distinct models.""""""
    cache_manager.add_model(""modelA/1"", ""KEY"")
    cache_manager.add_model(""modelB/1"", ""KEY"")
    cache_manager.add_model(""modelC/1"", ""KEY"")

# 2. EDGE TEST CASES

def test_add_model_eviction_lru(cache_manager):
    """"""Test that adding models over max_size evicts least recently used.""""""
    # Fill up cache
    cache_manager.add_model(""A/1"", ""KEY"")
    cache_manager.add_model(""B/1"", ""KEY"")
    cache_manager.add_model(""C/1"", ""KEY"")
    cache_manager.add_model(""D/1"", ""KEY"")
    # Add one more, triggers eviction (removes A/1, B/1, C/1 in order)
    cache_manager.add_model(""E/1"", ""KEY"")
    # Add another, triggers more evictions
    cache_manager.add_model(""F/1"", ""KEY"")

def test_add_model_lru_refresh(cache_manager):
    """"""Test that re-adding an existing model refreshes its LRU position.""""""
    cache_manager.add_model(""A/1"", ""KEY"")
    cache_manager.add_model(""B/1"", ""KEY"")
    cache_manager.add_model(""C/1"", ""KEY"")
    cache_manager.add_model(""D/1"", ""KEY"")
    # Refresh A/1
    cache_manager.add_model(""A/1"", ""KEY"")
    # Add E/1, should evict B/1, C/1, D/1 (A/1 was refreshed)
    cache_manager.add_model(""E/1"", ""KEY"")


def test_add_model_with_invalid_model_id(cache_manager):
    """"""Test that a model_id_alias with same name as another model_id is treated as distinct.""""""
    cache_manager.add_model(""modelA/1"", ""KEY"")
    cache_manager.add_model(""modelB/1"", ""KEY"", model_id_alias=""modelA/1"")

def test_add_model_evicts_all_when_cache_full(cache_manager):
    """"""Test that if more than max_size+3 models are added, all old models are evicted.""""""
    # Fill cache
    cache_manager.add_model(""A/1"", ""KEY"")
    cache_manager.add_model(""B/1"", ""KEY"")
    cache_manager.add_model(""C/1"", ""KEY"")
    cache_manager.add_model(""D/1"", ""KEY"")
    # Add 4 more, causing two eviction rounds
    cache_manager.add_model(""E/1"", ""KEY"")
    cache_manager.add_model(""F/1"", ""KEY"")
    cache_manager.add_model(""G/1"", ""KEY"")
    cache_manager.add_model(""H/1"", ""KEY"")
    # Only last 4 models should remain
    for mid in [""E/1"", ""F/1"", ""G/1"", ""H/1""]:
        pass
    for mid in [""A/1"", ""B/1"", ""C/1"", ""D/1""]:
        pass

def test_add_model_handles_exception_and_removes_from_queue(cache_manager):
    """"""Test that if ModelManager.add_model raises, the queue is cleaned up.""""""
    # Patch model_manager.add_model to raise
    orig_add_model = cache_manager.model_manager.add_model
    def raise_exc(*a, **kw):
        raise ValueError(""fail!"")
    cache_manager.model_manager.add_model = raise_exc
    with pytest.raises(ValueError):
        cache_manager.add_model(""Z/1"", ""KEY"")
    # Restore
    cache_manager.model_manager.add_model = orig_add_model

def test_add_model_with_alias_and_duplicate(cache_manager):
    """"""Test that adding same model with and without alias treats them as separate.""""""
    cache_manager.add_model(""A/1"", ""KEY"")
    cache_manager.add_model(""A/1"", ""KEY"", model_id_alias=""aliasA"")

# 3. LARGE SCALE TEST CASES

def test_add_many_models_and_evictions():
    """"""Test adding up to 20 models with cache size 10, check LRU eviction.""""""
    mm = ModelManager(DummyModelRegistry())
    cache = WithFixedSizeCache(mm, max_size=10)
    # Add 20 models
    for i in range(20):
        cache.add_model(f""model{i}/1"", ""KEY"")
    # Only last 10 should remain
    for i in range(10, 20):
        pass
    for i in range(10):
        pass

def test_add_models_with_aliases_large_scale():
    """"""Test adding models with unique aliases does not cause collisions.""""""
    mm = ModelManager(DummyModelRegistry())
    cache = WithFixedSizeCache(mm, max_size=50)
    # Add 50 models with unique aliases
    for i in range(50):
        cache.add_model(f""modelX/1"", ""KEY"", model_id_alias=f""alias_{i}"")
    # All aliases should be present
    for i in range(50):
        pass

def test_lru_eviction_pattern_stress():
    """"""Test LRU eviction pattern with repeated access and additions.""""""
    mm = ModelManager(DummyModelRegistry())
    cache = WithFixedSizeCache(mm, max_size=5)
    # Add 5 models
    for i in range(5):
        cache.add_model(f""M{i}/1"", ""KEY"")
    # Access models to change LRU order
    cache.add_model(""M2/1"", ""KEY"")
    cache.add_model(""M4/1"", ""KEY"")
    # Add new model, should evict oldest (M0/1, M1/1, M3/1 in order)
    cache.add_model(""M5/1"", ""KEY"")
    # Only most recently used and new should remain
    for mid in [""M2/1"", ""M4/1"", ""M5/1""]:
        pass

def test_add_models_performance_under_load():
    """"""Test that adding 100 models with cache size 50 only keeps last 50.""""""
    mm = ModelManager(DummyModelRegistry())
    cache = WithFixedSizeCache(mm, max_size=50)
    for i in range(100):
        cache.add_model(f""large_{i}/1"", ""KEY"")
    for i in range(50, 100):
        pass
    for i in range(50):
        pass

def test_add_models_with_same_alias_large_scale():
    """"""Test that adding many models with same alias overwrites previous.""""""
    mm = ModelManager(DummyModelRegistry())
    cache = WithFixedSizeCache(mm, max_size=10)
    for i in range(20):
        cache.add_model(f""modelQ_{i}/1"", ""KEY"", model_id_alias=""shared_alias"")
# codeflash_output is used to check that the output of the original code is the same as that of the optimized code.
```

</details>


To edit these changes `git checkout codeflash/optimize-pr1373-2025-06-24T21.57.17` and push.

[![Codeflash](https://img.shields.io/badge/Optimized%20with-Codeflash-yellow?style=flat&color=%23ffc428&logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iNDgwIiBoZWlnaHQ9ImF1dG8iIHZpZXdCb3g9IjAgMCA0ODAgMjgwIiBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgo8cGF0aCBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTI4Ni43IDAuMzc4NDE4SDIwMS43NTFMNTAuOTAxIDE0OC45MTFIMTM1Ljg1MUwwLjk2MDkzOCAyODEuOTk5SDk1LjQzNTJMMjgyLjMyNCA4OS45NjE2SDE5Ni4zNDVMMjg2LjcgMC4zNzg0MThaIiBmaWxsPSIjRkZDMDQzIi8+CjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMzExLjYwNyAwLjM3ODkwNkwyNTguNTc4IDU0Ljk1MjZIMzc5LjU2N0w0MzIuMzM5IDAuMzc4OTA2SDMxMS42MDdaIiBmaWxsPSIjMEIwQTBBIi8+CjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMzA5LjU0NyA4OS45NjAxTDI1Ni41MTggMTQ0LjI3NkgzNzcuNTA2TDQzMC4wMjEgODkuNzAyNkgzMDkuNTQ3Vjg5Ljk2MDFaIiBmaWxsPSIjMEIwQTBBIi8+CjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMjQyLjg3MyAxNjQuNjZMMTg5Ljg0NCAyMTkuMjM0SDMxMC44MzNMMzYzLjM0NyAxNjQuNjZIMjQyLjg3M1oiIGZpbGw9IiMwQjBBMEEiLz4KPC9zdmc+Cg==)](https://codeflash.ai)",Human,148906541,codeflash-ai[bot],closed,2025-06-24 21:57:23+00:00,2025-06-26 08:59:58+00:00,,,https://api.github.com/repos/roboflow/inference,https://github.com/roboflow/inference/pull/1385,perf,"The PR title and body clearly describe performance improvements and optimizations to an existing method, resulting in a 50% speedup. There is no indication of new features, bug fixes, or other types of changes. Therefore, the appropriate label is 'perf' for performance improvements.","The PR title and body clearly describe performance improvements and optimizations to an existing method, resulting in a 50% speedup. There is no indication of new features, bug fixes, or other types of changes. Therefore, the appropriate label is 'perf' for performance improvements.",Human,,,,,,Python,False,True,False,35.043055555555554,,1.4601273148148148,,122,11,24436.0,2247.0,True,The `WithFixedSizeCache.add_model` method is responsible for inserting models into a fixed-size cache that evicts older models (using LRU or similar logic) when new ones are added beyond the cache's capacity. This ensures the cache never exceeds a given maximum size and helps manage memory/resource consumption by keeping only a recent subset of models readily available.,"The optimized version greatly reduces runtime by eliminating repeated attribute lookups and redundant computations within the 'add_model' implementation. It defers heavy imports (such as 'gc' and 'torch') to the rare code paths where they are strictly necessary, thus minimizing import overhead in the hot path. Logging operations in frequently-called regions are reduced unless debugging is specifically enabled, which avoids log statement latency. Batch removal of evicted models is performed, calling 'gc.collect()' only once after all removals rather than after each evictionâ€”dramatically reducing the number of garbage collection cycles. Additional minor tweaks, such as favoring local variable assignments for frequently-used attributes and inlining a helper method when relevant, further enhance data locality and reduce function call overhead. The cache data structure (deque) is unchanged due to the small typical cache size, which is justified.",Code Smells and Structural Simplification,Remove Redundant Method Calls,high,True,,7539
2519831355,1280,âš¡ï¸ Speed up function `execute_gpt_4v_request` by 100% in PR #1214 (`openai-apikey-passthrough`),"## âš¡ï¸ This pull request contains optimizations for PR #1214
If you approve this dependent PR, these changes will be merged into the original PR branch `openai-apikey-passthrough`.
>This PR will be automatically closed if the original PR is merged.
----
### ğŸ“„ 100% (1.00x) speedup for ***`execute_gpt_4v_request` in `inference/core/workflows/core_steps/models/foundation/openai/v3.py`***

â±ï¸ Runtime :   **`107 milliseconds`**  **â†’** **`53.6 milliseconds`** (best of `5` runs)
### ğŸ“ Explanation and details

Here is an optimized version of your program for runtime and memory. The majority of runtime is IO/network-bound (API requests) and not CPU-bound code, so the best possible single-process CPU optimization is to **avoid repeated work** (e.g., repeated endpoint string formatting or client allocation) and **simplify fast paths**. If you can batch or async requests, that would reduce end-to-end latency, but that changes function signatures and semantics so is out of scope. Here we focus on making your function as lean as possible within its expected use. 

**Key improvements:**
- **Reuse OpenAI client (`OpenAI`) where possible**: Creating the client is surprisingly expensive per your profiling.
- **Optimize prompt and payload building:** Avoid unnecessary field-level assignments.
- **Use exception chaining efficiently.**
- **Minimize calls to `.startswith()` by using a tuple form.**
- **Precompute endpoint format string if possible.**
- **Move non-error computations out of try/except.**



**Summary:**  
- OpenAI client creation is now cached, saving repeated cost.
- Efficient prefix checking for OpenAI key.
- Payloads & try/except blocks are trimmed for speed and clarity.
- Function signatures and return values are preserved.
- Comments are updated only where logic is improved or needs clarification.

If you control parallelism at a higher level, running requests in parallel (with `asyncio` or threading) would yield much higher throughput as both requests and OpenAI are IO bound.


âœ… **Correctness verification report:**

| Test                        | Status            |
| --------------------------- | ----------------- |
| âš™ï¸ Existing Unit Tests | ğŸ”˜ **None Found** |
| ğŸŒ€ Generated Regression Tests | âœ… **9 Passed** |
| âª Replay Tests | ğŸ”˜ **None Found** |
| ğŸ” Concolic Coverage Tests | ğŸ”˜ **None Found** |
|ğŸ“Š Tests Coverage       |  |
<details>
<summary>ğŸŒ€ Generated Regression Tests Details</summary>

```python
import types
from typing import List, Optional

# imports
import pytest  # used for our unit tests
# function to test
import requests
from inference.core.env import API_BASE_URL
from inference.core.workflows.core_steps.models.foundation.openai.v3 import \
    execute_gpt_4v_request
from openai import OpenAI
from openai._types import NOT_GIVEN

# unit tests


# --------- Test helpers and monkeypatching ---------
class DummyResponse:
    """"""A dummy response object to simulate requests.Response.""""""
    def __init__(self, json_data=None, status_code=200, raise_exc=None, text=None):
        self._json_data = json_data or {}
        self.status_code = status_code
        self._raise_exc = raise_exc
        self.text = text or str(json_data)
    def json(self):
        return self._json_data
    def raise_for_status(self):
        if self._raise_exc:
            raise self._raise_exc

# --------- Basic Test Cases ---------











def test_proxied_request_missing_content(monkeypatch):
    """"""Test proxied request with missing 'content' in response (should raise).""""""
    def bad_post(url, json):
        return DummyResponse({""choices"": [{""message"": {}}]}, status_code=200)
    monkeypatch.setattr(requests, ""post"", bad_post)
    with pytest.raises(RuntimeError) as excinfo:
        execute_gpt_4v_request(
            roboflow_api_key=""rfkey123"",
            openai_api_key=""rf_key:account:abc"",
            prompt=[{""role"": ""user"", ""content"": ""Say hi""}],
            gpt_model_version=""gpt-4v"",
            max_tokens=10,
            temperature=0.5,
        )

def test_proxied_request_http_error(monkeypatch):
    """"""Test proxied request with HTTP error (should raise).""""""
    def bad_post(url, json):
        return DummyResponse({}, status_code=500, raise_exc=requests.HTTPError(""500""))
    monkeypatch.setattr(requests, ""post"", bad_post)
    with pytest.raises(RuntimeError) as excinfo:
        execute_gpt_4v_request(
            roboflow_api_key=""rfkey123"",
            openai_api_key=""rf_key:account:abc"",
            prompt=[{""role"": ""user"", ""content"": ""Say hi""}],
            gpt_model_version=""gpt-4v"",
            max_tokens=10,
            temperature=0.5,
        )

def test_direct_request_exception(monkeypatch):
    """"""Test direct request with OpenAI client raising exception (should raise).""""""
    class FailingOpenAIClient:
        def __init__(self, api_key):
            pass
        @property
        def chat(self):
            class C:
                @property
                def completions(self):
                    class D:
                        def create(self, *a, **k):
                            raise Exception(""OpenAI failure"")
                    return D()
            return C()
    monkeypatch.setattr(""openai.OpenAI"", lambda api_key: FailingOpenAIClient(api_key))
    with pytest.raises(RuntimeError) as excinfo:
        execute_gpt_4v_request(
            roboflow_api_key=""rfkey123"",
            openai_api_key=""sk-openai-002"",
            prompt=[{""role"": ""user"", ""content"": ""Say hi""}],
            gpt_model_version=""gpt-4v"",
            max_tokens=10,
            temperature=0.5,
        )

def test_proxied_request_index_error(monkeypatch):
    """"""Test proxied request with empty choices list (should raise).""""""
    def bad_post(url, json):
        return DummyResponse({""choices"": []}, status_code=200)
    monkeypatch.setattr(requests, ""post"", bad_post)
    with pytest.raises(RuntimeError) as excinfo:
        execute_gpt_4v_request(
            roboflow_api_key=""rfkey123"",
            openai_api_key=""rf_key:account:abc"",
            prompt=[{""role"": ""user"", ""content"": ""Say hi""}],
            gpt_model_version=""gpt-4v"",
            max_tokens=10,
            temperature=0.5,
        )

# --------- Large Scale Test Cases ---------








import types
from typing import List, Optional

# imports
import pytest  # used for our unit tests
# function to test
import requests
from inference.core.env import API_BASE_URL
from inference.core.workflows.core_steps.models.foundation.openai.v3 import \
    execute_gpt_4v_request
from openai import OpenAI
from openai._types import NOT_GIVEN

# unit tests


# --- Helpers for monkeypatching ---

class DummyResponse:
    def __init__(self, json_data, status_code=200):
        self._json = json_data
        self.status_code = status_code
        self.text = str(json_data)
    def json(self):
        return self._json
    def raise_for_status(self):
        if self.status_code >= 400:
            raise requests.exceptions.HTTPError(f""Status {self.status_code}"")

class DummyOpenAIChoices:
    def __init__(self, content):
        self.message = types.SimpleNamespace(content=content)

class DummyOpenAIResponse:
    def __init__(self, content):
        self.choices = [DummyOpenAIChoices(content)]

class DummyOpenAIChatCompletions:
    def __init__(self, content):
        self._content = content
    def create(self, model, messages, max_tokens, temperature):
        return DummyOpenAIResponse(self._content)

# --- Test cases ---

# BASIC TEST CASES


def test_proxied_openai_basic(monkeypatch):
    """"""Test proxied OpenAI call with normal parameters.""""""
    # Patch requests.post to return a dummy response
    def dummy_post(url, json):
        return DummyResponse({
            ""choices"": [
                {""message"": {""content"": ""proxied hello""}}
            ]
        })
    monkeypatch.setattr(requests, ""post"", dummy_post)
    # Patch API_BASE_URL to a dummy value for test
    monkeypatch.setattr(""inference.core.env.API_BASE_URL"", ""http://dummy"")
    # Call function with a proxied key
    codeflash_output = execute_gpt_4v_request(
        roboflow_api_key=""rf_dummy"",
        openai_api_key=""rf_key:account:abc123"",
        prompt=[{""role"": ""user"", ""content"": ""Say hello""}],
        gpt_model_version=""gpt-4-vision-preview"",
        max_tokens=10,
        temperature=0.5,
    ); result = codeflash_output


def test_invalid_openai_key(monkeypatch):
    """"""Test with an invalid OpenAI key (simulate exception from OpenAI).""""""
    def dummy_openai_init(self, api_key):
        raise Exception(""Invalid API key"")
    monkeypatch.setattr(OpenAI, ""__init__"", dummy_openai_init)
    with pytest.raises(RuntimeError) as e:
        execute_gpt_4v_request(
            roboflow_api_key=""rf_dummy"",
            openai_api_key=""sk-bad"",
            prompt=[{""role"": ""user"", ""content"": ""Test""}],
            gpt_model_version=""gpt-4-vision-preview"",
            max_tokens=10,
            temperature=0.5,
        )

def test_proxied_http_error(monkeypatch):
    """"""Test proxied call with HTTP error from requests.""""""
    def dummy_post(url, json):
        return DummyResponse({}, status_code=500)
    monkeypatch.setattr(requests, ""post"", dummy_post)
    monkeypatch.setattr(""inference.core.env.API_BASE_URL"", ""http://dummy"")
    with pytest.raises(RuntimeError) as e:
        execute_gpt_4v_request(
            roboflow_api_key=""rf_dummy"",
            openai_api_key=""rf_key:account:bad"",
            prompt=[{""role"": ""user"", ""content"": ""Test""}],
            gpt_model_version=""gpt-4-vision-preview"",
            max_tokens=10,
            temperature=0.5,
        )

def test_proxied_invalid_response_structure(monkeypatch):
    """"""Test proxied call with invalid response structure (missing keys).""""""
    def dummy_post(url, json):
        return DummyResponse({""bad"": ""data""})
    monkeypatch.setattr(requests, ""post"", dummy_post)
    monkeypatch.setattr(""inference.core.env.API_BASE_URL"", ""http://dummy"")
    with pytest.raises(RuntimeError) as e:
        execute_gpt_4v_request(
            roboflow_api_key=""rf_dummy"",
            openai_api_key=""rf_key:user:bad"",
            prompt=[{""role"": ""user"", ""content"": ""Test""}],
            gpt_model_version=""gpt-4-vision-preview"",
            max_tokens=10,
            temperature=0.5,
        )







def test_large_scale_proxied(monkeypatch):
    """"""Test proxied call with large prompt and max_tokens.""""""
    large_prompt = [{""role"": ""user"", ""content"": f""Message {i}""} for i in range(900)]
    def dummy_post(url, json):
        return DummyResponse({
            ""choices"": [
                {""message"": {""content"": ""large proxied""}}
            ]
        })
    monkeypatch.setattr(requests, ""post"", dummy_post)
    monkeypatch.setattr(""inference.core.env.API_BASE_URL"", ""http://dummy"")
    codeflash_output = execute_gpt_4v_request(
        roboflow_api_key=""rf_dummy"",
        openai_api_key=""rf_key:account:abc123"",
        prompt=large_prompt,
        gpt_model_version=""gpt-4-vision-preview"",
        max_tokens=999,
        temperature=0.5,
    ); result = codeflash_output
```

</details>


To edit these changes `git checkout codeflash/optimize-pr1214-2025-05-14T16.32.54` and push.

[![Codeflash](https://img.shields.io/badge/Optimized%20with-Codeflash-yellow?style=flat&color=%23ffc428&logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iNDgwIiBoZWlnaHQ9ImF1dG8iIHZpZXdCb3g9IjAgMCA0ODAgMjgwIiBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgo8cGF0aCBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTI4Ni43IDAuMzc4NDE4SDIwMS43NTFMNTAuOTAxIDE0OC45MTFIMTM1Ljg1MUwwLjk2MDkzOCAyODEuOTk5SDk1LjQzNTJMMjgyLjMyNCA4OS45NjE2SDE5Ni4zNDVMMjg2LjcgMC4zNzg0MThaIiBmaWxsPSIjRkZDMDQzIi8+CjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMzExLjYwNyAwLjM3ODkwNkwyNTguNTc4IDU0Ljk1MjZIMzc5LjU2N0w0MzIuMzM5IDAuMzc4OTA2SDMxMS42MDdaIiBmaWxsPSIjMEIwQTBBIi8+CjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMzA5LjU0NyA4OS45NjAxTDI1Ni41MTggMTQ0LjI3NkgzNzcuNTA2TDQzMC4wMjEgODkuNzAyNkgzMDkuNTQ3Vjg5Ljk2MDFaIiBmaWxsPSIjMEIwQTBBIi8+CjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMjQyLjg3MyAxNjQuNjZMMTg5Ljg0NCAyMTkuMjM0SDMxMC44MzNMMzYzLjM0NyAxNjQuNjZIMjQyLjg3M1oiIGZpbGw9IiMwQjBBMEEiLz4KPC9zdmc+Cg==)](https://codeflash.ai)",Human,148906541,codeflash-ai[bot],closed,2025-05-14 16:33:00+00:00,2025-05-19 19:46:04+00:00,2025-05-19 19:46:04+00:00,,https://api.github.com/repos/roboflow/inference,https://github.com/roboflow/inference/pull/1280,perf,The PR description and title clearly indicate that the changes are focused on optimizing the performance of the function `execute_gpt_4v_request` by reducing runtime and improving efficiency without adding new features or fixing bugs. This aligns with a performance improvement commit.,The PR description and title clearly indicate that the changes are focused on optimizing the performance of the function `execute_gpt_4v_request` by reducing runtime and improving efficiency without adding new features or fixing bugs. This aligns with a performance improvement commit.,Human,,,,,,Python,True,True,False,123.21777777777778,123.21777777777778,5.134074074074074,5.134074074074074,95,11,12624.0,988.0,True,"The function `execute_gpt_4v_request` is responsible for sending chat completion requests to the OpenAI GPT-4V API, either directly or proxied through another service depending on the API key prefix. It handles payload preparation, endpoint selection based on the API key, sending the HTTP request, error handling, and extracting the response message, all while maintaining compatibility and robustness.","The optimized version achieves a significant speedup by: (1) caching and reusing the OpenAI client, thus avoiding expensive client re-initialization on each call; (2) streamlining the payload construction by only setting required fields and avoiding redundant data assignments; (3) minimizing repeated string prefix checks by using tuple syntax for .startswith calls; (4) precomputing endpoint format strings outside of the main execution path and moving computations outside try/except blocks for cleaner error handling; (5) trimming down try/except coverage to only the error-prone regions, reducing Python interpreter overhead. There is also attention to exception chaining and clearer error propagation. These changes reduce per-call CPU and memory overhead, and make the control path more direct.",Code Smells and Structural Simplification,Remove Redundant Method Calls,high,True,,4517
2356811134,3329,â™»ï¸ Refactor: Migrate randString to rand v2,"# Description

This PR migrates `randString` from `math/rand` to `math/rand/v2` to improve randomness quality and concurrency safety. In addition, it reduces execution time by approximately **97%**, as shown in the benchmark test below.

```go
func Benchmark_RandomString(b *testing.B) {
	for i := 0; i < b.N; i++ {
		_ = randString(100)
	}
}
```

```
goos: linux
goarch: amd64
pkg: github.com/gofiber/fiber/v3/client
cpu: AMD EPYC 7763 64-Core Processor                
                â”‚   old.txt    â”‚               new.txt               â”‚
                â”‚    sec/op    â”‚   sec/op     vs base                â”‚
_RandomString-4   9862.0n Â± 0%   252.1n Â± 3%  -97.44% (p=0.000 n=10)
```

## Changes introduced

List the new features or adjustments introduced in this pull request. Provide details on benchmarks, documentation updates, changelog entries, and if applicable, the migration guide.

- [x] Benchmarks: Describe any performance benchmarks and improvements related to the changes.
- [ ] Documentation Update: Detail the updates made to the documentation and links to the changed files.
- [ ] Changelog/What's New: Include a summary of the additions for the upcoming release notes.
- [ ] Migration Guide: If necessary, provide a guide or steps for users to migrate their existing code to accommodate these changes.
- [ ] API Alignment with Express: Explain how the changes align with the Express API.
- [ ] API Longevity: Discuss the steps taken to ensure that the new or updated APIs are consistent and not prone to breaking changes.
- [ ] Examples: Provide examples demonstrating the new features or changes in action.

## Type of change

- [x] Performance improvement (non-breaking change which improves efficiency)
- [x] Code consistency (non-breaking change which improves code reliability and robustness)

## Checklist

Before you submit your pull request, please make sure you meet these requirements:

- [x] Followed the inspiration of the Express.js framework for new functionalities, making them similar in usage.
- [x] Conducted a self-review of the code and provided comments for complex or critical parts.
- [ ] Updated the documentation in the `/docs/` directory for [Fiber's documentation](https://docs.gofiber.io/).
- [ ] Added or updated unit tests to validate the effectiveness of the changes or new features.
- [x] Ensured that new and existing unit tests pass locally with the changes.
- [ ] Verified that any new dependencies are essential and have been agreed upon by the maintainers/community.
- [x] Aimed for optimal performance with minimal allocations in the new code.
- [x] Provided benchmarks for the new code to analyze and improve upon.
",Human,13825170,ksw2000,closed,2025-02-25 14:05:22+00:00,2025-02-25 16:11:46+00:00,2025-02-25 16:11:46+00:00,,https://api.github.com/repos/gofiber/fiber,https://github.com/gofiber/fiber/pull/3329,perf,"The PR description highlights a significant performance improvement by migrating to a new version of the random string generator, reducing execution time by approximately 97%. This clearly indicates a performance enhancement rather than a new feature or bug fix.","The PR description highlights a significant performance improvement by migrating to a new version of the random string generator, reducing execution time by approximately 97%. This clearly indicates a performance enhancement rather than a new feature or bug fix.",Human,,,,,,Go,True,True,False,2.1066666666666665,2.1066666666666665,0.0877777777777777,0.0877777777777777,42,7,2725.0,404.0,True,"The code provides a utility function, randString, that generates a random alphanumeric string of a specified length. This is commonly used for generating identifiers, test data, session tokens, etc. The problem it solves is producing high-quality, unpredictable, and efficiently generated random strings for use within the application.","The original implementation used Go's math/rand package, while the optimized version migrates to math/rand/v2. The v2 package is designed to offer improved randomness (better statistical qualities and fewer biases), better concurrency safety, and enhanced performance. Benchmark results show a dramatic reduction in execution time (97% faster). Algorithmically, the new version likely leverages a more efficient or parallelizable random number generation approach, reducing bottlenecks with global state or locks. Additionally, concurrency safety addresses previous risks of data races if randString was accessed from multiple goroutines. There is no indication of structural code simplification beyond the algorithmic change, but the move also increases future maintainability and reliability.",Algorithm-Level Optimizations,Select Computationally Efficient Algorithms,high,True,,1739
2544691147,3479,"Improve performance for ""equalFieldType"" function","OLD:
```
Benchmark_equalFieldType-12    	 3320424	       361.0 ns/op	      80 B/op	       9 allocs/op
```

NEW:
```
Benchmark_equalFieldType-12    	12170480	        99.85 ns/op	      16 B/op	       3 allocs/op
```
\+ solve the problem with passing on the tag name",Human,7063188,ReneWerner87,closed,2025-05-26 19:04:13+00:00,2025-05-27 08:48:37+00:00,2025-05-27 08:48:37+00:00,,https://api.github.com/repos/gofiber/fiber,https://github.com/gofiber/fiber/pull/3479,perf,"The changes improve the performance of the 'equalFieldType' function by reducing execution time and memory allocations, which is a clear performance enhancement.","The changes improve the performance of the 'equalFieldType' function by reducing execution time and memory allocations, which is a clear performance enhancement.",Human,,,,,,Go,True,True,False,13.74,13.74,0.5725,0.5725,49,5,272.0,32.0,True,"The 'equalFieldType' function determines if two fields have the same type, possibly as part of a reflection or serialization/deserialization process that needs to compare struct or field types based on their metadata or tags. It appears to solve the problem of efficiently comparing two fields, including potentially their tag names.","The optimized version significantly reduces both the time per operation (from ~361 ns to ~100 ns) and memory allocations (from 9 allocs to 3, and from 80 B to 16 B per operation). This implies major improvements in function efficiency. The probable algorithmic changes include removing redundant memory allocations, streamlining comparison logic, and possibly passing the tag name directly rather than computing or extracting it within the function. Any unnecessary operations, such as intermediate object creation, reflection calls, or complex string manipulations, were likely eliminated or simplified. The function's logic is probably more direct and focused, reflecting not only improved performance but cleaner code structure.",Code Smells and Structural Simplification,Remove Redundant Method Calls,high,True,,1203
2607579182,3532,Improve performance #3476,"# Description

Improve the performance of #3476 by using `nextCustom` and `next` separately

## Type of change

Please delete options that are not relevant.

- [x] Enhancement (improvement to existing features and functionality)

## Checklist

Before you submit your pull request, please make sure you meet these requirements:

- [ ] Followed the inspiration of the Express.js framework for new functionalities, making them similar in usage.
- [x] Conducted a self-review of the code and provided comments for complex or critical parts.
- [ ] Updated the documentation in the `/docs/` directory for [Fiber's documentation](https://docs.gofiber.io/).
- [ ] Added or updated unit tests to validate the effectiveness of the changes or new features.
- [ x Ensured that new and existing unit tests pass locally with the changes.
- [ ] Verified that any new dependencies are essential and have been agreed upon by the maintainers/community.
- [x] Aimed for optimal performance with minimal allocations in the new code.
- [ ] Provided benchmarks for the new code to analyze and improve upon.
",Human,13825170,ksw2000,closed,2025-06-20 13:50:28+00:00,2025-06-23 06:04:56+00:00,2025-06-23 06:04:56+00:00,,https://api.github.com/repos/gofiber/fiber,https://github.com/gofiber/fiber/pull/3532,perf,"The PR explicitly mentions improving performance by optimizing the use of `nextCustom` and `next` separately, which is a code change aimed at enhancing performance without adding new features or fixing bugs.","The PR explicitly mentions improving performance by optimizing the use of `nextCustom` and `next` separately, which is a code change aimed at enhancing performance without adding new features or fixing bugs.",Human,,,,,,Go,True,True,False,64.24111111111111,64.24111111111111,2.676712962962963,2.676712962962963,25,3,1107.0,173.0,True,"The code being discussed appears to be part of a web framework (possibly in Go, inspired by Express.js) that handles HTTP request processing. It seems to be optimizing the way next middleware or handler functions are invoked by splitting out use of `nextCustom` and `next` functions separately instead of sharing logic or function pointers.","Previously, the code might have used some common or less-specific handler routing, possibly causing unnecessary multiplexing or indirection when invoking middleware. The change specifically distinguishes the use of `nextCustom` and `next`, most likely eliminating an indirect function call or conditional logic for every invocation. This means specialized handlers are now used for different situations, allowing for more direct and possibly inlined function calls, reducing overhead, and improving CPU branch prediction in oft-used request paths. This is a structural optimization that avoids unnecessary generic logic, which can improve request throughput and micro-benchmark performance. There appear to be no major algorithmic changes, but rather targeted removal of redundant dispatching logic and more direct invocation paths. No explicit memory or loop-related optimization is detailed.",Code Smells and Structural Simplification,Remove Redundant Method Calls,high,True,,1350
2519312120,2040,Change `states_to_numbers` array from Jax to NumPy in `_reorder_kronecker_product`,"This PR in-place changes the [`n_unsorted`](https://github.com/netket/netket/blob/c6424cc9c540ffb9e3f1380b23d2be82abf3534a/netket/operator/_local_operator/helpers.py#L198) in the [`_reorder_kronecker_product`](https://github.com/netket/netket/blob/c6424cc9c540ffb9e3f1380b23d2be82abf3534a/netket/operator/_local_operator/helpers.py#L149) from a Jax to a NumPy array. 

It is only used internally in that function, and leaving it as a Jax array causes the [`np.argsort()`](https://github.com/netket/netket/blob/c6424cc9c540ffb9e3f1380b23d2be82abf3534a/netket/operator/_local_operator/helpers.py#L203) to become comparatively slow. 

Quick benchmark of constructing an operator with a rather large number of products of LocalOperators shows runtime went from approx. 10 minutes to approx. 3 minutes after the change.",Human,85965501,waleed-sh,closed,2025-05-14 13:20:06+00:00,2025-05-14 13:50:07+00:00,2025-05-14 13:50:07+00:00,,https://api.github.com/repos/netket/netket,https://github.com/netket/netket/pull/2040,perf,"The change improves the performance of the function by switching from Jax to NumPy arrays, resulting in a significant runtime reduction. This is a performance improvement, not a bug fix or feature addition.","The change improves the performance of the function by switching from Jax to NumPy arrays, resulting in a significant runtime reduction. This is a performance improvement, not a bug fix or feature addition.",Human,,,,,,Python,True,True,False,0.5002777777777778,0.5002777777777778,0.0208449074074074,0.0208449074074074,82,9,818.0,67.0,True,"The code in _reorder_kronecker_product is responsible for reordering products of quantum operators efficiently, mapping certain state representations to numbers using an array (states_to_numbers). This mapping is performed internally to facilitate sorting or indexing in subsequent calculations. The process involves using the n_unsorted array, which, prior to the change, was a Jax array.","Originally, the intermediate array (n_unsorted) was instantiated as a Jax array. This caused performance issues because NumPy operations like np.argsort performed poorly on Jax array inputs due to dispatching and device/context crossing overhead. The optimization replaces this with a standard NumPy array, ensuring np.argsort and related NumPy-based manipulations operate at native speed. Algorithmic logic remains unchanged, but the change eliminates overhead from framework interoperability and redundant conversions. This results in a dramatic runtime decrease (from ~10 to ~3 minutes), showing improved data locality and reduced abstraction penalties. No redundant code or methodology is removed, but the structural change (array type) alone provides the speedup.",Memory and Data Locality Optimizations,Access Data with Appropriate Type,high,True,,1393
2324987642,8994,realized tensors become RESHAPE(BUFFER),,Human,77887910,Qazalin,closed,2025-02-10 03:13:24+00:00,2025-02-10 09:17:54+00:00,2025-02-10 09:17:54+00:00,,https://api.github.com/repos/tinygrad/tinygrad,https://github.com/tinygrad/tinygrad/pull/8994,perf,"The title indicates a change in how tensors are realized, specifically transforming them into RESHAPE(BUFFER), which suggests a modification in implementation likely aimed at improving performance or efficiency rather than fixing a bug or adding a feature.","The title indicates a change in how tensors are realized, specifically transforming them into RESHAPE(BUFFER), which suggests a modification in implementation likely aimed at improving performance or efficiency rather than fixing a bug or adding a feature.",Human,,,,,,Python,True,True,False,6.075,6.075,0.253125,0.253125,39,4,,,False,"The code changes how realized tensors are represented: instead of being realized as potentially new memory allocations or as separate tensor objects, they are now directly described as a RESHAPE operation on an existing BUFFER. This likely means that tensors that are merely reshaped views, but with same underlying buffer, do not incur additional memory or computation costs besides metadata/shape manipulations.","Originally, realizing a tensor may have involved constructing new objects or copying data, especially if the realization process resulted in an explicit instantiation of an intermediate tensor. After the optimization, the code emits a RESHAPE(BUFFER) instruction, directly representing reshaped tensors as a view over the original buffer, avoiding extra object creation or data duplication. This removes potentially redundant allocations and computation, reduces memory usage, and can improve cache efficiency. There are no apparent algorithmic changesâ€”the mathematical intent is preservedâ€”but the realization process becomes more efficient both in memory and in CPU usage.",Memory and Data Locality Optimizations,Optimize Object Use,high,True,,1101
2545078467,1379,Faster UQFF serialization,"

<!-- This is an auto-generated comment: release notes by coderabbit.ai -->

## Summary by CodeRabbit

- **New Features**
  - Improved serialization of raw byte buffers, enabling direct and efficient export as tensors without unnecessary data cloning.
  - Additional residual tensors (`q_norm` and `k_norm`) are now included in exported model data for enhanced completeness.

- **Performance Improvements**
  - Optimized serialization process for quantized layers, reducing memory usage and improving efficiency.

<!-- end of auto-generated comment: release notes by coderabbit.ai -->",Human,65165915,EricLBuehler,closed,2025-05-27 02:02:14+00:00,2025-05-27 02:02:56+00:00,2025-05-27 02:02:56+00:00,,https://api.github.com/repos/EricLBuehler/mistral.rs,https://github.com/EricLBuehler/mistral.rs/pull/1379,perf,"The PR introduces performance improvements in the serialization process, optimizing memory usage and efficiency without adding new features or fixing bugs.","The PR introduces performance improvements in the serialization process, optimizing memory usage and efficiency without adding new features or fixing bugs.",Human,,,,,,Rust,True,True,False,0.0116666666666666,0.0116666666666666,0.0004861111111111,0.0004861111111111,25,3,598.0,79.0,True,"The code is responsible for serializing quantized layers and associated tensors in a machine learning model. It now serializes raw byte buffers more efficiently, bypassing unnecessary data cloning, and includes additional tensors (q_norm, k_norm) in the output for completeness. The goal is to optimize the speed and memory usage during model export.","Previously, the serialization process likely involved copying or cloning raw byte arrays into new memory buffers before exporting them, which incurs extra memory usage and overhead. The optimized code exports the raw byte buffers directly as tensors, eliminating the intermediate data copy. This reduces memory footprint and serialization time. The inclusion of new residual tensors expands the completeness but does not directly impact optimization. The primary performance gain comes from removing unnecessary data cloning and utilizing the data more directly during serialization. Structurally, the code has become leaner and more efficient in how it handles and exports raw tensor data.",Code Smells and Structural Simplification,Remove Redundant Method Calls,high,True,,1196
2558083620,776,Cache busting,,Human,173255911,242816,closed,2025-06-01 12:15:32+00:00,2025-06-01 12:20:24+00:00,2025-06-01 12:20:24+00:00,,https://api.github.com/repos/bionic-gpt/bionic-gpt,https://github.com/bionic-gpt/bionic-gpt/pull/776,perf,"The title 'Cache busting' suggests an improvement related to performance optimization by ensuring fresh content is served, which aligns with performance enhancements.","The title 'Cache busting' suggests an improvement related to performance optimization by ensuring fresh content is served, which aligns with performance enhancements.",Human,,,,,,Rust,True,True,False,0.0811111111111111,0.0811111111111111,0.0033796296296296,0.0033796296296296,13,2,,,False,"The code implements a cache busting mechanism, likely for static assets in a web application. This ensures that clients fetch updated resources when their contents change by modifying the asset URL or reference, often using a version hash or timestamp.","The optimized version introduces or improves the logic for cache busting, making asset URLs unique when their content changes. This may change the method for generating cache-busting values (e.g., switching from timestamps to content hashes, or automating the injection of cache-busting strings into URLs). The change removes the risk of users seeing stale resources due to aggressive browser caching, and may also eliminate redundant or manual cache-busting logic, streamlining the process and reducing maintenance overhead.",Memory and Data Locality Optimizations,"Caching, Buffering",high,True,,1039
2615702170,264,patch/propagate extract parallelism,"- **propagate extract parallelism**
- **remove redundant stripe sources**
",Human,16530606,karakanb,closed,2025-06-24 17:30:28+00:00,2025-06-24 17:43:15+00:00,2025-06-24 17:43:15+00:00,,https://api.github.com/repos/bruin-data/ingestr,https://github.com/bruin-data/ingestr/pull/264,perf,"The changes involve improving the parallelism of the extract process and removing redundant sources, which suggests an enhancement in performance rather than a bug fix or new feature.","The changes involve improving the parallelism of the extract process and removing redundant sources, which suggests an enhancement in performance rather than a bug fix or new feature.",Human,,,,,,Python,True,True,False,0.2130555555555555,0.2130555555555555,0.0088773148148148,0.0088773148148148,35,3,76.0,9.0,True,"The code is concerned with handling parallelism during extraction operations, possibly in a data processing or ETL framework. It involves propagating information or capabilities related to parallel extraction through the relevant abstraction or pipeline. Additionally, the code removes redundant 'stripe sources', which are likely data slice providers that became unnecessary after this parallelism propagation.","Previously, the code may have redundantly handled the extraction of data in parallel or maintained multiple 'stripe source' objects. The optimization propagates the extraction parallelism through the appropriate layers, reducing duplication of work. By removing redundant stripe sources, it cleans up data source management, leading to reduced overhead and potential memory savings. The changes streamline the code by focusing parallelism configuration where needed and not duplicating parallel logic or state.",Code Smells and Structural Simplification,Remove Redundant Method Calls,high,True,,1092
2352318434,1823,Reducing testing time of car/tests to be about 0.3 seconds (link to #1184),"I was able to reduce the testing time of car/tests to about 0.3 seconds just by removing the parameterized import in all the files and replacing it with for loops. This links to #1184. 

I can check to see if I can reduce it further to less than 0.1 seconds according to the bounty, but this pull request can either be used as a checkpoint (or completion if the reduction is not possible).",Human,29354143,tanyamalik2000,closed,2025-02-23 20:39:28+00:00,2025-03-02 04:07:52+00:00,,,https://api.github.com/repos/commaai/opendbc,https://github.com/commaai/opendbc/pull/1823,perf,"The change improves the performance of the test suite by reducing the testing time significantly, which classifies it as a performance improvement.","The change improves the performance of the test suite by reducing the testing time significantly, which classifies it as a performance improvement.",Human,,,,,,Python,False,True,False,151.47333333333333,,6.311388888888889,,74,13,391.0,73.0,True,"The code in question executes tests for a car-related module. Originally, the test files used a parameterized test import (likely from a testing framework extension such as pytest's parametrize or unittest's parameterized), which allows for running the same test logic over multiple inputs automatically. The problem being solved is excessive total test running time, which is undesirable in continuous integration and during development. The new approach replaces the parameterized import with explicit for loops to iterate over test cases within each test function, aiming to reduce overhead.","In the original version, parameterized imports likely caused the testing framework to generate and schedule many test cases dynamically, incurring overhead per instantiation, more I/O, and possibly redundant setup. In the optimized version, that machinery is bypassed: by manually looping over test cases in regular test functions, only a single test function is seen and scheduled by the framework, leading to less overhead and faster completion. This removes redundant meta-level test instantiation, reduces function call and setup time, and likely simplifies reporting (at the expense of less granular reporting per test case). The core test logic is unchanged, but the method of iteration and harness integration is made more streamlined for performance.",Code Smells and Structural Simplification,Remove Redundant Method Calls,high,True,,1250
2542615571,10,Refactor PPU for performance improvements,"This commit introduces two main performance enhancements to the PPU:

1.  **Eager DMG Frame Buffer Calculation:** The `Ppu::frame_buffer()` method previously used lazy evaluation for DMG mode, calculating the entire frame buffer from the shade_buffer on the first request. This commit changes `render_map_dmg` to populate `self.frame_buffer` directly during scanline rendering, similar to CGB mode. This distributes the computation cost and makes `Ppu::frame_buffer()` a consistently fast operation for both modes.

2.  **Optimize `fill_frame_buffer` with Pattern Copy:** The `Ppu::fill_frame_buffer` method, used for clearing the screen or filling it with a specific color, has been optimized. For the main loop that populates `self.frame_buffer` with a uniform color, the implementation now uses `std::ptr::copy_nonoverlapping` with a pre-filled repeating RGB pattern when the `simd` feature is enabled. This leverages potential compiler optimizations for bulk memory copies, which can be significantly faster than scalar iteration for this task. The original scalar loop is retained if the `simd` feature is not active.

These changes aim to reduce potential stutter in DMG mode and accelerate screen fill operations, contributing to overall emulator performance.

<!-- This is an auto-generated comment: release notes by coderabbit.ai -->
## Summary by CodeRabbit

- **Performance Improvements**
  - Enhanced frame buffer filling with SIMD optimization for faster rendering when supported.
- **Bug Fixes**
  - Improved accuracy of color rendering in DMG mode by updating the frame buffer immediately with the correct palette colors.
<!-- end of auto-generated comment: release notes by coderabbit.ai -->",Human,25441,joamag,open,2025-05-25 19:55:21+00:00,,,,https://api.github.com/repos/joamag/boytacean,https://github.com/joamag/boytacean/pull/10,perf,"The commit introduces performance enhancements to the PPU by optimizing frame buffer calculation and screen fill operations, which directly improve the performance of the system without adding new features or fixing bugs.","The commit introduces performance enhancements to the PPU by optimizing frame buffer calculation and screen fill operations, which directly improve the performance of the system without adding new features or fixing bugs.",Human,,,,,,Rust,False,False,True,,,,,41,5,1722.0,237.0,True,"The code is part of an emulator's PPU (Pixel Processing Unit) responsible for rendering graphics. The changes aim to improve rendering performance by distributing framebuffer computations across scanlines and optimizing bulk screen-filling with efficient memory copying, particularly when SIMD optimizations are available.","1. The first optimization shifts from a lazy computation model for the DMG frame buffer, which previously did all work on first access, to an eager, incremental approach where framebuffer pixels are populated during scanline rendering. This distributes computational load and eliminates spikes, allowing for smoother and more predictable performance when the frame buffer is accessed.
2. The second optimization replaces a scalar loop for filling the framebuffer with a single color by a fast, bulk memory operation (`std::ptr::copy_nonoverlapping`) that copies a precomputed RGB pattern, taking advantage of compiler and CPU vectorization (when SIMD is enabled). This is significantly faster than per-pixel assignment for large buffers. The code falls back to the previous simple loop if SIMD is not enabled.
Both changes remove inefficient patterns and exploit efficient routines, leading to lower rendering latency and better runtime uniformity. The modifications do not alter the rendering logic or algorithms used but rather how and when the computations are performed.",Algorithm-Level Optimizations,Select Computationally Efficient Algorithms,high,True,,1513
2316356365,988,WIP JLBH perf test with integrated async profiler call,WIP - this is a trail at adding a profiler to run with the JLBH perf tests,Human,174972019,benbonavia,open,2025-02-04 23:55:26+00:00,,,,https://api.github.com/repos/OpenHFT/Chronicle-Wire,https://github.com/OpenHFT/Chronicle-Wire/pull/988,perf,"The PR is about adding a profiler to performance tests, which is an improvement related to performance measurement and profiling.","The PR is about adding a profiler to performance tests, which is an improvement related to performance measurement and profiling.",Human,,,,,,Java,False,False,True,,,,,54,9,74.0,17.0,True,"The code attempts to integrate the async-profiler tool directly into the JLBH (Java Latency Benchmark Harness) performance test workflow, aiming to collect profile data during benchmark executions for performance analysis.","Original code runs JLBH benchmarking by itself and relies on external tools or manual steps for profiling. With the addition, the code now triggers the async-profiler programmatically as part of the test run, which may automate profiler data collection. However, there is no clear change to the benchmarking or measurement algorithms, nor to the memory usage, core logic, or redundant code removal; rather, there is an auxiliary enhancement for observability and diagnostics.",No Meaningful Change,null,high,True,,1072
2277950711,985,Optimisation around append sep [WIP],,Human,109181,tgd,open,2025-01-14 23:16:52+00:00,,,,https://api.github.com/repos/OpenHFT/Chronicle-Wire,https://github.com/OpenHFT/Chronicle-Wire/pull/985,perf,"The title indicates an optimization related to appending a separator, which suggests a performance improvement rather than a bug fix or new feature.","The title indicates an optimization related to appending a separator, which suggests a performance improvement rather than a bug fix or new feature.",Human,,,,,,Java,False,False,True,,,,,36,5,,,False,"The code in question is related to string manipulation, specifically focused on appending a separator (such as a comma, tab, or custom delimiter) between elements when building up a stringâ€”for example, converting a list of items into a delimited string. The usual challenge is avoiding an extra separator at the start or end, and the code solves this efficiently during the append process.","In the optimized version, redundant checks and control logic around when to insert the separator have been simplified. Instead of conditionally checking before each append whether the separator is needed, the code may employ logic such as always appending the separator before an element except the first, or using a flag to indicate if the separator should be added. This reduces unnecessary branches and makes the code easier to follow, leading to improved branch prediction and potentially better runtime performance. The structure is cleaner, removing extraneous conditions or counters.",Control-Flow and Branching Optimizations,Make Conditional Branches More Predictable,high,True,,1094
2260678480,984,Tweak JsonWire benchmark,,Human,109181,tgd,closed,2025-01-05 09:41:04+00:00,2025-01-08 03:48:00+00:00,2025-01-08 03:48:00+00:00,,https://api.github.com/repos/OpenHFT/Chronicle-Wire,https://github.com/OpenHFT/Chronicle-Wire/pull/984,perf,"The change is described as a 'tweak' to a benchmark, which suggests a modification to improve or adjust performance measurement rather than fixing a bug or adding a feature.","The change is described as a 'tweak' to a benchmark, which suggests a modification to improve or adjust performance measurement rather than fixing a bug or adding a feature.",Human,,,,,,Java,True,True,False,66.11555555555556,66.11555555555556,2.754814814814815,2.754814814814815,24,3,,,False,"The code is benchmarking the performance of JsonWire, likely a library for serializing and/or deserializing JSON data. The benchmark code measures and compares execution characteristics (such as speed or memory usage) of JsonWire operations under various inputs or configurations. The purpose is to identify performance characteristics or regressions in JsonWire.","The optimization in this commit modifies the benchmark logic rather than the underlying serialization or deserialization algorithms. Possible changes could include improving how test data is generated, how results are measured, or removing unnecessary operations from the benchmark loop. These tweaks may involve eliminating redundant measurements, simplifying setup or teardown procedures, or improving instrumentation. While this does not alter the core algorithm, it does remove or streamline unnecessary code from the benchmark, potentially reducing noise in measurements and making the benchmarks more accurate and efficient.",Code Smells and Structural Simplification,Remove Redundant Method Calls,medium,True,,1072
2504407177,4511,[stdlib] Small improvements to `BitSet`,"CC @abduld.

* Replaced `InlineArray.__getitem__` with the unchecked `unsafe_get` to eliminate bounds checks. This avoids significant IR bloatâ€”up to 20Ã— for cases like `BitSet(mask)._words[0]`.
* Removed an unnecessary `String` allocation to skip a `malloc` and reduce memory overhead.",Human,6129707,soraros,closed,2025-05-07 13:29:01+00:00,2025-05-13 06:11:33+00:00,,,https://api.github.com/repos/modular/modular,https://github.com/modular/modular/pull/4511,perf,"The changes improve performance by eliminating bounds checks and reducing memory overhead, which are optimizations rather than bug fixes or new features.","The changes improve performance by eliminating bounds checks and reducing memory overhead, which are optimizations rather than bug fixes or new features.",Human,,,,,,Mojo,False,True,False,136.70888888888888,,5.696203703703703,,39,5,288.0,38.0,True,"The code deals with improving the internal performance of a BitSet implementation in the standard library. Specifically, it modifies internal array access to use an unchecked method and removes a redundant string allocation. The aim is to make BitSet operations faster and use less memory.","The original code used InlineArray.__getitem__ for array access, which performed bounds checking. The optimized version switches to unsafe_get, eliminating those checks and reducing the intermediate representation (IR) footprint. This reduces overhead and makes access to BitSet words more efficient. Additionally, an unnecessary String allocation was removed, which saves a memory allocation (malloc) and reduces memory pressure, as well as improves execution time by eliminating superfluous object creation.",Memory and Data Locality Optimizations,Optimize Object Use,high,True,,1136
2527565003,443,optimize misc config ui,"optimize misc config ui, more origanized config layout and group.",Human,22167673,ObservedObserver,closed,2025-05-19 05:13:47+00:00,2025-05-20 02:16:22+00:00,2025-05-20 02:16:22+00:00,,https://api.github.com/repos/Kanaries/graphic-walker,https://github.com/Kanaries/graphic-walker/pull/443,perf,"The PR title and body indicate improvements to the UI layout and organization, which enhances the user experience but does not add a new feature or fix a bug. This is best classified as a performance improvement in terms of UI usability and organization.","The PR title and body indicate improvements to the UI layout and organization, which enhances the user experience but does not add a new feature or fix a bug. This is best classified as a performance improvement in terms of UI usability and organization.",Human,,,,,,TypeScript,True,True,False,21.04305555555555,21.04305555555555,0.8767939814814815,0.8767939814814815,23,4,65.0,10.0,True,"The code manages the UI layout and organization for a miscellaneous configuration settings page or module, responsible for presenting various config options to users in grouped and organized sections.","The optimization commit reorganizes the UI layout for the miscellaneous config section. The main changes involve restructuring the layout and grouping settings more logically for improved readability and navigation. There may be some removal of redundant UI elements or code associated with the layout, and the code becomes more streamlined. However, there are no substantial algorithmic changes or efficiency improvements at the computational or data level; the enhancements are primarily structural and related to code/readability maintenance.",Code Smells and Structural Simplification,Remove Duplicates,medium,True,,1053
2524180167,112,âš¡ï¸ Speed up method `ObjectComparer.__init__` by 51%,"### ğŸ“„ 51% (0.51x) speedup for ***`ObjectComparer.__init__` in `src/json_repair/object_comparer.py`***

â±ï¸ Runtime :   **`125 nanoseconds`**  **â†’** **`83.0 nanoseconds`** (best of `187` runs)
### ğŸ“ Explanation and details

Here is a more optimized version of your program.



**Optimizations made:**
- Added `__slots__ = ()` to prevent the overhead of the instance `__dict__` since the class does not define any instance attributes.
- Changed `return` to `pass` since `return` is unnecessary in `__init__` without a value.


âœ… **Correctness verification report:**

| Test                        | Status            |
| --------------------------- | ----------------- |
| âš™ï¸ Existing Unit Tests | ğŸ”˜ **None Found** |
| ğŸŒ€ Generated Regression Tests | âœ… **3 Passed** |
| âª Replay Tests | ğŸ”˜ **None Found** |
| ğŸ” Concolic Coverage Tests | âœ… **3 Passed** |
|ğŸ“Š Tests Coverage       | 100.0% |
<details>
<summary>ğŸŒ€ Generated Regression Tests Details</summary>

```python
import pytest  # used for our unit tests
from src.json_repair.object_comparer import ObjectComparer

# unit tests

# 1. Basic Test Cases



















import pytest
from src.json_repair.object_comparer import ObjectComparer

# unit tests

# -------------------------------
# Basic Test Cases
# -------------------------------

def test_init_no_arguments():
    """"""Test __init__ with no arguments.""""""
    obj = ObjectComparer()











def test_init_with_invalid_items_type():
    """"""Test __init__ with invalid items type (should raise TypeError).""""""
    with pytest.raises(TypeError):
        ObjectComparer(items=""not a list"")

def test_init_with_invalid_strict_type():
    """"""Test __init__ with invalid strict type (should raise TypeError).""""""
    with pytest.raises(TypeError):
        ObjectComparer(strict=""yes"")









from src.json_repair.object_comparer import ObjectComparer

def test_ObjectComparer___init__():
    ObjectComparer.__init__(ObjectComparer())
```

</details>


To edit these changes `git checkout codeflash/optimize-ObjectComparer.__init__-maqndxva` and push.

[![Codeflash](https://img.shields.io/badge/Optimized%20with-Codeflash-yellow?style=flat&color=%23ffc428&logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iNDgwIiBoZWlnaHQ9ImF1dG8iIHZpZXdCb3g9IjAgMCA0ODAgMjgwIiBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgo8cGF0aCBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTI4Ni43IDAuMzc4NDE4SDIwMS43NTFMNTAuOTAxIDE0OC45MTFIMTM1Ljg1MUwwLjk2MDkzOCAyODEuOTk5SDk1LjQzNTJMMjgyLjMyNCA4OS45NjE2SDE5Ni4zNDVMMjg2LjcgMC4zNzg0MThaIiBmaWxsPSIjRkZDMDQzIi8+CjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMzExLjYwNyAwLjM3ODkwNkwyNTguNTc4IDU0Ljk1MjZIMzc5LjU2N0w0MzIuMzM5IDAuMzc4OTA2SDMxMS42MDdaIiBmaWxsPSIjMEIwQTBBIi8+CjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMzA5LjU0NyA4OS45NjAxTDI1Ni41MTggMTQ0LjI3NkgzNzcuNTA2TDQzMC4wMjEgODkuNzAyNkgzMDkuNTQ3Vjg5Ljk2MDFaIiBmaWxsPSIjMEIwQTBBIi8+CjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMjQyLjg3MyAxNjQuNjZMMTg5Ljg0NCAyMTkuMjM0SDMxMC44MzNMMzYzLjM0NyAxNjQuNjZIMjQyLjg3M1oiIGZpbGw9IiMwQjBBMEEiLz4KPC9zdmc+Cg==)](https://codeflash.ai)",Human,148906541,codeflash-ai[bot],closed,2025-05-16 10:20:41+00:00,2025-05-16 10:27:00+00:00,2025-05-16 10:27:00+00:00,,https://api.github.com/repos/mangiucugna/json_repair,https://github.com/mangiucugna/json_repair/pull/112,perf,"The PR title and body describe an optimization that improves the performance of a method without adding new features or fixing bugs. The changes focus on speeding up the method, which classifies as a performance improvement.","The PR title and body describe an optimization that improves the performance of a method without adding new features or fixing bugs. The changes focus on speeding up the method, which classifies as a performance improvement.",Human,,,,,,Python,True,True,False,0.1052777777777777,0.1052777777777777,0.004386574074074,0.004386574074074,51,7,3247.0,235.0,True,"The ObjectComparer.__init__ method is a constructor for the ObjectComparer class, which appears to be used for comparing objects. In the optimized version, the class uses __slots__ to prevent dynamic attribute creation and instance __dict__ allocation, indicating that the class does not expect instance attributes and aims to minimize memory and object initialization overhead.","The main optimization is the addition of __slots__ = (), which prevents the creation of the instance __dict__. This reduces both memory usage per instance and method call overhead, resulting in a significant speedup. Another minor optimization is replacing an extraneous 'return' statement with 'pass' in the __init__ method, since __init__ does not need to return a value. There are no algorithmic changes, as the functional logic remains unchanged; the performance improvement comes purely from eliminating redundant or unnecessary structure within the class.",Memory and Data Locality Optimizations,Optimize Object Use,high,True,,2267
2524300649,114,âš¡ï¸ Speed up method `JSONParser.parse_number` by 17%,"### ğŸ“„ 17% (0.17x) speedup for ***`JSONParser.parse_number` in `src/json_repair/json_parser.py`***

â±ï¸ Runtime :   **`7.25 microseconds`**  **â†’** **`6.21 microseconds`** (best of `27` runs)
### ğŸ“ Explanation and details

Here is an optimized version of your program, with a focus on reducing runtime and memory usage in the `parse_number` and `get_char_at` functions, as suggested by the profile data.  
Key improvements.
- **Avoid unnecessary set creation** inside hot loops.
- **Minimize attribute access** in tight loops (local variable caching for `self.get_char_at` rather than repeated method call).
- **Reduce string concatenation** inside loops by collecting characters into a list and joining once at the end.

All comments are preserved except those adjacent to changed lines, which are updated if relevant.



**Key optimization notes:**
- `parse_number` now uses a `number_chars` list to gather characters, avoiding ""string + char"" concatenation which is O(nÂ²) in Python.  
- `get_char_at` is bound to a local variable to avoid repeated attribute/method lookup inside the loop.
- Direct string `""0123456789-.eE/,""` is used for membership check instead of recreating a set each call, as the set is tiny and `in`-string checks are fast for small sets.

All changes preserve existing functionality and logging behavior. The rest of the code remains unmodified (other than whitespace for style).  

You can further optimize if profiling shows parse_string as another hot spot,  
but the major bottleneck per your profile was only in parse_number and get_char_at.


âœ… **Correctness verification report:**

| Test                        | Status            |
| --------------------------- | ----------------- |
| âš™ï¸ Existing Unit Tests | ğŸ”˜ **None Found** |
| ğŸŒ€ Generated Regression Tests | âœ… **97 Passed** |
| âª Replay Tests | ğŸ”˜ **None Found** |
| ğŸ” Concolic Coverage Tests | âœ… **4 Passed** |
|ğŸ“Š Tests Coverage       | 81.8% |
<details>
<summary>ğŸŒ€ Generated Regression Tests Details</summary>

```python
from typing import Any, ClassVar, Literal

# imports
import pytest  # used for our unit tests
from src.json_repair.json_parser import JSONParser


# Minimal stubs for ContextValues and JsonContext to allow testing
class ContextValues:
    ARRAY = ""ARRAY""
    OBJECT_KEY = ""OBJECT_KEY""
    OBJECT_VALUE = ""OBJECT_VALUE""

# unit tests

@pytest.mark.parametrize(
    ""input_str,expected,desc"",
    [
        # Basic integer
        (""123"", 123, ""Simple integer""),
        (""0"", 0, ""Zero integer""),
        (""-42"", -42, ""Negative integer""),
        # Basic float
        (""3.14"", 3.14, ""Simple float""),
        (""-0.001"", -0.001, ""Negative float""),
        (""0.0"", 0.0, ""Zero float""),
        # Basic scientific notation
        (""1e3"", 1000.0, ""Scientific notation positive exponent""),
        (""-2.5E-2"", -0.025, ""Scientific notation negative exponent""),
        (""6.02e23"", 6.02e23, ""Large scientific notation""),
        # Number with trailing non-numeric
        (""42abc"", ""42"", ""Number followed by alpha (should fallback to string)""),
        (""3.14pie"", ""3.14"", ""Float followed by alpha (should fallback to string)""),
        (""-123.45xyz"", ""-123.45"", ""Negative float followed by alpha""),
        # Number with comma (should return as string)
        (""1,234"", ""1,234"", ""Number with comma""),
        (""12,345.67"", ""12,345.67"", ""Float with comma""),
        # Number ending with invalid char
        (""789-"", 789, ""Number ending with - (should ignore)""),
        (""5.3e/"", 5.3, ""Number ending with / (should ignore)""),
        (""10E,"", 10, ""Number ending with , (should ignore)""),
        # Edge: Only sign
        (""-"", """", ""Just a minus sign""),
        # Edge: Only decimal point
        (""."", """", ""Just a dot""),
        # Edge: Only exponent
        (""e"", """", ""Just an exponent""),
        # Edge: Empty string
        ("""", """", ""Empty input""),
        # Edge: Multiple dots
        (""1.2.3"", 1.2, ""Multiple dots, should parse up to second dot""),
        # Edge: Multiple exponents
        (""1e2e3"", 100.0, ""Multiple exponents, should parse up to second e""),
        # Edge: Leading zeros
        (""000123"", 123, ""Leading zeros""),
        (""000.456"", 0.456, ""Leading zeros in float""),
        # Edge: Negative zero
        (""-0"", 0, ""Negative zero""),
        # Edge: Large integer
        (""999999999"", 999999999, ""Large integer""),
        # Edge: Large negative integer
        (""-999999999"", -999999999, ""Large negative integer""),
        # Edge: Large float
        (""3.141592653589793238"", 3.141592653589793, ""Very long float (Python float precision)""),
        # Edge: Array context, should stop at comma
        (""123,456"", 123, ""Array context, stops at comma""),
        # Edge: Array context, with negative number
        (""-789,123"", -789, ""Array context, negative number stops at comma""),
        # Edge: Array context, float
        (""1.23,4.56"", 1.23, ""Array context, float stops at comma""),
        # Edge: Number with leading plus (should fail to parse as number)
        (""+123"", """", ""Leading plus is not handled, should return empty string""),
        # Edge: Number with embedded whitespace
        (""12 34"", 12, ""Whitespace breaks number parsing""),
        # Edge: Number with tab
        (""56\t78"", 56, ""Tab breaks number parsing""),
        # Edge: Negative float with exponent
        (""-1.23e-4"", -1.23e-4, ""Negative float with exponent""),
        # Edge: Float with positive exponent
        (""2.5E+3"", 2500.0, ""Float with explicit positive exponent""),
        # Edge: Float with exponent and trailing chars
        (""7.89e2abc"", ""7.89e2"", ""Float with exponent and trailing alpha""),
        # Edge: Number with multiple commas
        (""1,234,567"", ""1,234,567"", ""Number with multiple commas""),
        # Edge: Number with trailing whitespace
        (""123 "", 123, ""Number with trailing space""),
        # Edge: Number with leading whitespace
        ("" 456"", """", ""Leading whitespace not handled, should return empty string""),
        # Edge: Negative sign only
        (""-"", """", ""Just a negative sign""),
        # Edge: Dot only
        (""."", """", ""Just a dot""),
        # Edge: Exponent only
        (""e"", """", ""Just an e""),
        # Edge: Negative exponent only
        (""-e"", """", ""Negative sign and e""),
        # Edge: Float with no leading digit
        ("".5"", 0.5, ""Float with no leading digit""),
        # Edge: Float with no trailing digit
        (""5."", 5.0, ""Float with no trailing digit""),
        # Edge: Number with slash (should ignore slash)
        (""123/456"", 123, ""Slash breaks number parsing""),
        # Edge: Number with multiple slashes
        (""12/34/56"", 12, ""Multiple slashes break parsing""),
        # Edge: Number with embedded dash
        (""12-34"", 12, ""Dash in the middle breaks parsing""),
        # Edge: Number with multiple exponents (invalid)
        (""1e2e3"", 100.0, ""Multiple exponents, only first is parsed""),
        # Edge: Number with trailing dot and comma
        (""5.,"", 5.0, ""Number ending with dot and comma""),
        # Edge: Number with trailing dot and slash
        (""5./"", 5.0, ""Number ending with dot and slash""),
        # Edge: Number with trailing e and comma
        (""5e,"", 5, ""Number ending with e and comma""),
        # Edge: Number with trailing e and slash
        (""5e/"", 5, ""Number ending with e and slash""),
        # Edge: Number with trailing dash and comma
        (""5-,"", 5, ""Number ending with dash and comma""),
        # Edge: Number with trailing dash and slash
        (""5-/"", 5, ""Number ending with dash and slash""),
        # Edge: Number with comma at start (should return empty string)
        ("",123"", """", ""Comma at start, should return empty string""),
        # Edge: Number with dot at start (should return empty string)
        ("".123"", 0.123, ""Dot at start, float with no leading digit""),
        # Edge: Number with only comma
        ("","", """", ""Only comma""),
        # Edge: Number with only dash
        (""-"", """", ""Only dash""),
        # Edge: Number with only slash
        (""/"", """", ""Only slash""),
    ]
)
def test_parse_number_basic_and_edge(input_str, expected, desc):
    """"""
    Test basic and edge cases for parse_number.
    """"""
    parser = JSONParser(input_str)
    codeflash_output = parser.parse_number(); result = codeflash_output













from typing import Any, ClassVar, Literal

# imports
import pytest
from src.json_repair.json_parser import JSONParser


# Dummy ContextValues and JsonContext for test purposes
class ContextValues:
    OBJECT_KEY = ""OBJECT_KEY""
    OBJECT_VALUE = ""OBJECT_VALUE""
    ARRAY = ""ARRAY""

# unit tests

# -------------------------
# 1. Basic Test Cases
# -------------------------

@pytest.mark.parametrize(
    ""input_str,expected"",
    [
        # Integer
        (""123"", 123),
        (""0"", 0),
        (""-42"", -42),
        # Float
        (""3.14"", 3.14),
        (""-0.001"", -0.001),
        (""2e3"", 2000.0),
        (""-2E-2"", -0.02),
        # Leading zeros (should parse as int)
        (""007"", 7),
        # Trailing whitespace (should ignore)
        (""123 "", 123),
        # Number with comma (should return as string)
        (""1,234"", ""1,234""),
        # Number with trailing comma (should ignore comma)
        (""123,"", 123),
        # Number with trailing slash (should ignore slash)
        (""123/"", 123),
        # Number with trailing e (should ignore e)
        (""123e"", 123),
        # Number with trailing - (should ignore -)
        (""123-"", 123),
        # Negative float with exponent
        (""-1.23e-10"", -1.23e-10),
        # Float with positive exponent
        (""1.23E+10"", 1.23e10),
    ]
)
def test_parse_number_basic(input_str, expected):
    parser = JSONParser(input_str)
    codeflash_output = parser.parse_number(); result = codeflash_output
    if isinstance(expected, float):
        pass
    else:
        pass

# -------------------------
# 2. Edge Test Cases
# -------------------------

@pytest.mark.parametrize(
    ""input_str,expected"",
    [
        # Empty string
        ("""", """"),
        # Only minus sign
        (""-"", """"),
        # Only dot
        (""."", """"),
        # Only exponent
        (""e"", """"),
        # Only comma
        ("","", """"),
        # Only slash
        (""/"", """"),
        # Multiple dots (invalid float)
        (""1.2.3"", ""1.2.3""),
        # Multiple exponents (invalid float)
        (""1e2e3"", ""1e2e3""),
        # Number followed by alpha (should call parse_string, so returns as string)
        (""123abc"", ""123abc""),
        # Number with comma in array context (should stop at comma)
        (""123,456"", 123),
        # Negative number with trailing comma
        (""-42,"", -42),
        # Negative float with trailing comma
        (""-42.5,"", -42.5),
        # Number with embedded slash (invalid, returns as string)
        (""12/34"", ""12/34""),
        # Number with embedded comma (invalid, returns as string)
        (""12,34"", ""12,34""),
        # Large negative exponent
        (""1e-308"", 1e-308),
        # Large positive exponent
        (""1e308"", 1e308),
        # Number with leading whitespace (should parse correctly)
        (""   42"", 42),
        # Number with trailing whitespace and comma
        (""42 ,"", 42),
        # Number with leading/trailing whitespace
        (""  42  "", 42),
    ]
)
def test_parse_number_edge(input_str, expected):
    parser = JSONParser(input_str.strip())
    codeflash_output = parser.parse_number(); result = codeflash_output
    if isinstance(expected, float):
        pass
    else:
        pass














from src.json_repair.json_parser import JSONParser

def test_JSONParser_parse_number():
    JSONParser.parse_number(JSONParser('e', None, False, json_fd_chunk_length=0, stream_stable=True))

def test_JSONParser_parse_number_2():
    JSONParser.parse_number(JSONParser('53', None, None, json_fd_chunk_length=0, stream_stable=False))
```

</details>


To edit these changes `git checkout codeflash/optimize-JSONParser.parse_number-maqpo82d` and push.

[![Codeflash](https://img.shields.io/badge/Optimized%20with-Codeflash-yellow?style=flat&color=%23ffc428&logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iNDgwIiBoZWlnaHQ9ImF1dG8iIHZpZXdCb3g9IjAgMCA0ODAgMjgwIiBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgo8cGF0aCBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTI4Ni43IDAuMzc4NDE4SDIwMS43NTFMNTAuOTAxIDE0OC45MTFIMTM1Ljg1MUwwLjk2MDkzOCAyODEuOTk5SDk1LjQzNTJMMjgyLjMyNCA4OS45NjE2SDE5Ni4zNDVMMjg2LjcgMC4zNzg0MThaIiBmaWxsPSIjRkZDMDQzIi8+CjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMzExLjYwNyAwLjM3ODkwNkwyNTguNTc4IDU0Ljk1MjZIMzc5LjU2N0w0MzIuMzM5IDAuMzc4OTA2SDMxMS42MDdaIiBmaWxsPSIjMEIwQTBBIi8+CjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMzA5LjU0NyA4OS45NjAxTDI1Ni41MTggMTQ0LjI3NkgzNzcuNTA2TDQzMC4wMjEgODkuNzAyNkgzMDkuNTQ3Vjg5Ljk2MDFaIiBmaWxsPSIjMEIwQTBBIi8+CjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMjQyLjg3MyAxNjQuNjZMMTg5Ljg0NCAyMTkuMjM0SDMxMC44MzNMMzYzLjM0NyAxNjQuNjZIMjQyLjg3M1oiIGZpbGw9IiMwQjBBMEEiLz4KPC9zdmc+Cg==)](https://codeflash.ai)",Human,148906541,codeflash-ai[bot],closed,2025-05-16 11:24:40+00:00,2025-05-16 11:35:05+00:00,,,https://api.github.com/repos/mangiucugna/json_repair,https://github.com/mangiucugna/json_repair/pull/114,perf,"The PR introduces an optimization that speeds up the existing method `JSONParser.parse_number` by 17% without changing its functionality, which is a performance improvement.","The PR introduces an optimization that speeds up the existing method `JSONParser.parse_number` by 17% without changing its functionality, which is a performance improvement.",Human,,,,,,Python,False,True,False,0.1736111111111111,,0.0072337962962962,,51,7,13358.0,1458.0,True,"The code implements the parse_number method in a JSON parser, which reads a numeric literal (integer or float, with possible exponents) from a JSON text input. It advances through the string, gathers valid numeric characters, converts them to their appropriate Python type (int or float), and handles various edge cases (like embedded commas, invalid trailing chars, etc). Helper logic like get_char_at is used to look at characters by index efficiently.","The optimization focuses on improving hot loops inside parse_number by: 1) Collecting characters into a list and joining at the end, instead of continual (O(n^2)) string concatenation; 2) Avoiding the unnecessary creation of sets for constant small membership checks, instead using a string and 'in' checks (which are optimized in Python for small cases); 3) Caching method references (get_char_at) to a local variable to avoid repeated costly attribute access in a tight loop. These changes reduce both time complexity for frequent operations and repeated unnecessary resource use, resulting in a documented 17% performance speedup.",Algorithm-Level Optimizations,Select Computationally Efficient Algorithms,high,True,,4858
2524313861,115,âš¡ï¸ Speed up method `JSONParser.parse_comment` by 29%,"### ğŸ“„ 29% (0.29x) speedup for ***`JSONParser.parse_comment` in `src/json_repair/json_parser.py`***

â±ï¸ Runtime :   **`9.00 microseconds`**  **â†’** **`7.00 microseconds`** (best of `47` runs)
### ğŸ“ Explanation and details

Here's an optimized version of your `JSONParser` program, targeting hot-spots in `parse_comment` and reducing unnecessary work and function calls. The `get_char_at()` calls are now minimized inside tight loops for comment scanning, and the logic around accumulating `termination_characters` is more efficient. All semantics are preserved.



### Optimization Notes

- **Loop minimization**: Instead of repeatedly calling `get_char_at()` for each character in a comment (inefficient for long comments), tight loops work directly on the underlying string, updating the index variable.
- **String slicing**: Builds the comment with one slice instead of repeated string concatenation.
- **Logging unchanged**: Any logging/side-effects remain at exactly the same logical points.
- **No unnecessary dictionary lookups**: The context object is checked only once per function call.
- **Branch ordering**: Strips some redundancy and code path splits for improved readability and performance.
- **No change to function signatures or observable input/output**.

If you have further line profiling data, deeper optimization can be tailored!


âœ… **Correctness verification report:**

| Test                        | Status            |
| --------------------------- | ----------------- |
| âš™ï¸ Existing Unit Tests | ğŸ”˜ **None Found** |
| ğŸŒ€ Generated Regression Tests | ğŸ”˜ **None Found** |
| âª Replay Tests | ğŸ”˜ **None Found** |
| ğŸ” Concolic Coverage Tests | âœ… **10 Passed** |
|ğŸ“Š Tests Coverage       | 93.0% |

To edit these changes `git checkout codeflash/optimize-JSONParser.parse_comment-maqpwq0a` and push.

[![Codeflash](https://img.shields.io/badge/Optimized%20with-Codeflash-yellow?style=flat&color=%23ffc428&logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iNDgwIiBoZWlnaHQ9ImF1dG8iIHZpZXdCb3g9IjAgMCA0ODAgMjgwIiBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgo8cGF0aCBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTI4Ni43IDAuMzc4NDE4SDIwMS43NTFMNTAuOTAxIDE0OC45MTFIMTM1Ljg1MUwwLjk2MDkzOCAyODEuOTk5SDk1LjQzNTJMMjgyLjMyNCA4OS45NjE2SDE5Ni4zNDVMMjg2LjcgMC4zNzg0MThaIiBmaWxsPSIjRkZDMDQzIi8+CjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMzExLjYwNyAwLjM3ODkwNkwyNTguNTc4IDU0Ljk1MjZIMzc5LjU2N0w0MzIuMzM5IDAuMzc4OTA2SDMxMS42MDdaIiBmaWxsPSIjMEIwQTBBIi8+CjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMzA5LjU0NyA4OS45NjAxTDI1Ni41MTggMTQ0LjI3NkgzNzcuNTA2TDQzMC4wMjEgODkuNzAyNkgzMDkuNTQ3Vjg5Ljk2MDFaIiBmaWxsPSIjMEIwQTBBIi8+CjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMjQyLjg3MyAxNjQuNjZMMTg5Ljg0NCAyMTkuMjM0SDMxMC44MzNMMzYzLjM0NyAxNjQuNjZIMjQyLjg3M1oiIGZpbGw9IiMwQjBBMEEiLz4KPC9zdmc+Cg==)](https://codeflash.ai)",Human,148906541,codeflash-ai[bot],closed,2025-05-16 11:31:16+00:00,2025-05-16 11:35:59+00:00,,,https://api.github.com/repos/mangiucugna/json_repair,https://github.com/mangiucugna/json_repair/pull/115,perf,"The PR title and body describe an optimization that improves the performance of an existing method without changing its functionality, which classifies as a performance improvement.","The PR title and body describe an optimization that improves the performance of an existing method without changing its functionality, which classifies as a performance improvement.",Human,,,,,,Python,False,True,False,0.0786111111111111,,0.0032754629629629,,52,7,2936.0,249.0,True,"The `JSONParser.parse_comment` method processes and extracts comments found within JSON-like input strings, likely for the purposes of parsing or repairing non-standard JSON. It scans the input to identify and accumulate comment text, skipping it or storing it as needed depending on parsing goals.","The original method relied heavily on repeatedly calling a helper method (`get_char_at()`) within a loop to access characters in the comment, and used repeated string concatenation to assemble the comment value. The optimized version works directly with the underlying string and manipulates the index variable, minimizing function calls inside loops. String building is performed via slicing, reducing the time and memory overhead of incremental concatenation. Additionally, redundant dictionary/context lookups and unnecessary conditional checks have been reduced or eliminated. Branching logic is slightly reordered for clarity and efficiency without changing program semantics.",Loop Transformations,Remove Conditional by Loop Unrolling,high,True,,2173
2469218203,4192,perf(weave): address data loading perf issue on eval compare,"## Description

<!--
Include reference to internal ticket ""Fixes WB-NNNNN"" and/or GitHub issue ""Fixes #NNNN"" (if applicable)
-->

This addresses a part of the evaluation comparison performance issue. No the customer scenario will no longer crash. 

UI wise, there is no noticeable behavior change.

Here I attach [a video(internal)](https://www.loom.com/share/e07ee35dcd464094bc06aebf97fb8962?sid=691778a9-6156-4b2f-884a-095f3a78d29c) to explain the change.

## Testing

This PR is manually tested against the customer scenario and locally. 
",Human,202173503,chance-wnb,closed,2025-04-19 00:27:50+00:00,2025-04-22 18:11:08+00:00,2025-04-22 18:11:08+00:00,,https://api.github.com/repos/wandb/weave,https://github.com/wandb/weave/pull/4192,perf,The PR title and description indicate that the change improves the performance of data loading during evaluation comparison without adding new features or fixing bugs.,The PR title and description indicate that the change improves the performance of data loading during evaluation comparison without adding new features or fixing bugs.,Human,,,,,,Python,True,True,False,89.72166666666666,89.72166666666666,3.7384027777777775,3.7384027777777775,60,9,557.0,66.0,True,"The code in question is part of an evaluation comparison feature in the weave module, focusing on loading and displaying data for side-by-side comparisons. The commit addresses performance issues that were causing crashes for customer scenarios where large datasets or slow-loading processes caused the UI to become unresponsive or break. The fix improves data loading to handle such scenarios robustly without altering visible UI behavior.","Compared to the original version, the optimized code introduces changes aimed at eliminating performance bottlenecks during data loading for evaluation comparisons. This likely includes algorithmic improvements such as more efficient data fetching, possibly batching, debouncing, or lazy-loading items instead of loading everything at once. No significant algorithm is replaced, but the code may now avoid redundant or repeated data fetches, and likely improves memory usage and UI responsiveness. Additional improvements could involve eliminating unnecessary blocking calls or restructuring control flow to avoid processing already-loaded or irrelevant data records. There is no evidence of new algorithms, changes to data structures, or rewrites of core logic, but rather improved handling of how and when data requests are made.",Algorithm-Level Optimizations,Select Computationally Efficient Algorithms,high,True,,1276
2358030784,4127,slight improvements to user group endpoints,"## Description

Customer was having long load times from user-group and persona endpoints; these changes improve performance by about ~30% in some cases (worst I saw was equivalent performance to the non-eager loading version). 

## How Has This Been Tested?

This was tested with about 10,000 users, 100 user groups, and 500 personas. 

## Backporting (check the box to trigger backport action)

Note: You have to check that the action passes, otherwise resolve the conflicts manually and tag the patches.

- [ ] This PR should be backported (make sure to check that the backport attempt succeeds)
- [x] [Optional] Override Linear Check
",Human,183385614,evan-onyx,open,2025-02-26 01:05:19+00:00,,,,https://api.github.com/repos/onyx-dot-app/onyx,https://github.com/onyx-dot-app/onyx/pull/4127,perf,"The PR description explicitly states that the changes improve performance by about 30%, indicating that the primary purpose of the change is to enhance performance rather than add features or fix bugs.","The PR description explicitly states that the changes improve performance by about 30%, indicating that the primary purpose of the change is to enhance performance rather than add features or fix bugs.",Human,,,,,,Python,False,False,True,,,,,43,6,652.0,103.0,True,"The code modifies the backend endpoints responsible for handling user-group and persona data retrieval, focusing on reducing their response times. The likely bottleneck addressed is the latency caused by inefficient data access patterns when fetching large numbers of users and groups, potentially involving repeated database queries (the N+1 select problem). The modified code aims to retrieve the required user-group and persona information more efficiently for high user and group counts.","The original code likely loaded user-group and persona data in a non-eager fashion; for each user or group, the code may have triggered additional database queries (N+1 query pattern), causing significant delays at larger scales. The optimized version introduces eager loading: it fetches all the necessary related data in a single query (with proper join statements or ORM prefetching), minimizing round-trips to the database. This reduces overhead, lowers total query count, improves time complexity related to I/O, and enhances throughput for large datasets. No algorithmic redesign happened, but a significant change in data-fetching strategy and data locality was applied. Redundant queries or code paths have likely been eliminated as a result.",Memory and Data Locality Optimizations,Increase Cache Efficiency via Locality,high,True,,1291
2517537659,752,opt.: appbar `scrolledUnderElevation`,"Fixes #751

## Summary by Sourcery

Update project dependencies for improved compatibility and performance

Enhancements:
- Replace the code_text_field package with re_editor
- Bump fl_lib git reference from v1.0.281 to v1.0.283",Human,10864310,lollipopkit,closed,2025-05-13 20:02:08+00:00,2025-05-13 20:02:30+00:00,2025-05-13 20:02:30+00:00,,https://api.github.com/repos/lollipopkit/flutter_server_box,https://github.com/lollipopkit/flutter_server_box/pull/752,perf,"The PR updates project dependencies and replaces a package to improve compatibility and performance, which is a change that improves performance rather than adding a new feature or fixing a bug.","The PR updates project dependencies and replaces a package to improve compatibility and performance, which is a change that improves performance rather than adding a new feature or fixing a bug.",Human,,,,,,Dart,True,True,False,0.0061111111111111,0.0061111111111111,0.0002546296296296,0.0002546296296296,37,3,236.0,31.0,True,"The code in this commit updates project dependencies to enhance compatibility and performance. Specifically, it replaces the 'code_text_field' package with 're_editor', and updates the 'fl_lib' dependency to a newer version. These changes aim to improve the foundations on which the app builds, likely addressing issues, minimizing bugs, and potentially improving runtime behavior depending on the new dependencies' performance.","The main optimization occurs through dependency substitution and upgrading: (1) The 'code_text_field' package is swapped out for the 're_editor' package, likely because the new package offers better performance, improved features, or bug fixes. (2) The 'fl_lib' dependency is updated to a newer commit, obtaining upstream optimizations and fixes. There are no algorithmic or structural changes to the codebase itself, but improvements may be realized due to more efficient or compatible external libraries. No explicit algorithm, data structure, or code-level redundant logic changes are apparent, as optimizations result from external package improvements rather than internal logic refactoring.",Data Structure Selection and Adaptation,Darwinian Selection,high,True,,1170
2369238232,74,Dramatically increase OB speed,,Human,32915015,joshyattridge,closed,2025-03-03 22:27:52+00:00,2025-03-03 22:29:38+00:00,2025-03-03 22:29:38+00:00,,https://api.github.com/repos/joshyattridge/smart-money-concepts,https://github.com/joshyattridge/smart-money-concepts/pull/74,perf,"The title indicates a significant improvement in speed, which corresponds to a performance enhancement.","The title indicates a significant improvement in speed, which corresponds to a performance enhancement.",Human,,,,,,Python,True,True,False,0.0294444444444444,0.0294444444444444,0.0012268518518518,0.0012268518518518,30,4,,,False,"The code is responsible for processing operations related to order books (OB), a core component in trading software systems where bid and ask orders are matched. It manages, updates, or queries the state of the order book, possibly parsing, inserting, matching, or removing orders as required for market operations.","The optimized commit dramatically increases the speed of order book operations. The changes likely include replacing less efficient algorithms with more optimal alternativesâ€”perhaps switching from a linear search or naive updating logic to a hash map or a tree structure for faster lookup, or introducing bulk/batched operations instead of per-order processing. Redundant checks or unnecessary method calls may have been removed. There may be improved memory access patterns (such as using contiguous data structures), streamlined control flow, or elimination of bottlenecks identified in performance profiling. The commit probably also contains some stylistic or structural refactoring to make critical-path logic faster or more readable.",Algorithm-Level Optimizations,Select Computationally Efficient Algorithms,medium,True,,1082
2369253951,75,Increased the speed to calculate the liquidity function,,Human,32915015,joshyattridge,closed,2025-03-03 22:39:31+00:00,2025-03-03 22:40:28+00:00,2025-03-03 22:40:28+00:00,,https://api.github.com/repos/joshyattridge/smart-money-concepts,https://github.com/joshyattridge/smart-money-concepts/pull/75,perf,"The change improves the performance of the liquidity function calculation, which is a performance enhancement.","The change improves the performance of the liquidity function calculation, which is a performance enhancement.",Human,,,,,,Python,True,True,False,0.0158333333333333,0.0158333333333333,0.0006597222222222,0.0006597222222222,55,8,,,False,"The code calculates a liquidity function, likely determining available liquidity or some related metric in a financial or trading domain. It aggregates or computes values based on inputs such as order books, pool reserves, or similar data structures.","In the optimized version, the calculation logic for liquidity has been streamlined to reduce computational overhead. For example, the new code may use a more direct mathematical formula or consolidate multiple loops into a single pass, reducing the number of iterations or conditional checks. Any redundant intermediate variable assignments, checks, or unnecessary function calls involved in the original calculation are eliminated. This results in lower runtime cost and possibly lower space usage. No structural changes to data flow or architectural composition are evidentâ€”improvements are focused on making the algorithm behind the liquidity calculation faster and less computationally intensive.",Algorithm-Level Optimizations,Select Computationally Efficient Algorithms,high,True,,1074
2354104157,193,Memory optimisations - remove the oninput/-value wrapping,,Human,239357,jvanoostveen,closed,2025-02-24 15:56:10+00:00,2025-05-08 11:07:43+00:00,2025-05-08 11:07:43+00:00,,https://api.github.com/repos/AFASSoftware/maquette,https://github.com/AFASSoftware/maquette/pull/193,perf,"The change is described as a memory optimization, which implies an improvement in performance rather than a bug fix or new feature.","The change is described as a memory optimization, which implies an improvement in performance rather than a bug fix or new feature.",Human,,,,,,TypeScript,True,True,False,1747.1925,1747.1925,72.7996875,72.7996875,57,7,,,False,"The code handles input events and value changes for UI elements, likely processing user input in forms or data fields. Previously, it wrapped the handling logic for 'oninput' and related value changes with extra wrapper functions or intermediary layers.","Originally, the 'oninput' event and value management were encapsulated in additional layers, possibly to abstract away browser quirks or to generalize input handling. The optimization removes these wrappers, allowing the input events and values to be handled directly, thus reducing memory allocations and function call overhead. This change reduces indirection and can lead to improved runtime efficiency by eliminating unnecessary closures or intermediate data structures. Additionally, it simplifies the code by removing redundant method calls or wrappers that do not provide essential functionality in the current context.",Code Smells and Structural Simplification,Remove Redundant Method Calls,high,True,,1074
2617294066,41033,chore: ce changes related to decoupling webworker,"## Description
We are improving the LCP by reducing the time to reach the first evaluation, aiming for a 1.8 to 2.2 second reduction. To achieve this, weâ€™ve implemented the following changes:

Code Splitting of Widgets: During page load, only the widgets required for the initial evaluation are loaded and registered. The remaining widgets are registered after the first evaluation message is sent. This parallelizes widget loading with evaluation computation, reducing the critical path.

Web Worker Offloading: Macro tasks such as clearCache and JavaScript library installation have been moved to the web worker setup. These are now executed in a separate thread, allowing the firstUnevaluatedTree to be computed in parallel with JS library installation.

Parallel JS Library Loading: All JavaScript libraries are now loaded in parallel within the web worker, instead of sequentially, improving efficiency.

Deferred Rendering of AppViewer: We now render the AppViewer component only after registering the remaining widgets. This ensures that heavy rendering tasksâ€”such as expensive selector computations and loading additional chunks related to the AppViewerâ€”can execute in parallel with the first evaluation, further enhancing performance.

## Automation

/ok-to-test tags=""@tag.All""

### :mag: Cypress test results
<!-- This is an auto-generated comment: Cypress test results  -->
> [!CAUTION]
> ğŸ”´ ğŸ”´ ğŸ”´ Some tests have failed.
> Workflow run: <https://github.com/appsmithorg/appsmith/actions/runs/15894953337>
> Commit: 2dc9dbcd6b60cb63ec954713dbf7335d788df9a4
> <a href=""https://internal.appsmith.com/app/cypress-dashboard/rundetails-65890b3c81d7400d08fa9ee5?branch=master&workflowId=15894953337&attempt=1&selectiontype=test&testsstatus=failed&specsstatus=fail"" target=""_blank"">Cypress dashboard</a>.
> Tags: @tag.All
> Spec: 
> The following are new failures, please fix them before merging the PR: <ol>
> <li>cypress/e2e/Regression/ClientSide/OtherUIFeatures/Analytics_spec.js</ol>
> <a href=""https://internal.appsmith.com/app/cypress-dashboard/identified-flaky-tests-65890b3c81d7400d08fa9ee3?branch=master"" target=""_blank"">List of identified flaky tests</a>.
> <hr>Thu, 26 Jun 2025 07:57:26 UTC
<!-- end of auto-generated comment: Cypress test results  -->


## Communication
Should the DevRel and Marketing teams inform users about this change?
- [ ] Yes
- [ ] No


<!-- This is an auto-generated comment: release notes by coderabbit.ai -->
## Summary by CodeRabbit

- **New Features**
  - Added support for deferred loading of JavaScript libraries and improved control over page rendering and first page load behavior.
  - Introduced granular widget registration, allowing partial widget initialization for faster initial rendering.
  - Added new Redux actions and selectors to manage and track evaluation and rendering state.
  - Added explicit cache clearing for widget factory memoization functions.

- **Improvements**
  - Refactored widget loading to be asynchronous and on-demand, reducing initial load time and improving modularity.
  - Enhanced sagas and reducers to better handle first-time evaluations and widget registration.
  - Optimized JS library loading to occur in parallel for improved performance.
  - Modularized theme application and improved conditional rendering in the App Viewer.
  - Reorganized widget registration to initialize widgets individually rather than in bulk.
  - Improved memoization decorator to allow explicit cache clearing globally.
  - Updated evaluation sagas to support partial widget initialization and deferred JS library loading.
  - Updated widget loading utilities and tests to support asynchronous dynamic loading.

- **Bug Fixes**
  - Improved conditional logic to prevent errors when rendering components with missing functions.

- **Tests**
  - Expanded and refactored test suites to cover asynchronous widget loading, partial initialization, and evaluation saga behaviors.
  - Added tests verifying widget factory cache behavior and first evaluation integration.

- **Chores**
  - Updated imports and code structure for clarity and maintainability.
  - Reorganized type imports and moved interface declarations to dedicated modules.
<!-- end of auto-generated comment: release notes by coderabbit.ai -->",Human,121419957,vsvamsi1,open,2025-06-25 07:45:14+00:00,,,,https://api.github.com/repos/appsmithorg/appsmith,https://github.com/appsmithorg/appsmith/pull/41033,perf,"The PR primarily focuses on performance improvements by decoupling web worker tasks, parallelizing widget loading, and deferring rendering to reduce load times and improve efficiency. These changes enhance the performance of the application without adding new features or fixing bugs.","The PR primarily focuses on performance improvements by decoupling web worker tasks, parallelizing widget loading, and deferring rendering to reduce load times and improve efficiency. These changes enhance the performance of the application without adding new features or fixing bugs.",Human,,,,,,TypeScript,False,False,True,,,,,49,7,4334.0,545.0,True,"The code improves the performance of a web application's initial rendering phase. It does so by decoupling widget loading and registration, moving resource-intensive setup tasks (like JavaScript library installation and cache clearing) off the main thread into a web worker, and deferring expensive rendering operations until after critical computations are complete. This parallelization reduces the time-to-first-evaluation and perceived load time, aiming for a significant time reduction.","Compared to the original version, the optimized code: (1) Splits widget code such that only critical widgets are loaded upfront while the rest are loaded after the initial evaluation, reducing synchronous blocking during the page load; (2) Moves macro tasks (like cache clearing and JS library installation) into a web worker, freeing the main thread to compute the evaluation tree in parallel; (3) Loads all JavaScript libraries in parallel instead of sequentially, cutting down initialization time; (4) Defers expensive AppViewer rendering until after remaining widgets are registered, thus parallelizing intensive rendering with ongoing evaluation. Algorithmically, the logic shifts from synchronous, sequential operations to an orchestrated, parallel and deferred execution model. Redundant or blocking control flows (such as bulk widget registration and sequential library installation) are removed/rewritten for asynchronicity and concurrency.",Algorithm-Level Optimizations,Select Computationally Efficient Algorithms,high,True,,2094
2398828721,39757,chore: remove analytics execution from the critical path,"## Description

- Pushed out the sendExecuteAnalyticsEvent from the critical path of returning action's execution result.
- Improved the critical Path of sendExecuteAnalyticsEvent by running the application mono concurrent to other events.
- Added more telemetry code around the execution flow.


Fixes #`Issue Number`  
_or_  
Fixes `Issue URL`
> [!WARNING]  
> _If no issue exists, please create an issue first, and check with the maintainers if the issue is valid._

## Automation

/ok-to-test tags=""@tag.All""

### :mag: Cypress test results
<!-- This is an auto-generated comment: Cypress test results  -->
> [!TIP]
> ğŸŸ¢ ğŸŸ¢ ğŸŸ¢ All cypress tests have passed! ğŸ‰ ğŸ‰ ğŸ‰
> Workflow run: <https://github.com/appsmithorg/appsmith/actions/runs/13919689126>
> Commit: ddf93dd06cd4facabdde5898d1cc40ce7dc4765f
> <a href=""https://internal.appsmith.com/app/cypress-dashboard/rundetails-65890b3c81d7400d08fa9ee5?branch=master&workflowId=13919689126&attempt=1"" target=""_blank"">Cypress dashboard</a>.
> Tags: `@tag.All`
> Spec:
> <hr>Tue, 18 Mar 2025 10:28:52 UTC
<!-- end of auto-generated comment: Cypress test results  -->


## Communication
Should the DevRel and Marketing teams inform users about this change?
- [ ] Yes
- [ ] No


<!-- This is an auto-generated comment: release notes by coderabbit.ai -->

## Summary by CodeRabbit

- **New Features**
	- Introduced additional action tracking identifiers to support enhanced analytics and authentication validation.
- **Refactor**
	- Optimized asynchronous operations for data retrieval to improve responsiveness.
	- Enhanced the flow and error handling of action execution, ensuring smoother and more reliable performance.

<!-- end of auto-generated comment: release notes by coderabbit.ai -->",Human,121419957,vsvamsi1,closed,2025-03-17 19:12:46+00:00,2025-03-18 11:51:51+00:00,2025-03-18 11:51:51+00:00,,https://api.github.com/repos/appsmithorg/appsmith,https://github.com/appsmithorg/appsmith/pull/39757,perf,"The changes focus on improving the execution flow by removing analytics from the critical path and optimizing asynchronous operations, which enhances performance without adding new features or fixing bugs.","The changes focus on improving the execution flow by removing analytics from the critical path and optimizing asynchronous operations, which enhances performance without adding new features or fixing bugs.",Human,,,,,,TypeScript,True,True,False,16.65138888888889,16.65138888888889,0.6938078703703704,0.6938078703703704,56,8,1780.0,226.0,True,"The code manages action execution within an application and previously included analytics event reporting directly on the action's critical execution path. This meant analytics reporting could add latency to the response delivered to users. The update refactors this so that the core action execution result is returned immediately, and the analytics event reporting happens asynchronously (off the critical path). Additional telemetry code is added to better monitor application flow and execution.","Originally, the critical execution path included a synchronous (blocking) call to send analytics data, which risked delaying the user's receipt of the execution result due to analytics network or processing latencies. The optimized version moves the sendExecuteAnalyticsEvent call to run outside the critical path, likely in another thread or asynchronously, so the user receives results faster and analytics are reported in parallel. This reduces blocking wait time and improves responsiveness. The optimization also adds more telemetry for better observability. There are no major algorithmic changes, but significant code restructuring occurs to decouple analytics and main execution, and redundant, time-wasting blocking in the path is eliminated.",Control-Flow and Branching Optimizations,Remove Branches by Doing Extra Work,high,True,,1577
2336649960,31868,[IBD] specialize block serialization,"This change is part of [[IBD] - Tracking PR for speeding up Initial Block Download](https://github.com/bitcoin/bitcoin/pull/32043)

---

This PR is drafted until I remeasure everything after the recent merges and I need to find a way to simplify the 1 byte writes more nicely, I don't like all the specializations.

---

### Summary

This PR contain a few different optimization I found by IBD profiling, and via the newly added block seralization benchmarks. It also takes advantage of the recently merged [`std::span` changes](https://github.com/bitcoin/bitcoin/pull/31519) enabling propagating static extents.

The commits merge similar (de)serialization methods, and separates them internally with  `if constexpr` - similarly to how it has been [done here before](https://github.com/bitcoin/bitcoin/pull/28203). This enabled further `SizeComputer` optimizations as well.

### Context
Other than these, since single byte writes are used very often (used for every `(u)int8_t` or `std::byte` or `bool` and for every `VarInt`'s first byte which is also needed for every `(pre)Vector`), it makes sense to avoid the generalized serialization infrastructure that isn't needed:
* `AutoFile` write doesn't need to allocate 4k buffer for a single byte now;
* `VectorWriter` and `DataStream` avoids memcpy/insert calls;
* `CSHA256::Write` can avoid `memcpy`.

`DeserializeBlock` is dominated by the hash calculations so the optimizations barely affect it.

### Measurements

<details>
<summary>C compiler ............................ AppleClang 16.0.0.16000026</summary>

> Before:

|            ns/block |             block/s |    err% |     total | benchmark
|--------------------:|--------------------:|--------:|----------:|:----------
|          195,610.62 |            5,112.20 |    0.3% |     11.00 | `SerializeBlock`
|           12,061.83 |           82,906.19 |    0.1% |     11.01 | `SizeComputerBlock`

> After:

|            ns/block |             block/s |    err% |     total | benchmark
|--------------------:|--------------------:|--------:|----------:|:----------
|          174,569.19 |            5,728.39 |    0.6% |     10.89 | `SerializeBlock`
|           10,241.16 |           97,645.21 |    0.0% |     11.00 | `SizeComputerBlock`

</details>

> `SerializeBlock` - ~12.% faster
> `SizeComputerBlock` - ~17.7% faster

-----


<details>
<summary>C++ compiler .......................... GNU 13.3.0</summary>

> Before:

|            ns/block |             block/s |    err% |       ins/block |       cyc/block |    IPC |      bra/block |   miss% |     total | benchmark
|--------------------:|--------------------:|--------:|----------------:|----------------:|-------:|---------------:|--------:|----------:|:----------
|          867,857.55 |            1,152.26 |    0.0% |    8,015,883.90 |    3,116,099.08 |  2.572 |   1,517,035.87 |    0.5% |     10.81 | `SerializeBlock`
|           30,928.27 |           32,332.88 |    0.0% |      221,683.03 |      111,055.84 |  1.996 |      53,037.03 |    0.8% |     11.03 | `SizeComputerBlock`

> After:

|            ns/block |             block/s |    err% |       ins/block |       cyc/block |    IPC |      bra/block |   miss% |     total | benchmark
|--------------------:|--------------------:|--------:|----------------:|----------------:|-------:|---------------:|--------:|----------:|:----------
|          615,000.56 |            1,626.01 |    0.0% |    8,015,883.64 |    2,208,340.88 |  3.630 |   1,517,035.62 |    0.5% |     10.56 | `SerializeBlock`
|           25,676.76 |           38,945.72 |    0.0% |      159,390.03 |       92,202.10 |  1.729 |      42,131.03 |    0.9% |     11.00 | `SizeComputerBlock`

</details>

> `SerializeBlock` - ~41.1% faster
> `SizeComputerBlock` - ~20.4% faster

----

While this wasn't the main motivation for the change, IBD on Ubuntu/GCC on SSD with i9 indicates a 2% speedup as well:

<details>
<summary>Details</summary>

```bash
COMMITS=""05314bde0b06b820225f10c6529b5afae128ff81 1cd94ec2511874ec68b92db34ad7ec7d9534fed1""; \
STOP_HEIGHT=880000; DBCACHE=10000; \
C_COMPILER=gcc; CXX_COMPILER=g++; \
hyperfine \
--export-json ""/mnt/my_storage/ibd-${COMMITS// /-}-${STOP_HEIGHT}-${DBCACHE}-${C_COMPILER}.json"" \
--runs 3 \
--parameter-list COMMIT ${COMMITS// /,} \
--prepare ""killall bitcoind || true; rm -rf /mnt/my_storage/BitcoinData/*; git checkout {COMMIT}; git clean -fxd; git reset --hard; cmake -B build -DCMAKE_BUILD_TYPE=Release -DENABLE_WALLET=OFF -DCMAKE_C_COMPILER=$C_COMPILER -DCMAKE_CXX_COMPILER=$CXX_COMPILER && cmake --build build -j$(nproc) --target bitcoind && ./build/bin/bitcoind -datadir=/mnt/my_storage/BitcoinData -stopatheight=1 -printtoconsole=0 || true"" \
--cleanup ""cp /mnt/my_storage/BitcoinData/debug.log /mnt/my_storage/logs/debug-{COMMIT}-$(date +%s).log || true"" \
""COMPILER=$C_COMPILER COMMIT={COMMIT} ./build/bin/bitcoind -datadir=/mnt/my_storage/BitcoinData -stopatheight=$STOP_HEIGHT -dbcache=$DBCACHE -prune=550 -printtoconsole=0""
Benchmark 1: COMPILER=gcc COMMIT=05314bde0b06b820225f10c6529b5afae128ff81 ./build/bin/bitcoind -datadir=/mnt/my_storage/BitcoinData -stopatheight=880000 -dbcache=10000 -prune=550 -printtoconsole=0
  Time (mean Â± Ïƒ):     33647.918 s Â± 508.655 s    [User: 71503.409 s, System: 4404.899 s]
  Range (min â€¦ max):   33283.439 s â€¦ 34229.026 s    3 runs
 
Benchmark 2: COMPILER=gcc COMMIT=1cd94ec2511874ec68b92db34ad7ec7d9534fed1 ./build/bin/bitcoind -datadir=/mnt/my_storage/BitcoinData -stopatheight=880000 -dbcache=10000 -prune=550 -printtoconsole=0
  Time (mean Â± Ïƒ):     33062.491 s Â± 183.335 s    [User: 71246.532 s, System: 4318.490 s]
  Range (min â€¦ max):   32888.211 s â€¦ 33253.706 s    3 runs
 
Summary
  COMPILER=gcc COMMIT=1cd94ec2511874ec68b92db34ad7ec7d9534fed1 ./build/bin/bitcoind -datadir=/mnt/my_storage/BitcoinData -stopatheight=880000 -dbcache=10000 -prune=550 -printtoconsole=0 ran
    1.02 Â± 0.02 times faster than COMPILER=gcc COMMIT=05314bde0b06b820225f10c6529b5afae128ff81 ./build/bin/bitcoind -datadir=/mnt/my_storage/BitcoinData -stopatheight=880000 -dbcache=10000 -prune=550 -printtoconsole=0
```

</details>",Human,1841944,l0rinc,open,2025-02-14 16:48:23+00:00,,,,https://api.github.com/repos/bitcoin/bitcoin,https://github.com/bitcoin/bitcoin/pull/31868,perf,"The PR introduces optimizations to block serialization that improve performance significantly, as demonstrated by the benchmark results. The changes focus on speeding up serialization and size computation, which are performance improvements rather than bug fixes or new features.","The PR introduces optimizations to block serialization that improve performance significantly, as demonstrated by the benchmark results. The changes focus on speeding up serialization and size computation, which are performance improvements rather than bug fixes or new features.",Human,,,,,,C++,False,False,True,,,,,36,4,6205.0,632.0,True,"This code manages the serialization and deserialization of blocks within Bitcoin Core, specifically streamlining the process for writing and reading data (including single-byte values and block structures) over different I/O streams. This is crucial during Initial Block Download (IBD), where very large numbers of blocks must be efficiently serialized and transmitted. The changes aim to make block serialization faster and more efficient, particularly by making use of recently introduced improvements like std::span with static extents and specializing write paths for common cases such as single-byte writes.","Previously, the code relied on general-purpose serialization infrastructure, which involved mechanisms like allocating large buffers (e.g., a 4k buffer to serialize even a single byte), making unnecessary memory allocations, copying, and inserting data through generic code paths (like VectorWriter, DataStream, and CSHA256::Write memcpys). The optimized code specializes for frequent cases (such as single-byte writes for uint8_t/bool/VarInt headers) by avoiding the heavy generalized paths when not needed. It leverages compile-time branching (if constexpr) to select optimized serialization/deserialization logic for the relevant types, and unifies/de-duplicates logic where possible. The impact on performance is significant, with microbenchmarks reporting 12-41% speedup on serialization routines and a measured 2% improvement in total IBD times on practical hardware.",Algorithm-Level Optimizations,Select Computationally Efficient Algorithms,high,True,,3159
2336988355,4336,Minor changes to specific declarations in animation.lua,"### Work done
I made some slight optimizations for some of the variable declarations in the functions of animation.lua. reduced the number of variables being declared in the beginning of every function, while also eliminating some variables that are being redeclared in infinite loops, leaving only initializations.",Human,197926377,codecanal,closed,2025-02-14 20:18:39+00:00,2025-02-15 17:33:11+00:00,,,https://api.github.com/repos/beyond-all-reason/Beyond-All-Reason,https://github.com/beyond-all-reason/Beyond-All-Reason/pull/4336,perf,"The changes involve slight optimizations and improvements in variable declarations to reduce redundancy and improve efficiency, which aligns with performance improvements rather than fixing bugs or adding features.","The changes involve slight optimizations and improvements in variable declarations to reduce redundancy and improve efficiency, which aligns with performance improvements rather than fixing bugs or adding features.",Human,,,,,,Lua,False,True,False,21.24222222222222,,0.8850925925925925,,55,7,316.0,47.0,True,"The code in animation.lua handles animation logic, likely within a game or graphical application. The functions within manipulate and update animations, with variables managing state, frames, or time steps to perform smooth transitions.","In the optimized version, variable declarations were reduced at the beginning of functions by declaring only what's necessary. Additionally, redundant variable redeclarations inside infinite loops were removed, and initializations were left outside the loops. This change eliminates unnecessary memory allocation and repeated variable creation during execution, streamlining the code and possibly improving runtime efficiency, especially in tight loop scenarios.",Code Smells and Structural Simplification,Remove Redundant Method Calls,high,True,,1092
2590261382,251382,Optimized concat with reduce,"<!-- Thank you for submitting a Pull Request. Please:
* Read our Pull Request guidelines:
  https://github.com/microsoft/vscode/wiki/How-to-Contribute#pull-requests
* Associate an issue with the Pull Request.
* Ensure that the code is up-to-date with the `main` branch.
* Include a description of the proposed changes and how to test them.
-->
The `concat` method creates a new collection each time based on the existing one. Using `push` in this case saves resources.",Human,4091305,Connormiha,open,2025-06-13 13:20:06+00:00,,,,https://api.github.com/repos/microsoft/vscode,https://github.com/microsoft/vscode/pull/251382,perf,"The change improves the performance of the code by optimizing the concat operation to use push, which saves resources and avoids creating new collections repeatedly.","The change improves the performance of the code by optimizing the concat operation to use push, which saves resources and avoids creating new collections repeatedly.",Human,,,,,,TypeScript,False,False,True,,,,,28,4,475.0,70.0,True,"The code in question concatenates multiple lists or collections together. The original implementation likely used the 'concat' method, which creates a new array for every concatenation, whereas the optimized version uses 'reduce' with 'push' to accumulate values into a single result array, minimizing array allocations.","Previously, every call to 'concat' produced a new array object, leading to repeated allocations and increased workload for the garbage collector, especially when concatenating many arrays in sequence. The optimized code uses a single array, and uses 'push' to append elements in-place via a reducer pattern. This reduces memory usage and improves runtime performance by minimizing the number of intermediate array objects and copy operations.",Algorithm-Level Optimizations,Select Computationally Efficient Algorithms,high,True,,1145
2269709704,218,"Speed up, reduce memory usage of file reading","Our FS abstraction works with strings; rather than reading the file into `[]byte` then copying it into a string, we can read the file directly into a correctly-sized `strings.Builder`.

Also, now that we have a string, we can instead check for BOMs with strings, which is faster as the Go compiler optimizes those reads/comparisons.",Human,5341706,jakebailey,closed,2025-01-10 05:04:13+00:00,2025-01-10 21:22:00+00:00,,,https://api.github.com/repos/microsoft/typescript-go,https://github.com/microsoft/typescript-go/pull/218,perf,"The changes improve the performance of file reading by reducing memory usage and speeding up operations, which aligns with performance improvements.","The changes improve the performance of file reading by reducing memory usage and speeding up operations, which aligns with performance improvements.",Human,,,,,,Go,False,True,False,16.296388888888888,,0.6790162037037036,,45,8,334.0,54.0,True,"The code reads the contents of a file into memory, returning it as a string. Originally, it read the file into a byte slice, then converted that to a string. The code also checks for byte order marks (BOMs) at the start of the file, to handle file encodings properly.","The optimized version changes the file reading process to use a strings.Builder that is directly sized to hold the entire file, avoiding a []byte buffer and the copy from []byte to string. This reduces memory usage and eliminates an extra buffer copy. The check for BOMs is moved to operate directly on the string (using optimized string comparison) instead of byte slices, which leverages Go's fast string operations and further improves performance. Redundant operations, such as temporary buffers and conversions, are removed, simplifying the code's logic.",Code Smells and Structural Simplification,Remove Redundant Method Calls,high,True,,1151
2369320781,405,Reduce runtime/allocations during test run,"These are some of the top offenders for memory allocation and runtime. The collection of these brings the runtime of the runner from 6.5s to 6s on my machine, a 7.7% improvement.",Human,5341706,jakebailey,closed,2025-03-03 23:33:42+00:00,2025-03-04 00:47:08+00:00,2025-03-04 00:47:08+00:00,,https://api.github.com/repos/microsoft/typescript-go,https://github.com/microsoft/typescript-go/pull/405,perf,"The changes improve performance by reducing runtime and memory allocations during test runs, leading to faster execution times.","The changes improve performance by reducing runtime and memory allocations during test runs, leading to faster execution times.",Human,,,,,,Go,True,True,False,1.2238888888888888,1.2238888888888888,0.0509953703703703,0.0509953703703703,42,5,178.0,32.0,True,"The code is part of a test runner system, and its purpose is to execute and manage test runs efficiently. It involves orchestrating test execution, possibly collecting and reporting results, and managing related data structures during the process.","The commit focuses on reducing unnecessary memory allocation and lowering runtime costs within the testing infrastructure. This is achieved by making various targeted changes including: removing or minimizing creation of temporary objects, reusing data structures where possible, avoiding work that produces intermediate allocations, and possibly restructuring algorithms to avoid inefficient constructs. Redundant or easily optimized method calls and logic were identified and removed or revised, leading to an aggregate runtime improvement of ~7.7%. The changes seem directed at hotspots identified via profiling.",Memory and Data Locality Optimizations,Optimize Object Use,high,True,,1109
2555753483,11934,update to stop closures from lazy functions and linq,"Fixes : Allocation issue.

### Context
Looking at a trace of allocations. It was shown that some of the allocations were coming from closures. This pr addresses the closures found.

### Changes Made
* Removed lazy from exclude tester function since it was not needed since lifetime of lazy object was within the method itself.
* switched from linq clause for add range to manually adding items, because the linq version caused a closure from a method it did not have context with.

### Testing
Used ILSpy to verify that the closures disappeared. (DisplayClass represents a closure)
Before
![image](https://github.com/user-attachments/assets/123f3b9f-4d2e-4f1d-bcba-2dcbe9144741)

Afterwards
![image](https://github.com/user-attachments/assets/9a7b9f91-1466-4de9-a0ff-579e0d5407bc)



### Notes
",Human,62913657,ttstanley,closed,2025-05-30 21:45:28+00:00,2025-06-06 19:07:40+00:00,2025-06-06 19:07:40+00:00,,https://api.github.com/repos/dotnet/msbuild,https://github.com/dotnet/msbuild/pull/11934,perf,"The changes address an allocation issue by removing unnecessary closures and optimizing code to prevent allocations, which improves performance without adding new features or fixing bugs.","The changes address an allocation issue by removing unnecessary closures and optimizing code to prevent allocations, which improves performance without adding new features or fixing bugs.",Human,,,,,,C#,True,True,False,165.37,165.37,6.890416666666667,6.890416666666667,52,9,814.0,103.0,True,"The code manages collection operations and filtering, utilizing helper functions (such as exclude tester) and methods to aggregate items. Previously, it used lazy evaluation and LINQ constructs that inadvertently created closures, leading to additional memory allocations. The goal is to avoid these allocations by restructuring logic to not require closures.","The original code used a Lazy<T> to defer the creation of an object, but the lazy object was only used within the method's local scope, thus offering no benefit while introducing unnecessary overhead. Furthermore, the code used a LINQ clause (e.g., Select or Where) to add multiple items to a collection, resulting in the compiler generating closures (DisplayClass) to capture variablesâ€”again incurring extra allocations. The optimized version removes the Lazy<T>, opting for direct object use, and replaces the LINQ add-range approach with an explicit loop, thus avoiding these compiler-generated closures and reducing unnecessary allocations. This results in lower heap pressure and improves runtime efficiency by eliminating unnecessary objects and allocations.",Code Smells and Structural Simplification,Remove Redundant Method Calls,high,True,,1312
2452691617,114517,[mono][interp] Avoid doing extra lookups when not needed,micro optimization to reduce the cost of not finding a match for an intrinsic that has a name match but an invalid signature.,Human,24063,lewing,closed,2025-04-11 02:17:22+00:00,2025-04-11 16:27:32+00:00,2025-04-11 16:27:32+00:00,,https://api.github.com/repos/dotnet/runtime,https://github.com/dotnet/runtime/pull/114517,perf,"The change is described as a micro optimization to reduce unnecessary lookups, which improves performance without adding features or fixing bugs.","The change is described as a micro optimization to reduce unnecessary lookups, which improves performance without adding features or fixing bugs.",Human,,,,,,C#,True,True,False,14.169444444444444,14.169444444444444,0.5903935185185185,0.5903935185185185,56,8,125.0,23.0,True,"The code is part of Mono's interpreter and is responsible for resolving intrinsics, which are runtime-optimized functions. It attempts to match by name and then by signature to find a valid intrinsic implementation. If a name matches but the signature does not, it used to perform extra lookup work that was unnecessary when there could be no valid match.","Previously, the code would proceed to perform further lookups or checks even after it was determined that an intrinsic's name matches but its signature does not, incurring extra cost despite knowing failure is certain. The optimized code short-circuits or exits early from this logic, avoiding those extra, redundant steps. There are no algorithmic changes to how matches are found, but the control flow is changed to skip unnecessary processing. Performance improves by reducing CPU cycles spent in this non-match scenario. The code is cleaner, with eliminated redundant logic.",Code Smells and Structural Simplification,Remove Redundant Method Calls,high,True,,1123
2622581875,117071,Reduce HTTP headers validation overhead,"When adding/reading headers where we don't have a special parser, ""parsing"" only validates that there are no new lines in the value. This change special-cases this (common) case and avoids allocating the `HeaderStoreItemInfo`.

Existing code paths where only non-validating APIs are used stay the same.
Overhead for cases of reading headers with validation, where they were added without validation, and we do have a known parser (still common) is minimal and an acceptable tradeoff IMO (extra branch).

| Method          | Toolchain | Mean      | Error    | Ratio | Allocated | Alloc Ratio |
|---------------- |---------- |----------:|---------:|------:|----------:|------------:|
| Add             | main      |  44.31 ns | 0.386 ns |  1.00 |      32 B |        1.00 |
| Add             | pr        |  21.35 ns | 0.018 ns |  0.48 |         - |        0.00 |
|                 |           |           |          |       |           |             |
| AddEnumerable   | main      |  36.27 ns | 0.592 ns |  1.00 |      32 B |        1.00 |
| AddEnumerable   | pr        |  28.80 ns | 0.265 ns |  0.79 |         - |        0.00 |
|                 |           |           |          |       |           |             |
| GetValues       | main      |  92.39 ns | 0.302 ns |  1.00 |      64 B |        1.00 |
| GetValues       | pr        |  40.35 ns | 0.090 ns |  0.44 |      32 B |        0.50 |
|                 |           |           |          |       |           |             |
| AddAndGetValues | main      |  94.01 ns | 0.192 ns |  1.00 |      64 B |        1.00 |
| AddAndGetValues | pr        |  42.92 ns | 0.229 ns |  0.46 |      32 B |        0.50 |
|                 |           |           |          |       |           |             |
| CloneHeaders    | main      | 905.95 ns | 1.891 ns |  1.00 |    1112 B |        1.00 |
| CloneHeaders    | pr        | 490.74 ns | 1.064 ns |  0.54 |     600 B |        0.54 |

<details>
<summary>Benchmark code</summary>

```c#
BenchmarkRunner.Run<HeadersBench>(args: args);

[MemoryDiagnoser(false)]
public class HeadersBench
{
    private readonly HttpResponseHeaders _headers = new HttpResponseMessage().Headers;
    private readonly HttpRequestHeaders _headersToClone = new HttpRequestMessage().Headers;
    private readonly string[] _fooAsArray = [""Foo""];

    public HeadersBench()
    {
        _headersToClone.TryAddWithoutValidation(""priority"", ""u=0, i"");
        _headersToClone.TryAddWithoutValidation(""sec-ch-ua-mobile"", ""?0"");
        _headersToClone.TryAddWithoutValidation(""sec-ch-ua-platform"", ""\""Windows\"""");
        _headersToClone.TryAddWithoutValidation(""sec-fetch-dest"", ""document"");
        _headersToClone.TryAddWithoutValidation(""sec-fetch-mode"", ""navigate"");
        _headersToClone.TryAddWithoutValidation(""sec-fetch-site"", ""none"");
        _headersToClone.TryAddWithoutValidation(""sec-fetch-user"", ""?1"");
        _headersToClone.TryAddWithoutValidation(""upgrade-insecure-requests"", ""1"");
    }

    [Benchmark]
    public void Add()
    {
        _headers.Add(""X-Custom"", ""Foo"");
        _headers.Clear();
    }

    [Benchmark]
    public void AddEnumerable()
    {
        _headers.Add(""X-Custom"", _fooAsArray);
        _headers.Clear();
    }

    [Benchmark]
    public object GetValues()
    {
        _headers.TryAddWithoutValidation(""X-Custom"", ""Foo"");
        IEnumerable<string> values = _headers.GetValues(""X-Custom"");
        _headers.Clear();
        return values;
    }

    [Benchmark]
    public object AddAndGetValues()
    {
        _headers.Add(""X-Custom"", ""Foo"");
        IEnumerable<string> values = _headers.GetValues(""X-Custom"");
        _headers.Clear();
        return values;
    }

    [Benchmark]
    public HttpRequestHeaders CloneHeaders()
    {
        HttpRequestHeaders newHeaders = new HttpRequestMessage().Headers;

        foreach (KeyValuePair<string, IEnumerable<string>> header in _headersToClone)
        {
            newHeaders.Add(header.Key, header.Value);
        }

        return newHeaders;
    }
}
```

</details>",Human,25307628,MihaZupan,open,2025-06-26 23:02:39+00:00,,,,https://api.github.com/repos/dotnet/runtime,https://github.com/dotnet/runtime/pull/117071,perf,"The change introduces a performance optimization by reducing overhead in HTTP headers validation, as demonstrated by the benchmark results showing reduced allocation and execution time.","The change introduces a performance optimization by reducing overhead in HTTP headers validation, as demonstrated by the benchmark results showing reduced allocation and execution time.",Human,,,,,,C#,False,False,True,,,,,39,5,4135.0,432.0,True,"The code manages HTTP headers, specifically optimizing how header values are validated when they are added or read. In cases where there is no special parser for a header, the value is currently only checked for the absence of newlines (to prevent header injection). The new code detects this common case and avoids unnecessary allocation of a HeaderStoreItemInfo object, thus streamlining this critical path and reducing memory allocation and computation.","The change introduces a shortcut for the common scenario of adding or reading headers that do not require complex validation or parsingâ€”only a check for newline characters is needed. The optimized version detects this condition and avoids the creation and storage of an extra HeaderStoreItemInfo object, reducing both execution time and memory allocation. The more complex header parsing paths and validation logic remain unchanged for special cases. Benchmarks show nearly a 2x improvement in performance and the removal or reduction of memory allocations. The change does introduce a minor extra branching in some scenarios, but these are rare and have negligible impact compared to the mainline gains.",Memory and Data Locality Optimizations,Optimize Object Use,high,True,,2191
2309904375,112047,WasmAppBuilder: Remove double computation of a value,Credit goes to https://pvs-studio.com/en/blog/posts/csharp/1216/,Human,1764393,filipnavara,closed,2025-01-31 19:38:16+00:00,2025-01-31 22:40:48+00:00,2025-01-31 22:40:48+00:00,,https://api.github.com/repos/dotnet/runtime,https://github.com/dotnet/runtime/pull/112047,perf,"The change removes redundant computation, which improves the efficiency of the code without adding new features or fixing a bug.","The change removes redundant computation, which improves the efficiency of the code without adding new features or fixing a bug.",Human,,,,,,C#,True,True,False,3.042222222222222,3.042222222222222,0.1267592592592592,0.1267592592592592,52,7,64.0,4.0,True,"The code in question appears to be part of the WasmAppBuilder tool, where a value was being computed twice unnecessarily. The code has been updated to calculate this value only once and use the stored result subsequently, rather than recomputing it. This solves redundant computation and improves efficiency.","In the original version, a certain value (potentially the result of a function call or expensive computation) was computed twice, once for an 'if' check and again separately for further logic. In the optimized version, the result of the computation is stored in a variable, and then the variable is used in both places, eliminating unnecessary recomputation. This change improves performance by avoiding redundant work and enhances code clarity and maintainability, but does not change the overall logic or algorithm.",Code Smells and Structural Simplification,Remove Redundant Method Calls,high,True,,1086
2412640161,35835,Changes to AsyncLocal usage for better lazy loading performance,"Changed AsyncLocal to ThreadId for better performance
Fixes #35832 ",Human,111216098,henriquewr,closed,2025-03-24 05:11:54+00:00,2025-03-25 21:40:12+00:00,2025-03-25 21:40:12+00:00,,https://api.github.com/repos/dotnet/efcore,https://github.com/dotnet/efcore/pull/35835,perf,"The change improves performance by replacing AsyncLocal with ThreadId for better lazy loading performance, which is a performance optimization rather than a bug fix or new feature.","The change improves performance by replacing AsyncLocal with ThreadId for better lazy loading performance, which is a performance optimization rather than a bug fix or new feature.",Human,,,,,,C#,True,True,False,40.471666666666664,40.471666666666664,1.6863194444444445,1.6863194444444445,63,9,68.0,9.0,True,"The code manages some form of context or data scoped to the current logical thread or async call. Previously, it used AsyncLocal to store or retrieve this data for lazy loading scenarios. The change replaces AsyncLocal (which flows with async calls and can incur more overhead) with usage of a ThreadId, indicating a shift to thread-based context management instead of async-context. This is aimed at making per-thread data access faster, likely in cases where asynchrony/context propagation is not required.","In the original code, AsyncLocal was used to maintain context, which provides correctness across async/await boundaries but at significant allocation and lookup overhead. In the optimized version, the code now uses ThreadId as the key or context reference, reducing overhead by not flowing context through async and leveraging a more lightweight thread-local approach. The algorithmic logic mostly stays the same (contextual storage), but the underlying mechanism changes from async-local storage to thread-local storage. This significantly improves runtime performance in environments where asynchronous context flow isn't strictly needed. Redundant allocation, and the overhead of AsyncLocal's value flows, are removed. The code is structurally simpler and more efficient in scenarios where thread affinity is guaranteed.",Memory and Data Locality Optimizations,Optimize Object Use,high,True,,1172
2386158448,18377,Test-TP: Reference assembly loading fixes,"I noticed that with the current reference loading of TypeProviders, if I had 139 reference assemblies (in a solution memory), I ended up calling the Assembly.Load for 897 times. This is because so many assemblies have the same references like System.Memory, System.Xml, System.Buffers, System.Threading.Tasks.Extensions, ... And the code said ""load all reference assemblies"". Simple fix: Check already loaded reference assemblies before trying to call the slow Assembly.Load again.

The sourceAssembliesTable_ is a ConcurrentDictionary to ensure thread-safety. However, instead of code using it in thread-safe way, it was used by double-lookup. So that is fixed to actually use it properly. (It's role is to be used as a guard to sourceAssemblies_ array, which is manually lazy-loaded from the queue.)

These changes match the FSharp.TypeProvider.SDK merged PR.
",Human,229355,Thorium,closed,2025-03-11 19:47:24+00:00,2025-03-12 12:06:13+00:00,2025-03-12 12:06:13+00:00,,https://api.github.com/repos/dotnet/fsharp,https://github.com/dotnet/fsharp/pull/18377,perf,"The changes improve the efficiency of assembly loading by avoiding redundant Assembly.Load calls and fixing thread-safety usage, which enhances performance without adding new features or fixing bugs explicitly.","The changes improve the efficiency of assembly loading by avoiding redundant Assembly.Load calls and fixing thread-safety usage, which enhances performance without adding new features or fixing bugs explicitly.",Human,,,,,,F#,True,True,False,16.31361111111111,16.31361111111111,0.6797337962962963,0.6797337962962963,41,5,867.0,125.0,True,"The code is responsible for loading reference assemblies needed by TypeProviders in a .NET tooling scenario. Previously, it attempted to repeatedly call Assembly.Load for every reference it encountered, whether or not it was already loaded, causing redundant loading and performance overhead. The new code ensures that before attempting to load an assembly, it checks if that assembly has already been loaded using a concurrent dictionary, thus minimizing repeat loads. It also fixes improper use of the concurrent dictionary as a thread-safety guard, replacing a redundant double-lookup mechanism with correct usage.","Originally, the code would iterate over all reference assemblies and call the potentially expensive Assembly.Load on each, regardless of whether that assembly had already been loaded, leading to significant overhead (as in the example, 139 actual references resulting in 897 loads). The optimized code introduces a check with a thread-safe concurrent dictionary to track which assemblies have already been loaded, ensuring Assembly.Load is only called as needed. Additionally, the dictionary is now used in a thread-safe, correct manner (avoiding the unsafe double-lookup idiom). This reduces redundant work, decreases load times, and improves resource utilization, especially in scenarios with overlapping references. There is also a structural/idiomatic improvement in concurrent code.",Code Smells and Structural Simplification,Remove Redundant Method Calls,high,True,,1352
2483117033,18509,"SIMD vectorization of Array.sum<int>, etc","## Description

Specific overloads (float, float32, int, int64) of Seq.sum, ~~Seq.average,~~ Array.sum ~~and Array.average~~ to take advantage of vectorization in System.Linq.Enumerable module.

This is potentially a naive first try to solve #16230 by the spirit of @T-Gro comment https://github.com/dotnet/fsharp/issues/16230#issuecomment-2826895557

## Checklist

- [ ] Test cases added
- [x] Performance benchmarks added in case of performance changes
- [x] Release notes entry updated:
    > Please make sure to add an entry with short succinct description of the change as well as link to this pull request to the respective release notes file, if applicable.
",Human,229355,Thorium,open,2025-04-26 12:04:41+00:00,,,,https://api.github.com/repos/dotnet/fsharp,https://github.com/dotnet/fsharp/pull/18509,perf,"The PR introduces SIMD vectorization to improve the performance of specific overloads of sum functions, which is a performance enhancement rather than a bug fix or new feature.","The PR introduces SIMD vectorization to improve the performance of specific overloads of sum functions, which is a performance enhancement rather than a bug fix or new feature.",Human,,,,,,F#,False,False,True,,,,,41,5,677.0,94.0,True,"The code implements SIMD vectorization for functions such as Array.sum and Seq.sum for types like int, int64, float, and float32. These functions compute the sum of all elements in an array or sequence, and the change leverages hardware acceleration (SIMD) via the System.Linq.Enumerable module to process multiple elements at once, thereby accelerating the summation.","In the original version, summing was done using standard sequential iteration (i.e., a simple loop accumulating the total). The optimized version replaces or augments this with SIMD vectorized operations, likely using System.Linq.Enumerable to take advantage of hardware vector instructions. This leads to significant improvements in time complexity in practice for large arrays by enabling multiple elements to be summed in a single instruction, reducing the number of loop iterations and memory accesses. There are no significant algorithmic changesâ€”just an enhancement of the existing algorithm's implementation to leverage faster hardware paths. Redundant or unnecessary code remains mostly untouched, as this optimization focuses on performance improvements rather than code simplification.",Algorithm-Level Optimizations,Select Computationally Efficient Algorithms,high,True,,1271
2573225924,61822,"optimization, reduce memory usage","remove the creation of unnecessary lambda wrappers.

Speeds up 11% the project initialization time of tsserver in large repositories.

Fixes #61821

",Human,28921716,VincentBailly,closed,2025-06-06 11:20:52+00:00,2025-06-09 18:48:31+00:00,2025-06-09 18:48:31+00:00,,https://api.github.com/repos/microsoft/TypeScript,https://github.com/microsoft/TypeScript/pull/61822,perf,"The change improves performance by reducing memory usage and speeding up project initialization time, which aligns with the 'perf' category.","The change improves performance by reducing memory usage and speeding up project initialization time, which aligns with the 'perf' category.",Human,,,,,,TypeScript,True,True,False,79.46083333333333,79.46083333333333,3.3108680555555554,3.3108680555555554,33,4,155.0,21.0,True,"The code relates to the initialization of the TypeScript server (tsserver), particularly during project startup in large repositories. It manages how lambda functions or closures are created to handle certain logic, probably related to configuring or initializing project structures.","Previously, the code would generate new lambda (arrow-function) wrappers unnecessarilyâ€”possibly within tight loops or hot paths. The optimization removes or reduces this creation, likely by either reusing existing functions, moving code to named functions, or restructuring call sites to avoid closure allocations. This change reduces both memory usage and initialization latency because each function instance (lambda) in JavaScript/TypeScript creates a new object, which can have a non-trivial cost when repeated many times. The commit leads to an 11% reduction in initialization time, indicating significant real-world performance benefit. No algorithmic overhaul was performed, but redundant instance creation was removed, reducing pressure on the garbage collector and improving warmup.",Code Smells and Structural Simplification,Remove Redundant Method Calls,high,True,,1141
2596620305,2530,Improve JupyterLab extension build time,"This PR reduces the build time of the JupyterLab package from ~70s/30s/30s (on Windows/Ubuntu/Mac) to ~15s/10s/13s by eliminating the heavyweight `jupyterlab` dependency. Instead, we build and bundle the JavaScript + Python directly using the `@jupyter/builder` npm package. 

Additionally, we get rid of `jlpm` (a.k.a. yarn) in favor of using `npm` which we already use in the rest of our build.

The `jupyterlab-core` dir contains the minimal configuration, that `@jupyter/builder` uses, that previously lived in the `jupyterlab` package. This is really all we needed from there.
Â 
The resulting packages should be identical.

I've ruled out:
- eliminating the webpack dependency in favor of a different bundler, since the JupyterLab extension architecture uses webpack module federation - there's no getting rid of it.
- removing the JupyterLab package completely - this feels very heavyhanded to me, considering how minimal the build now is and how we still get value from the package IMO (Q# syntax highlighting). 
- checking in the built files - the built files are webpack-bundled, and not very readable. Checking in these artifacts directly would again be pretty heavy-handed, sacrificing maintainability to save a few seconds of build time.

Fixes #2482",Human,16928427,minestarks,closed,2025-06-16 19:03:41+00:00,2025-06-16 23:08:14+00:00,2025-06-16 23:08:14+00:00,,https://api.github.com/repos/microsoft/qsharp,https://github.com/microsoft/qsharp/pull/2530,perf,"The PR introduces a new build approach that significantly improves the build time of the JupyterLab extension, which is a performance enhancement rather than a bug fix or new feature.","The PR introduces a new build approach that significantly improves the build time of the JupyterLab extension, which is a performance enhancement rather than a bug fix or new feature.",Human,,,,,,Rust,True,True,False,4.075833333333334,4.075833333333334,0.1698263888888889,0.1698263888888889,39,5,1275.0,192.0,True,"This code refactors the build system for a JupyterLab extension. Previously, the build process relied heavily on the monolithic 'jupyterlab' package and an additional JavaScript build tool (jlpm/yarn). The new approach directly uses '@jupyter/builder' and npm, bypassing unnecessary dependencies and complex configuration. The result is significantly faster build times while maintaining functionally identical packages.","The main improvements are: 1) Removing a heavyweight dependency ('jupyterlab') from the build chain; 2) Switching from 'jlpm/yarn' to the already-used 'npm' to avoid redundant tool invocation; 3) Creating a minimal configuration in 'jupyterlab-core' that provides only the essential build context needed by '@jupyter/builder', previously sourced from the full JupyterLab package. There are no changes to the actual features or output artifacts; only the build system is optimized. These changes reduce build time by avoiding overhead from installing/building unnecessary packages and eliminate duplicated build steps or tools.",Code Smells and Structural Simplification,Remove Redundant Method Calls,high,True,,1424
2394225726,546,Query Optimizations,"
<!-- ELLIPSIS_HIDDEN -->



> [!IMPORTANT]
> Optimizes OAuth provider creation in `crud.tsx` by using `Promise.all` for concurrent execution.
> 
>   - **Optimization**:
>     - Replaces loop with `Promise.all` for concurrent execution of OAuth provider creation in `onCreate` function in `crud.tsx`.
> 
> <sup>This description was created by </sup>[<img alt=""Ellipsis"" src=""https://img.shields.io/badge/Ellipsis-blue?color=175173"">](https://www.ellipsis.dev?ref=stack-auth%2Fstack-auth&utm_source=github&utm_medium=referral)<sup> for f883e1a7362f32c8585b5d571eb4d9a76ac80eef. It will automatically update as commits are pushed.</sup>

<!-- ELLIPSIS_HIDDEN -->",Human,16204510,TheCactusBlue,closed,2025-03-14 17:22:57+00:00,2025-05-07 23:44:12+00:00,,,https://api.github.com/repos/stack-auth/stack-auth,https://github.com/stack-auth/stack-auth/pull/546,perf,"The change improves the performance of OAuth provider creation by using Promise.all for concurrent execution, which is a performance optimization.","The change improves the performance of OAuth provider creation by using Promise.all for concurrent execution, which is a performance optimization.",Human,,,,,,TypeScript,False,True,False,1302.3541666666667,,54.26475694444445,,19,2,673.0,63.0,True,"The code handles the creation of OAuth providers, likely as part of a user management feature in a web application. It previously iterated serially over a set of OAuth provider configurations, creating each one in turn. The optimization changes the execution so that all OAuth provider creations are initiated concurrently using JavaScript's Promise.all, which allows all provider creation promises to execute in parallel, thus speeding up the overall process.","Original code performed OAuth provider creation in a loop, likely awaiting each provider's creation before continuing with the next, resulting in sequential (blocking) behavior. In the optimized version, Promise.all is used to initiate all provider creation promises at once and then await their results as a group. This increases efficiency by leveraging concurrency: all asynchronous creation operations run in parallel, reducing wall-clock time for the overall operation. There is no algorithmic change in problem approach, but a significant improvement in runtime behavior for IO-bound or asynchronous operations. The logic becomes cleaner and more idiomatic for asynchronous JS/TS code.",Algorithm-Level Optimizations,Select Computationally Efficient Algorithms,high,True,,1294
2400016065,2397,[performance improvements] json_repair.repair_json() improve performance,"As explained in [the documentation](https://github.com/mangiucugna/json_repair/?tab=readme-ov-file#performance-considerations,) if you already check that the json is valid, you can pass `skip_json_loads=True` to improve performance.",Human,4247706,mangiucugna,closed,2025-03-18 07:37:59+00:00,2025-03-21 18:25:20+00:00,2025-03-21 18:25:19+00:00,,https://api.github.com/repos/crewAIInc/crewAI,https://github.com/crewAIInc/crewAI/pull/2397,perf,"The PR explicitly mentions improving performance of the json_repair.repair_json() function by adding an option to skip JSON validation, which is a performance enhancement.","The PR explicitly mentions improving performance of the json_repair.repair_json() function by adding an option to skip JSON validation, which is a performance enhancement.",Human,,,,,,Python,True,True,False,82.78916666666667,82.78888888888889,3.4495486111111116,3.4495370370370373,72,5,232.0,21.0,True,"The code provides a function `repair_json()` in the `json_repair` module that attempts to fix malformed or invalid JSON strings so they become parseable. It includes an optimization where, if the input is guaranteed to already be valid JSON, the function can skip the expensive re-validation step, improving performance for validated input.","Originally, `repair_json()` would attempt to load and parse its input JSON every time, even if the caller already knew it was valid, incurring unnecessary computational overhead. The optimized version introduces an optional parameter `skip_json_loads`; when set to True, it bypasses the redundant parsing/check. As a result, if the caller is certain of the input's validity, they can avoid duplicating parsing work, reducing runtime and possibly memory use for large JSON strings. The change does not alter the overall logic but exposes a faster path for a known state, thus increasing efficiency.",Code Smells and Structural Simplification,Remove Redundant Method Calls,high,True,,1169
2337334370,2136,âš¡ï¸ Speed up function `calculate_node_levels` by 48x,"### ğŸ“„ 4,848% (48.48x) speedup for ***`calculate_node_levels` in `src/crewai/flow/utils.py`***

â±ï¸ Runtime :   **`58.0 milliseconds`**  **â†’** **`1.17 millisecond`** (best of `318` runs)
<details>
<summary> ğŸ“ Explanation and details</summary>

To optimize the given `calculate_node_levels` function, we can make several changes focusing on reducing the number of nested loops and leveraging data structures more efficiently. Here is the revised function.

1. Replace list `queue` (with `pop(0)`) with `deque` which provides O(1) time complexity for append and pop operations.
2. Precompute method dependencies instead of repeatedly checking conditions inside loops.
3. Organize the steps for better readability and separate route processing into a helper function.



Key optimizations.
1. Using `deque` instead of list `queue` to optimize appending and popping elements.
2. Precomputing listener dependencies reduces the number of checks and allows faster access.
3. Factoring out the router processing logic into the `process_router_paths` function improves readability and maintainability.

</details>

âœ… **Correctness verification report:**


| Test                        | Status            |
| --------------------------- | ----------------- |
| âš™ï¸ Existing Unit Tests | ğŸ”˜ **None Found** |
| ğŸŒ€ Generated Regression Tests | âœ… **31 Passed** |
| âª Replay Tests | ğŸ”˜ **None Found** |
| ğŸ” Concolic Coverage Tests | ğŸ”˜ **None Found** |
|ğŸ“Š Tests Coverage       | 100.0% |
<details>
<summary>ğŸŒ€ Generated Regression Tests Details</summary>

```python
from typing import Any, Dict, List, Set

# imports
import pytest  # used for our unit tests
from crewai.flow.utils import calculate_node_levels


# Mock flow class for testing
class MockFlow:
    def __init__(self, methods, listeners, routers, router_paths):
        self._methods = methods
        self._listeners = listeners
        self._routers = routers
        self._router_paths = router_paths

# unit tests
def test_single_start_method_no_listeners_or_routers():
    flow = MockFlow(
        methods={""start"": MockMethod(True)},
        listeners={},
        routers=set(),
        router_paths={}
    )
    expected = {""start"": 0}
    codeflash_output = calculate_node_levels(flow)

def test_multiple_start_methods_no_listeners_or_routers():
    flow = MockFlow(
        methods={""start1"": MockMethod(True), ""start2"": MockMethod(True)},
        listeners={},
        routers=set(),
        router_paths={}
    )
    expected = {""start1"": 0, ""start2"": 0}
    codeflash_output = calculate_node_levels(flow)

def test_single_or_listener():
    flow = MockFlow(
        methods={""start"": MockMethod(True)},
        listeners={""listener"": (""OR"", [""start""])},
        routers=set(),
        router_paths={}
    )
    expected = {""start"": 0, ""listener"": 1}
    codeflash_output = calculate_node_levels(flow)

def test_multiple_or_listeners():
    flow = MockFlow(
        methods={""start1"": MockMethod(True), ""start2"": MockMethod(True)},
        listeners={""listener1"": (""OR"", [""start1""]), ""listener2"": (""OR"", [""start2""])},
        routers=set(),
        router_paths={}
    )
    expected = {""start1"": 0, ""start2"": 0, ""listener1"": 1, ""listener2"": 1}
    codeflash_output = calculate_node_levels(flow)

def test_single_and_listener():
    flow = MockFlow(
        methods={""start1"": MockMethod(True), ""start2"": MockMethod(True)},
        listeners={""listener"": (""AND"", [""start1"", ""start2""])},
        routers=set(),
        router_paths={}
    )
    expected = {""start1"": 0, ""start2"": 0, ""listener"": 1}
    codeflash_output = calculate_node_levels(flow)

def test_multiple_and_listeners():
    flow = MockFlow(
        methods={""start1"": MockMethod(True), ""start2"": MockMethod(True), ""start3"": MockMethod(True)},
        listeners={""listener1"": (""AND"", [""start1"", ""start2""]), ""listener2"": (""AND"", [""start2"", ""start3""])},
        routers=set(),
        router_paths={}
    )
    expected = {""start1"": 0, ""start2"": 0, ""start3"": 0, ""listener1"": 1, ""listener2"": 1}
    codeflash_output = calculate_node_levels(flow)

def test_mixed_conditions():
    flow = MockFlow(
        methods={""start1"": MockMethod(True), ""start2"": MockMethod(True)},
        listeners={""listener1"": (""OR"", [""start1""]), ""listener2"": (""AND"", [""start1"", ""start2""])},
        routers=set(),
        router_paths={}
    )
    expected = {""start1"": 0, ""start2"": 0, ""listener1"": 1, ""listener2"": 1}
    codeflash_output = calculate_node_levels(flow)

def test_single_router():
    flow = MockFlow(
        methods={""start"": MockMethod(True)},
        listeners={""listener"": (""OR"", [""path1""])},
        routers={""start""},
        router_paths={""start"": [""path1""]}
    )
    expected = {""start"": 0, ""listener"": 1}
    codeflash_output = calculate_node_levels(flow)

def test_multiple_routers():
    flow = MockFlow(
        methods={""start1"": MockMethod(True), ""start2"": MockMethod(True)},
        listeners={""listener1"": (""OR"", [""path1""]), ""listener2"": (""OR"", [""path2""])},
        routers={""start1"", ""start2""},
        router_paths={""start1"": [""path1""], ""start2"": [""path2""]}
    )
    expected = {""start1"": 0, ""start2"": 0, ""listener1"": 1, ""listener2"": 1}
    codeflash_output = calculate_node_levels(flow)

def test_no_start_methods():
    flow = MockFlow(
        methods={""method1"": MockMethod(False)},
        listeners={},
        routers=set(),
        router_paths={}
    )
    expected = {}
    codeflash_output = calculate_node_levels(flow)

def test_listeners_with_no_trigger_methods():
    flow = MockFlow(
        methods={""start"": MockMethod(True)},
        listeners={""listener"": (""OR"", [])},
        routers=set(),
        router_paths={}
    )
    expected = {""start"": 0}
    codeflash_output = calculate_node_levels(flow)

def test_empty_flow():
    flow = MockFlow(
        methods={},
        listeners={},
        routers=set(),
        router_paths={}
    )
    expected = {}
    codeflash_output = calculate_node_levels(flow)

def test_large_number_of_methods_and_listeners():
    methods = {f""start{i}"": MockMethod(True) for i in range(100)}
    listeners = {f""listener{i}"": (""OR"", [f""start{i}""]) for i in range(100)}
    flow = MockFlow(
        methods=methods,
        listeners=listeners,
        routers=set(),
        router_paths={}
    )
    expected = {f""start{i}"": 0 for i in range(100)}
    expected.update({f""listener{i}"": 1 for i in range(100)})
    codeflash_output = calculate_node_levels(flow)



def test_deterministic_behavior():
    flow = MockFlow(
        methods={""start"": MockMethod(True)},
        listeners={""listener"": (""OR"", [""start""])},
        routers=set(),
        router_paths={}
    )
    expected = {""start"": 0, ""listener"": 1}
    codeflash_output = calculate_node_levels(flow)
    codeflash_output = calculate_node_levels(flow)

# Mock method class for testing
class MockMethod:
    def __init__(self, is_start_method):
        if is_start_method:
            self.__is_start_method__ = True
# codeflash_output is used to check that the output of the original code is the same as that of the optimized code.

from typing import Any, Dict, List, Set

# imports
import pytest  # used for our unit tests
from crewai.flow.utils import calculate_node_levels


# Helper classes to simulate flow structure
class Method:
    def __init__(self, is_start_method=False):
        self.__is_start_method__ = is_start_method

class Flow:
    def __init__(self):
        self._methods = {}
        self._listeners = {}
        self._routers = set()
        self._router_paths = {}

# unit tests
def test_single_start_method():
    flow = Flow()
    flow._methods['start_method'] = Method(is_start_method=True)
    codeflash_output = calculate_node_levels(flow)

def test_multiple_start_methods():
    flow = Flow()
    flow._methods['start_method_1'] = Method(is_start_method=True)
    flow._methods['start_method_2'] = Method(is_start_method=True)
    codeflash_output = calculate_node_levels(flow)

def test_single_or_listener():
    flow = Flow()
    flow._methods['start_method'] = Method(is_start_method=True)
    flow._listeners['listener_method'] = ('OR', ['start_method'])
    codeflash_output = calculate_node_levels(flow)

def test_single_and_listener():
    flow = Flow()
    flow._methods['start_method'] = Method(is_start_method=True)
    flow._listeners['listener_method'] = ('AND', ['start_method'])
    codeflash_output = calculate_node_levels(flow)

def test_multiple_or_listeners():
    flow = Flow()
    flow._methods['start_method'] = Method(is_start_method=True)
    flow._listeners['listener_method_1'] = ('OR', ['start_method'])
    flow._listeners['listener_method_2'] = ('OR', ['start_method'])
    codeflash_output = calculate_node_levels(flow)

def test_multiple_and_listeners():
    flow = Flow()
    flow._methods['start_method'] = Method(is_start_method=True)
    flow._listeners['listener_method_1'] = ('AND', ['start_method'])
    flow._listeners['listener_method_2'] = ('AND', ['start_method'])
    codeflash_output = calculate_node_levels(flow)

def test_mixed_or_and_listeners():
    flow = Flow()
    flow._methods['start_method'] = Method(is_start_method=True)
    flow._listeners['or_listener'] = ('OR', ['start_method'])
    flow._listeners['and_listener'] = ('AND', ['start_method'])
    codeflash_output = calculate_node_levels(flow)

def test_single_router():
    flow = Flow()
    flow._methods['start_method'] = Method(is_start_method=True)
    flow._routers.add('start_method')
    flow._router_paths['start_method'] = ['router_path']
    codeflash_output = calculate_node_levels(flow)

def test_router_with_listeners():
    flow = Flow()
    flow._methods['start_method'] = Method(is_start_method=True)
    flow._routers.add('start_method')
    flow._router_paths['start_method'] = ['router_path']
    flow._listeners['listener_method'] = ('OR', ['router_path'])
    codeflash_output = calculate_node_levels(flow)

def test_multiple_routers():
    flow = Flow()
    flow._methods['start_method'] = Method(is_start_method=True)
    flow._routers.add('start_method')
    flow._routers.add('router_method_2')
    flow._router_paths['start_method'] = ['router_path_1']
    flow._router_paths['router_method_2'] = ['router_path_2']
    codeflash_output = calculate_node_levels(flow)

def test_empty_flow():
    flow = Flow()
    codeflash_output = calculate_node_levels(flow)

def test_cycle_in_flow():
    flow = Flow()
    flow._methods['start_method'] = Method(is_start_method=True)
    flow._listeners['cycle_method'] = ('OR', ['start_method'])
    flow._listeners['start_method'] = ('OR', ['cycle_method'])
    codeflash_output = calculate_node_levels(flow)

def test_disconnected_nodes():
    flow = Flow()
    flow._methods['start_method'] = Method(is_start_method=True)
    flow._methods['disconnected_method'] = Method()
    codeflash_output = calculate_node_levels(flow)

def test_listeners_with_no_trigger_methods():
    flow = Flow()
    flow._methods['start_method'] = Method(is_start_method=True)
    flow._listeners['listener_method'] = ('OR', [])
    codeflash_output = calculate_node_levels(flow)

def test_large_number_of_methods():
    flow = Flow()
    for i in range(1000):
        flow._methods[f'method_{i}'] = Method(is_start_method=(i == 0))
        if i > 0:
            flow._listeners[f'method_{i}'] = ('OR', [f'method_{i-1}'])
    codeflash_output = calculate_node_levels(flow)

def test_deep_hierarchical_structure():
    flow = Flow()
    flow._methods['start_method'] = Method(is_start_method=True)
    for i in range(1, 1000):
        flow._listeners[f'method_{i}'] = ('OR', [f'method_{i-1}' if i > 1 else 'start_method'])
    codeflash_output = calculate_node_levels(flow)
# codeflash_output is used to check that the output of the original code is the same as that of the optimized code.
```

</details>



[![Codeflash](https://img.shields.io/badge/Optimized%20with-Codeflash-yellow?style=flat&color=%23ffc428&logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iNDgwIiBoZWlnaHQ9ImF1dG8iIHZpZXdCb3g9IjAgMCA0ODAgMjgwIiBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgo8cGF0aCBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTI4Ni43IDAuMzc4NDE4SDIwMS43NTFMNTAuOTAxIDE0OC45MTFIMTM1Ljg1MUwwLjk2MDkzOCAyODEuOTk5SDk1LjQzNTJMMjgyLjMyNCA4OS45NjE2SDE5Ni4zNDVMMjg2LjcgMC4zNzg0MThaIiBmaWxsPSIjRkZDMDQzIi8+CjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMzExLjYwNyAwLjM3ODkwNkwyNTguNTc4IDU0Ljk1MjZIMzc5LjU2N0w0MzIuMzM5IDAuMzc4OTA2SDMxMS42MDdaIiBmaWxsPSIjMEIwQTBBIi8+CjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMzA5LjU0NyA4OS45NjAxTDI1Ni41MTggMTQ0LjI3NkgzNzcuNTA2TDQzMC4wMjEgODkuNzAyNkgzMDkuNTQ3Vjg5Ljk2MDFaIiBmaWxsPSIjMEIwQTBBIi8+CjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMjQyLjg3MyAxNjQuNjZMMTg5Ljg0NCAyMTkuMjM0SDMxMC44MzNMMzYzLjM0NyAxNjQuNjZIMjQyLjg3M1oiIGZpbGw9IiMwQjBBMEEiLz4KPC9zdmc+Cg==)](https://codeflash.ai)
",Human,1271289,misrasaurabh1,closed,2025-02-15 01:36:40+00:00,2025-02-25 20:29:22+00:00,,,https://api.github.com/repos/crewAIInc/crewAI,https://github.com/crewAIInc/crewAI/pull/2136,perf,The PR introduces a significant performance improvement (48x speedup) in the function `calculate_node_levels` by optimizing data structures and algorithmic steps without changing functionality. This is a clear performance enhancement.,The PR introduces a significant performance improvement (48x speedup) in the function `calculate_node_levels` by optimizing data structures and algorithmic steps without changing functionality. This is a clear performance enhancement.,Human,,,,,,Python,False,True,False,258.87833333333333,,10.786597222222222,,51,7,13258.0,1020.0,True,"The function `calculate_node_levels` computes the 'level' (e.g., depth or distance from a start node) of each node within a flow-like structure, presumably reflecting the execution or dependency hierarchy in a node-based workflow system. It determines which nodes (methods, listeners, routers) are triggered, when, and in what order, producing a dictionary mapping each node to its depth level.","The optimized version makes three main changes: (1) Replaces the use of a Python list as a queue (with O(n) pop(0) operations) with `collections.deque`, cutting queue operations to O(1) time; (2) Instead of repeatedly recalculating dependencies on each traversal, it precomputes listener dependencies up front, significantly reducing the number of checks performed during execution and avoiding redundant condition evaluations; (3) The code structure is improved for readability by factoring router-specific logic into a helper function. Overall, these changes avoid nested and repeated iterations, leading to a drastic (48x) speedup, improved memory access patterns, and greater maintainability.",Memory and Data Locality Optimizations,"Caching, Buffering",high,True,,4718
2337335339,2137,âš¡ï¸ Speed up method `CrewAgentParser._extract_thought` by 672%,"### ğŸ“„ 672% (6.72x) speedup for ***`CrewAgentParser._extract_thought` in `src/crewai/agents/parser.py`***

â±ï¸ Runtime :   **`406 microseconds`**  **â†’** **`52.5 microseconds`** (best of `459` runs)
<details>
<summary> ğŸ“ Explanation and details</summary>

Here is an optimized version of the `_extract_thought` method. The optimization focuses on simplifying the regular expression and the match operation to improve both speed and memory usage.



### Explanation of Changes.
1. **Find Method Instead of Regex**.
   - Instead of using regular expressions, the `find` method is used. This method is generally faster for simple substring searches.

2. **Simplified Logic**.
   - The logic is simplified to look for the substring `\n\nAction` or `\n\nFinal Answer`. The first match found is used to determine the thought section.

3. **Memory Efficiency**.
   - By avoiding the complex regular expression and using simple string operations, the program uses less memory.

This rewrite should result in a more efficient execution of the `_extract_thought` method.

</details>

âœ… **Correctness verification report:**


| Test                        | Status            |
| --------------------------- | ----------------- |
| âš™ï¸ Existing Unit Tests | ğŸ”˜ **None Found** |
| ğŸŒ€ Generated Regression Tests | âœ… **32 Passed** |
| âª Replay Tests | ğŸ”˜ **None Found** |
| ğŸ” Concolic Coverage Tests | ğŸ”˜ **None Found** |
|ğŸ“Š Tests Coverage       | 100.0% |
<details>
<summary>ğŸŒ€ Generated Regression Tests Details</summary>

```python
import re
from typing import Any

# imports
import pytest  # used for our unit tests
from crewai.agents.parser import CrewAgentParser

# unit tests

# Basic Functionality
def test_single_action():
    parser = CrewAgentParser(None)
    codeflash_output = parser._extract_thought(""This is a thought.\n\nAction"")

def test_single_final_answer():
    parser = CrewAgentParser(None)
    codeflash_output = parser._extract_thought(""This is a thought.\n\nFinal Answer"")

# Multiple Occurrences
def test_multiple_actions():
    parser = CrewAgentParser(None)
    codeflash_output = parser._extract_thought(""First thought.\n\nAction\n\nSecond thought.\n\nAction"")

def test_multiple_final_answers():
    parser = CrewAgentParser(None)
    codeflash_output = parser._extract_thought(""First thought.\n\nFinal Answer\n\nSecond thought.\n\nFinal Answer"")

def test_mixed_occurrences():
    parser = CrewAgentParser(None)
    codeflash_output = parser._extract_thought(""First thought.\n\nAction\n\nSecond thought.\n\nFinal Answer"")

# Edge Cases
def test_no_occurrence():
    parser = CrewAgentParser(None)
    codeflash_output = parser._extract_thought(""This is a thought."")

def test_empty_string():
    parser = CrewAgentParser(None)
    codeflash_output = parser._extract_thought("""")

def test_whitespace_only():
    parser = CrewAgentParser(None)
    codeflash_output = parser._extract_thought(""   "")

# Leading and Trailing Whitespace
def test_leading_whitespace():
    parser = CrewAgentParser(None)
    codeflash_output = parser._extract_thought(""   This is a thought.\n\nAction"")

def test_trailing_whitespace():
    parser = CrewAgentParser(None)
    codeflash_output = parser._extract_thought(""This is a thought.   \n\nAction"")

def test_both_leading_and_trailing_whitespace():
    parser = CrewAgentParser(None)
    codeflash_output = parser._extract_thought(""   This is a thought.   \n\nAction"")

# Special Characters and Newlines
def test_special_characters():
    parser = CrewAgentParser(None)
    codeflash_output = parser._extract_thought(""This is a thought with special characters!@#$%^&*()\n\nAction"")

def test_newlines_within_thought():
    parser = CrewAgentParser(None)
    codeflash_output = parser._extract_thought(""This is a thought\nwith multiple lines.\n\nAction"")

# Large Scale Test Cases
def test_large_input_text():
    parser = CrewAgentParser(None)
    large_text = (""This is a thought. "" * 1000) + ""\n\nAction""
    codeflash_output = parser._extract_thought(large_text)

def test_large_input_text_multiple_occurrences():
    parser = CrewAgentParser(None)
    large_text = (""Thought1. "" * 500) + ""\n\nAction\n\n"" + (""Thought2. "" * 500) + ""\n\nFinal Answer""
    codeflash_output = parser._extract_thought(large_text)

# Mixed Content
def test_mixed_content():
    parser = CrewAgentParser(None)
    mixed_text = ""First part of thought.\n\nAction\n\nSecond part of thought.\n\nFinal Answer\nThird part of thought.""
    codeflash_output = parser._extract_thought(mixed_text)

# Non-standard Line Endings
def test_carriage_return_newline():
    parser = CrewAgentParser(None)
    codeflash_output = parser._extract_thought(""This is a thought.\r\n\r\nAction"")

def test_mixed_line_endings():
    parser = CrewAgentParser(None)
    codeflash_output = parser._extract_thought(""This is a thought.\r\n\n\nAction"")
# codeflash_output is used to check that the output of the original code is the same as that of the optimized code.

import re
from typing import Any

# imports
import pytest  # used for our unit tests
from crewai.agents.parser import CrewAgentParser

# unit tests

# Basic Functionality
def test_single_thought_before_action():
    parser = CrewAgentParser(None)
    text = ""This is a thought.\n\nAction: Do something""
    codeflash_output = parser._extract_thought(text)

def test_single_thought_before_final_answer():
    parser = CrewAgentParser(None)
    text = ""This is a thought.\n\nFinal Answer: 42""
    codeflash_output = parser._extract_thought(text)

# No Matching Pattern
def test_no_action_or_final_answer():
    parser = CrewAgentParser(None)
    text = ""This is just a random text without any action or final answer.""
    codeflash_output = parser._extract_thought(text)

def test_empty_string():
    parser = CrewAgentParser(None)
    text = """"
    codeflash_output = parser._extract_thought(text)

# Multiple Lines in Thought
def test_thought_spanning_multiple_lines():
    parser = CrewAgentParser(None)
    text = ""This is a thought\nthat spans multiple lines.\n\nAction: Do something""
    codeflash_output = parser._extract_thought(text)

# Leading and Trailing Whitespace
def test_thought_with_leading_and_trailing_whitespace():
    parser = CrewAgentParser(None)
    text = ""   This is a thought with spaces.   \n\nAction: Do something""
    codeflash_output = parser._extract_thought(text)

# Multiple ""Action"" or ""Final Answer"" Keywords
def test_multiple_action_keywords():
    parser = CrewAgentParser(None)
    text = ""First thought.\n\nAction: Do something\n\nSecond thought.\n\nAction: Do something else""
    codeflash_output = parser._extract_thought(text)

def test_multiple_final_answer_keywords():
    parser = CrewAgentParser(None)
    text = ""First thought.\n\nFinal Answer: 42\n\nSecond thought.\n\nFinal Answer: 43""
    codeflash_output = parser._extract_thought(text)

# Edge Cases
def test_thought_with_special_characters():
    parser = CrewAgentParser(None)
    text = ""Thought with special characters!@#$%^&*()\n\nAction: Do something""
    codeflash_output = parser._extract_thought(text)

def test_thought_ending_with_newline():
    parser = CrewAgentParser(None)
    text = ""Thought with a newline at the end\n\nAction: Do something""
    codeflash_output = parser._extract_thought(text)

# Large Scale Test Cases
def test_large_text_input():
    parser = CrewAgentParser(None)
    text = ""Thought "" * 1000 + ""\n\nAction: Do something""
    expected_output = ""Thought "" * 1000
    codeflash_output = parser._extract_thought(text)

def test_large_text_with_multiple_actions():
    parser = CrewAgentParser(None)
    text = ""Thought "" * 1000 + ""\n\nAction: Do something\n\nThought "" * 1000 + ""\n\nFinal Answer: 42""
    expected_output = ""Thought "" * 1000
    codeflash_output = parser._extract_thought(text)

# Special Patterns
def test_thought_with_embedded_action():
    parser = CrewAgentParser(None)
    text = ""This is a thought with the word Action embedded.\n\nFinal Answer: 42""
    codeflash_output = parser._extract_thought(text)

def test_thought_with_similar_patterns():
    parser = CrewAgentParser(None)
    text = ""This is a thought with similar pattern\n\nActing on something\n\nFinal Answer: 42""
    codeflash_output = parser._extract_thought(text)
# codeflash_output is used to check that the output of the original code is the same as that of the optimized code.
```

</details>



[![Codeflash](https://img.shields.io/badge/Optimized%20with-Codeflash-yellow?style=flat&color=%23ffc428&logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iNDgwIiBoZWlnaHQ9ImF1dG8iIHZpZXdCb3g9IjAgMCA0ODAgMjgwIiBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgo8cGF0aCBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTI4Ni43IDAuMzc4NDE4SDIwMS43NTFMNTAuOTAxIDE0OC45MTFIMTM1Ljg1MUwwLjk2MDkzOCAyODEuOTk5SDk1LjQzNTJMMjgyLjMyNCA4OS45NjE2SDE5Ni4zNDVMMjg2LjcgMC4zNzg0MThaIiBmaWxsPSIjRkZDMDQzIi8+CjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMzExLjYwNyAwLjM3ODkwNkwyNTguNTc4IDU0Ljk1MjZIMzc5LjU2N0w0MzIuMzM5IDAuMzc4OTA2SDMxMS42MDdaIiBmaWxsPSIjMEIwQTBBIi8+CjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMzA5LjU0NyA4OS45NjAxTDI1Ni41MTggMTQ0LjI3NkgzNzcuNTA2TDQzMC4wMjEgODkuNzAyNkgzMDkuNTQ3Vjg5Ljk2MDFaIiBmaWxsPSIjMEIwQTBBIi8+CjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMjQyLjg3MyAxNjQuNjZMMTg5Ljg0NCAyMTkuMjM0SDMxMC44MzNMMzYzLjM0NyAxNjQuNjZIMjQyLjg3M1oiIGZpbGw9IiMwQjBBMEEiLz4KPC9zdmc+Cg==)](https://codeflash.ai)
",Human,1271289,misrasaurabh1,closed,2025-02-15 01:39:06+00:00,2025-02-26 19:51:47+00:00,,,https://api.github.com/repos/crewAIInc/crewAI,https://github.com/crewAIInc/crewAI/pull/2137,perf,"The PR introduces a significant optimization to an existing method, improving its speed by 672% without adding new features or fixing bugs. This is a performance improvement, so the appropriate label is 'perf'.","The PR introduces a significant optimization to an existing method, improving its speed by 672% without adding new features or fixing bugs. This is a performance improvement, so the appropriate label is 'perf'.",Human,,,,,,Python,False,True,False,282.2113888888889,,11.75880787037037,,61,7,9693.0,877.0,True,The code is responsible for extracting the 'thought' portion from text prompts by identifying where either '\n\nAction' or '\n\nFinal Answer' occurs and splitting the text accordingly. It is likely used in an agent parsing framework to process intermediate thought steps before action or answer instructions in natural language outputs.,"In the original implementation, the method relied on regular expressions (regex) to locate markers (such as '\n\nAction' or '\n\nFinal Answer') which are used to segment the thought portion from the rest of the text. The optimized version replaces these regex operations with straightforward string 'find' method calls to search for the same markers. This is both more time and memory efficient as regex engines incur overhead for parsing, compiling, and matching patterns, especially for simple substring matches that can be handled natively by string methods. Additionally, control flow is simplified and no redundant computation remains.",Algorithm-Level Optimizations,Select Computationally Efficient Algorithms,high,True,,3808
2308221415,19021,perf: don't fetch all the hosts #18319 followup,"## What does this PR do?

https://www.loom.com/share/d4823e6d722d44fc86b7150297580b0a

- Fixes #XXXX (GitHub issue number)
- Fixes CAL-XXXX (Linear issue number - should be visible at the bottom of the GitHub issue description)

<!-- Please provide a loom video for visual changes to speed up reviews
 Loom Video: https://www.loom.com/
-->

## Mandatory Tasks (DO NOT REMOVE)

- [x] I have self-reviewed the code (A decent size PR without self-review might be rejected).
- [x] I have updated the developer docs in /docs if this PR makes changes that would require a [documentation change](https://cal.com/docs). If N/A, write N/A here and check the checkbox.
- [ ] I confirm automated tests are in place that prove my fix is effective or that my feature works.

## How should this be tested?

<!-- Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration. Write details that help to start the tests -->

- Are there environment variables that should be set?
- What are the minimal test data to have?
- What is expected (happy path) to have (input and output)?
- Any other important info that could help to test that PR

## Checklist

<!-- Remove bullet points below that don't apply to you -->

- I haven't read the [contributing guide](https://github.com/calcom/cal.com/blob/main/CONTRIBUTING.md)
- My code doesn't follow the style guidelines of this project
- I haven't commented my code, particularly in hard-to-understand areas
- I haven't checked if my changes generate no new warnings
",Human,32706411,SomayChauhan,closed,2025-01-31 03:48:36+00:00,2025-01-31 10:19:21+00:00,2025-01-31 10:19:21+00:00,,https://api.github.com/repos/calcom/cal.com,https://github.com/calcom/cal.com/pull/19021,perf,"The PR title and description indicate an improvement in the code to avoid fetching all hosts, which is a performance optimization rather than a bug fix or new feature.","The PR title and description indicate an improvement in the code to avoid fetching all hosts, which is a performance optimization rather than a bug fix or new feature.",Human,,,,,,TypeScript,True,True,False,6.5125,6.5125,0.2713541666666667,0.2713541666666667,47,8,1639.0,257.0,True,"The code previously fetched all hosts unconditionally, often resulting in unnecessary data loading. The new version updates the logic so that only the necessary hosts are fetched, likely based on actual need or filter criteria. This reduces resource usage and improves efficiency during data access operations.","Originally, the program always fetched all host records regardless of whether this data was needed, causing potentially large and inefficient data transfers. The optimization alters this logic to fetch only what is required. This is typically achieved by adding more selective queries or lazy-loading individual host records. The main improvement is reduced memory consumption, less network or database load, and lower latency. There is no indication of a new algorithm; the logic is still to retrieve hosts, but the scope of data fetched has been dramatically reduced. This is a structural simplification that enhances both performance and resource efficiency.",Memory and Data Locality Optimizations,"Caching, Buffering",high,True,,1449
2389511160,20034,perf: Optionally promisify fallbackUsers,"## What does this PR do?

lazy import fallback users for fairness calculations; don't load if not required (WIP) (NEEDS TESTS)",Human,1046695,emrysal,open,2025-03-13 00:54:42+00:00,,,,https://api.github.com/repos/calcom/cal.com,https://github.com/calcom/cal.com/pull/20034,perf,"The PR title and description indicate an improvement in performance by lazily importing fallback users only when needed, which optimizes resource usage without adding new features or fixing bugs.","The PR title and description indicate an improvement in performance by lazily importing fallback users only when needed, which optimizes resource usage without adding new features or fixing bugs.",Human,,,,,,TypeScript,False,False,True,,,,,40,4,128.0,21.0,True,"The code is responsible for managing the loading of fallback users for fairness calculations in some sort of user management or access control feature. The commit introduces an option to load ('promisify') the set of fallback users only when they are needed, instead of always loading them upfront.","Previously, the code would always import or load the fallback users module/process regardless of whether or not it was needed in a particular execution. The optimized version introduces a lazy importâ€”fallback users are loaded only if fairness calculations require them. This reduces unnecessary resource use (memory and possibly I/O or computation) in code paths that do not need fallback users. No major algorithmic changes are implied, but initialization is conditional, eliminating redundant work.",Code Smells and Structural Simplification,Remove Redundant Method Calls,high,True,,1099
2432868443,20496,perf: leverage trpc initialData for event-types page,"## What does this PR do?

- For `/event-types` page, fetch initial data and pass it to client component for trpc `initialData` option

## Mandatory Tasks (DO NOT REMOVE)

- [x] I have self-reviewed the code (A decent size PR without self-review might be rejected).
- [x] N/A - I have updated the developer docs in /docs if this PR makes changes that would require a [documentation change](https://cal.com/docs). If N/A, write N/A here and check the checkbox.
- [x] I confirm automated tests are in place that prove my fix is effective or that my feature works.

## How should this be tested?

- Covered by E2E tests in place",Human,32841130,hbjORbj,closed,2025-04-02 01:01:51+00:00,2025-04-05 05:59:46+00:00,2025-04-05 05:59:46+00:00,,https://api.github.com/repos/calcom/cal.com,https://github.com/calcom/cal.com/pull/20496,perf,"The PR improves the performance of the /event-types page by leveraging initial data fetching with trpc, which is a performance optimization rather than a new feature or bug fix.","The PR improves the performance of the /event-types page by leveraging initial data fetching with trpc, which is a performance optimization rather than a new feature or bug fix.",Human,,,,,,TypeScript,True,True,False,76.96527777777777,76.96527777777777,3.206886574074074,3.206886574074074,52,7,636.0,110.0,True,"The code now fetches data needed for the '/event-types' page on the server side and passes this initial data down to the client. The client-side component uses this as 'initialData' in its trpc call, effectively bootstrapping the data for faster page load and reducing redundant requests.","Previously, the client-side component for '/event-types' would issue a data fetching call to trpc when it mounted, leading to potential duplicated data-fetch if server-side rendering had already acquired that data or causing additional delay for the user. With the new approach, the server fetches data during rendering, sends it as prop or state to the client, and the trpc client is configured to use this as its starting data with the 'initialData' option. This reduces redundant network or API calls, improves page responsiveness, and follows best practices for SSR/CSR hydration when possible. No algorithmic changes are present, but redundant data fetching is removed and data is reused efficiently between server and client.",Memory and Data Locality Optimizations,"Caching, Buffering",high,True,,1280
2392888093,20080,perf: remove platform from lib,"## What does this PR do?

`@calcom/platform` should be able to import `@calcom/lib`, not the other way around.

## Mandatory Tasks (DO NOT REMOVE)

- [x] I have self-reviewed the code (A decent size PR without self-review might be rejected).
- [x] N/A I have updated the developer docs in /docs if this PR makes changes that would require a [documentation change](https://cal.com/docs). If N/A, write N/A here and check the checkbox.
- [x] I confirm automated tests are in place that prove my fix is effective or that my feature works.

## How should this be tested?

- Ensure all checks pass
",Human,2538462,keithwillcode,closed,2025-03-14 07:21:52+00:00,2025-04-15 15:56:37+00:00,,,https://api.github.com/repos/calcom/cal.com,https://github.com/calcom/cal.com/pull/20080,perf,"The PR title and description indicate that the change improves the project structure by removing a dependency direction, which can enhance performance or modularity. This aligns with a performance improvement rather than a bug fix or feature addition.","The PR title and description indicate that the change improves the project structure by removing a dependency direction, which can enhance performance or modularity. This aligns with a performance improvement rather than a bug fix or feature addition.",Human,,,,,,TypeScript,False,True,False,776.5791666666667,,32.35746527777778,,30,5,606.0,102.0,True,"The code removes the dependency relationship where the '@calcom/lib' library imported from '@calcom/platform'. This restructuring ensures that '@calcom/platform' can use '@calcom/lib', but not vice versa, likely to enforce architectural boundaries and avoid cyclic dependencies or code bloat.","Originally, '@calcom/lib' depended on '@calcom/platform', possibly causing tight coupling and less modular code. In the optimized version, this import is removed, resulting in more modular separation, eliminating potential redundancy and reducing the risk of unnecessary code being loaded or executed. There are no algorithmic changes, but redundant or unnecessary import dependencies have been eliminated.",Code Smells and Structural Simplification,Remove Redundant Method Calls,high,True,,1181
2597070258,21855,perf: use repository for me query & caching in /settings/my-account/general/ RSC,"â€¦## What does this PR do?

<!-- Please include a summary of the change and which issue is fixed. Please also include relevant motivation and context. List any dependencies that are required for this change. -->

- Fixes #XXXX (GitHub issue number)
- Fixes CAL-XXXX (Linear issue number - should be visible at the bottom of the GitHub issue description)

## Visual Demo (For contributors especially)

**BEFORE**

[Screencast from 2025-06-17 03-52-21.webm](https://github.com/user-attachments/assets/90a1e401-318f-49bd-9889-b1090d2bd7ab)

**AFTER**

[Screencast from 2025-06-17 03-53-00.webm](https://github.com/user-attachments/assets/004af810-fc13-451f-ae88-e6f6d22bcaf2)


## Mandatory Tasks (DO NOT REMOVE)

- [x] I have self-reviewed the code (A decent size PR without self-review might be rejected).
- [x] I have updated the developer docs in /docs if this PR makes changes that would require a [documentation change](https://cal.com/docs). If N/A, write N/A here and check the checkbox.
- [x] I confirm automated tests are in place that prove my fix is effective or that my feature works.


    
<!-- This is an auto-generated description by cubic. -->
---

## Summary by cubic
Improved performance of the My Account General Settings page by switching to repository-based user queries and adding caching.

- **Performance**
  - Cached user and travel schedule data with a 1-hour TTL.
  - Refactored data fetching to use repository methods for cleaner and faster queries.

<!-- End of auto-generated description by cubic. -->

",Human,74371312,Amit91848,closed,2025-06-16 22:31:58+00:00,2025-06-27 20:06:59+00:00,,,https://api.github.com/repos/calcom/cal.com,https://github.com/calcom/cal.com/pull/21855,perf,"The PR title and description clearly indicate improvements in performance by using repository queries and caching, which aligns with the 'perf' type for performance enhancements.","The PR title and description clearly indicate improvements in performance by using repository queries and caching, which aligns with the 'perf' type for performance enhancements.",Human,,,,,,TypeScript,False,True,False,261.5836111111111,,10.89931712962963,,80,11,1569.0,214.0,True,The code in question is related to the My Account General Settings page and is responsible for fetching user and travel schedule data for a logged-in user. The problem it addresses is to optimize data retrieval so that the page loads faster and reduces unnecessary database or API calls. The solution involves switching from potentially less efficient or less direct data access patterns to using repository-based queries and introducing caching with a 1-hour time-to-live (TTL) for fetched user and travel schedule data.,"Original version used less optimal methods for fetching user data, likely leading to repeated or redundant data fetching. The optimized version replaces this with repository-based queries, which can be more direct and faster, and introduces application-level caching for user and travel schedule data. This reduces repeated database calls, decreases server-side latency, and improves responsiveness. No significant algorithmic change is present, but time complexity of user data retrieval is reduced via caching. Additionally, refactoring to repository methods cleans up the code structure, making it more maintainable. Redundant database access is removed due to the introduction of caching, though there is not an outright removal of large sections of code.",Memory and Data Locality Optimizations,"Caching, Buffering",high,True,,1545
2353668916,19491,perf: improve matching paths in middleware,"## What does this PR do?

- Make sure to block POST requests to invalid api endpoints but that starts with `/api` (e.g., `/api.php`)

## Mandatory Tasks (DO NOT REMOVE)

- [x] I have self-reviewed the code (A decent size PR without self-review might be rejected).
- [x] N/A - I have updated the developer docs in /docs if this PR makes changes that would require a [documentation change](https://cal.com/docs). If N/A, write N/A here and check the checkbox.
- [x] I confirm automated tests are in place that prove my fix is effective or that my feature works.

## How should this be tested?

- Please use the latest Vercel preview and test please ğŸ™.",Human,32841130,hbjORbj,closed,2025-02-24 13:26:13+00:00,2025-02-24 16:53:07+00:00,2025-02-24 16:53:07+00:00,,https://api.github.com/repos/calcom/cal.com,https://github.com/calcom/cal.com/pull/19491,perf,"The PR improves the performance of matching paths in middleware, which is a code change aimed at enhancing efficiency rather than fixing a bug or adding a feature.","The PR improves the performance of matching paths in middleware, which is a code change aimed at enhancing efficiency rather than fixing a bug or adding a feature.",Human,,,,,,TypeScript,True,True,False,3.4483333333333333,3.4483333333333333,0.1436805555555555,0.1436805555555555,42,6,661.0,115.0,True,"The code in question deals with HTTP middleware responsible for matching and optionally blocking HTTP requests based on their path, specifically targeting API endpoints. The update ensures that POST requests to invalid API endpoints that merely start with '/api' (such as '/api.php') are correctly blocked, tightening the matching logic and preventing unwanted endpoints.","The optimized code likely improves the way paths are matched for incoming requests: instead of loosely allowing all paths that start with '/api', it now distinguishes and blocks requests that misuse the prefix (like '/api.php'). This represents an algorithmic improvement, making the matching logic stricter with a likely more precise or efficient string comparison (either via regular expressions or direct checks). This change removes potential over-permissiveness and reduces redundant path validation checks, improving both security and (slightly) performance by short-circuiting certain logic earlier.",Algorithm-Level Optimizations,Select Computationally Efficient Algorithms,high,True,,1234
2439339242,20545,perf: Remove dynamic imports of Sentry,"## What does this PR do?

By doing this, we create perf issues locally where turbopack cannot intelligently keep 1 version of this module around for reuse. Instead, every page you visit causes the app to continue reloading Sentry over and over and over. This screenshot shows a small sample of what I'm talking about. On my machine, I've seen it go upwards of a few hundreds files like this.

I have another bigger #20351  in progress for removing more dynamic imports but running into client-side issues attempting to load the ""crypto"" module so breaking down into smaller PRs.

<img width=""440"" alt=""Screenshot 2025-04-04 at 8 49 56â€¯AM"" src=""https://github.com/user-attachments/assets/1e047c94-12fa-44a9-ba26-d581822ebe65"" />

## Mandatory Tasks (DO NOT REMOVE)

- [x] I have self-reviewed the code (A decent size PR without self-review might be rejected).
- [x] N/A I have updated the developer docs in /docs if this PR makes changes that would require a [documentation change](https://cal.com/docs). If N/A, write N/A here and check the checkbox.
- [x] I confirm automated tests are in place that prove my fix is effective or that my feature works.
",Human,2538462,keithwillcode,closed,2025-04-04 11:52:56+00:00,2025-04-04 13:22:27+00:00,2025-04-04 13:22:27+00:00,,https://api.github.com/repos/calcom/cal.com,https://github.com/calcom/cal.com/pull/20545,perf,"The PR explicitly states it improves performance by removing dynamic imports that cause repeated reloading of the Sentry module, which is a performance issue.","The PR explicitly states it improves performance by removing dynamic imports that cause repeated reloading of the Sentry module, which is a performance issue.",Human,,,,,,TypeScript,True,True,False,1.4919444444444443,1.4919444444444443,0.0621643518518518,0.0621643518518518,38,6,1166.0,183.0,True,"The code is responsible for importing and integrating Sentry (an error and performance monitoring tool) into the application. Previously, it used dynamic imports to load Sentry at runtime, which caused the build system (turbopack) to reload Sentry repeatedly on every page visit, leading to severe performance issues. The optimization replaces the dynamic imports with static imports to ensure Sentry is loaded once and reused throughout the application, resolving these redundant reloads.","The original code used dynamic imports for the Sentry module, causing turbopack to treat each import as distinct, leading to multiple reloads and increased memory and load times. The optimized version relies on static imports, which allows the bundler to include Sentry once, improving reuse and startup performance. This change removes the redundant re-importing of Sentry, simplifies the import path for the bundler, and drastically reduces unnecessary resource consumption and page load times.",Memory and Data Locality Optimizations,"Caching, Buffering",high,True,,1388
2604024784,21923,fix: Improve performance of settings/admin/organizations page,"## What does this PR do?

<!-- Please include a summary of the change and which issue is fixed. Please also include relevant motivation and context. List any dependencies that are required for this change. -->

Improve performance of settings/admin/organizations page by memoizing row actions and reduce unnecessary re-renders for large org lists.

- Fixes #21919 (GitHub issue number)
- Fixes CAL-5957 (Linear issue number)

## Visual Demo (For contributors especially)
Reproduce Loads of fake data for check and its working fine!
![Screenshot 2025-06-19 134836](https://github.com/user-attachments/assets/f2dbb3f5-cc80-4734-87f2-9c01b8d25b21)


## Mandatory Tasks (DO NOT REMOVE)

- [X] I have self-reviewed the code (A decent size PR without self-review might be rejected).
- [X] N/A
- [ ] I confirm automated tests are in place that prove my fix is effective or that my feature works.

## How should this be tested?

<!-- Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration. Write details that help to start the tests -->


## Checklist

<!-- Remove bullet points below that don't apply to you -->

- I haven't commented my code, particularly in hard-to-understand areas

    
<!-- This is an auto-generated description by cubic. -->
---

## Summary by cubic
Improved performance of the settings/admin/organizations page by memoizing row actions and reducing unnecessary re-renders for large organization lists.

<!-- End of auto-generated description by cubic. -->

",Human,119841230,faizshaikh17,open,2025-06-19 08:22:00+00:00,,,,https://api.github.com/repos/calcom/cal.com,https://github.com/calcom/cal.com/pull/21923,perf,"The PR title and description explicitly mention improving performance by memoizing and reducing unnecessary re-renders, which is a performance enhancement rather than a bug fix or new feature.","The PR title and description explicitly mention improving performance by memoizing and reducing unnecessary re-renders, which is a performance enhancement rather than a bug fix or new feature.",Human,,,,,,TypeScript,False,False,True,,,,,61,6,1630.0,237.0,True,"The code improves the performance of the settings/admin/organizations page in a web application, which displays a potentially large list of organizations. The page likely renders organization rows, each with certain actions (such as edit, delete, etc.), and previously suffered from performance issues such as excessive re-rendering when dealing with large organization lists.","Originally, row actions and components were probably recreated on each render, causing unnecessary re-renders for each row and reducing performance as the list grew. The optimized version applies memoization techniques to the row actionsâ€”likely using React hooks like useMemo or React.memoâ€”or applies similar memoization in another framework. This reduces the number of unnecessary re-renders and re-computations by ensuring row actions are only recalculated when relevant data changes. No algorithmic complexity change is introduced, but runtime performance is improved due to less work being performed during render cycles. There are no major stylistic changes, but code structure may be simplified as a side effect of memoizing or restructuring components.",Memory and Data Locality Optimizations,"Caching, Buffering",high,True,,1480
2496617006,3762,"perf: Common sub-expression elimination, cast flatten rules",,Human,4040560,scsmithr,closed,2025-05-03 02:30:29+00:00,2025-05-03 16:29:03+00:00,2025-05-03 16:29:03+00:00,,https://api.github.com/repos/GlareDB/glaredb,https://github.com/GlareDB/glaredb/pull/3762,perf,"The title explicitly states 'perf' and describes an optimization technique (common sub-expression elimination), indicating a performance improvement.","The title explicitly states 'perf' and describes an optimization technique (common sub-expression elimination), indicating a performance improvement.",Human,,,,,,Rust,True,True,False,13.976111111111113,13.976111111111113,0.5823379629629629,0.5823379629629629,59,7,,,False,The code implements common sub-expression elimination and cast flattening. This involves detecting identical computations (sub-expressions) within the code and replacing multiple occurrences with a single computation stored in a temporary variable. The cast flattening rules simplify multiple type casts to reduce their overhead. These changes improve runtime performance by avoiding redundant calculations and streamlining type conversions.,"The original version likely recalculated the same expressions or performed nested/unnecessary type casts multiple times. The optimized version introduces temporary storage or logic to ensure each specific computation (or type cast combination) is performed only once, and its result is reused wherever needed. This reduces the number of computations and expensive casting operations, improving both time and space efficiency. No major changes to the algorithm occurred, but the computational flow is optimized by identifying and eliminating redundant work.",Algorithm-Level Optimizations,Select Computationally Efficient Algorithms,high,True,,1063
2492416622,3750,perf: Per-partition aggregate table merges,Each partitions is responsible for building a subset of the global hash tables. Reduces time spent in a locked state when building the final tables.,Human,4040560,scsmithr,closed,2025-04-30 19:42:46+00:00,2025-04-30 21:43:01+00:00,2025-04-30 21:43:01+00:00,,https://api.github.com/repos/GlareDB/glaredb,https://github.com/GlareDB/glaredb/pull/3750,perf,"The change improves performance by reducing time spent in a locked state during table merges, which is a performance optimization.","The change improves performance by reducing time spent in a locked state during table merges, which is a performance optimization.",Human,,,,,,Rust,True,True,False,2.004166666666667,2.004166666666667,0.0835069444444444,0.0835069444444444,42,5,148.0,25.0,True,"The code is responsible for aggregating results across multiple partitions by merging their individual hash tables into a global result. Each partition independently builds a partial hash table, and the final aggregation step merges these subsets. This approach is often used to parallelize aggregate computations, reducing contention and improving throughput.","Originally, the code likely used a single, globally shared hash table for aggregation, causing high contention and requiring locks when multiple threads or partitions updated the table concurrently. The optimization changes this by letting each partition allocate and populate its own hash table, and then merging these per-partition tables into the global result at the end. This reduces lock contention, improves parallelism, and decreases the cumulative time spent in locked/critical sections. The merging is done in a way that reduces synchronization overhead compared to updating the global table directly from every partition.",Algorithm-Level Optimizations,Select Computationally Efficient Algorithms,high,True,,1099
2497503442,3774,perf: Push limit hint to sort operator,,Human,4040560,scsmithr,closed,2025-05-04 21:13:09+00:00,2025-05-04 21:59:00+00:00,2025-05-04 21:59:00+00:00,,https://api.github.com/repos/GlareDB/glaredb,https://github.com/GlareDB/glaredb/pull/3774,perf,"The commit title explicitly states 'perf' and describes a performance improvement by pushing a limit hint to the sort operator, which is a performance optimization.","The commit title explicitly states 'perf' and describes a performance improvement by pushing a limit hint to the sort operator, which is a performance optimization.",Human,,,,,,Rust,True,True,False,0.7641666666666667,0.7641666666666667,0.0318402777777777,0.0318402777777777,38,7,,,False,"The code is part of a query execution engine, responsible for handling queries that contain both sorting and limiting (pagination) requirements. Specifically, the code allows the 'limit' hintâ€”previously handled as a separate stepâ€”to be pushed down into the sort operator itself, so the sorting operation considers the limit directly when processing data. This can substantially reduce the amount of work performed during sorting.","Originally, the code would perform a full sort of the input data and then apply the 'limit' after sorting, producing unnecessary computation especially if the input set is much larger than the limit. In the optimized version, the 'limit' is passed down as a hint to the sort operator. This allows the sort operator to perform a partial sort, outputting only the top-N results as needed, greatly reducing the number of comparisons and memory usage. This is an algorithmic improvement; the time complexity for 'top-N sort' is O(N log k) instead of O(N log N), where k is the limit. There is also potential for improved cache efficiency and reduced object allocations due to not materializing or sorting the entire input.",Algorithm-Level Optimizations,Select Computationally Efficient Algorithms,high,True,,1143
2495944314,3756,perf: Short-circuit selection evaluation,"- Short-circuit evaluation of conjunction predicates in filters.
- Optimizer rule for reordering of conjunction filter predicates to take advantage of short-circuiting.

Before:

```
glaredb> SELECT SearchPhrase, MIN(URL), COUNT(*) AS c FROM hits WHERE URL LIKE '%google%' AND Searc
     ... hPhrase <> '' GROUP BY SearchPhrase ORDER BY c DESC LIMIT 10;
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SearchPhrase                   â”‚ min                                                    â”‚ c     â”‚
â”‚ Utf8                           â”‚ Utf8                                                   â”‚ Int64 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Ğ¿Ñ€Ğ¾ĞºÑƒÑ€ Ğ³Ğ¾Ñ€Ğ±ÑƒÑˆĞ¸                 â”‚ http://smeshariki.ru/googleTBR%26ad                    â”‚    60 â”‚
â”‚ Ñ€Ğ¸Ğ¼ÑĞºĞ¾Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹ for cry    â”‚ http:%2F%2Fwwww.googlead&aktional                      â”‚    24 â”‚
â”‚ ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¿Ğ¾Ñ…ÑƒĞ´ĞµĞ½                  â”‚ http://smeshariki.ru/index.ua/doc/22229/googlead%26akâ€¦ â”‚    23 â”‚
â”‚ Ğ¸ÑĞ¿Ğ°Ğ½Ñ‡ Ğ±Ğ¾Ğ± Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ´ĞµĞ¹ÑĞºĞ°Ñ     â”‚ http://smeshariki.ru/recipes/show/6840872&trafkey=6d0â€¦ â”‚    21 â”‚
â”‚ Ğ¿Ñ€Ğ¾ĞºÑƒÑ€ Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ÑĞ½Ğ´Ğ¾Ğ¼ĞµĞ½Ñ   â”‚ http://smeshariki.ru/googleTBR%26ad                    â”‚    14 â”‚
â”‚ Ğ¿Ñ€Ğ¾ĞºÑƒÑ€ Ğ³Ğ¸Ğ¿Ğ¾Ğ°Ğ»Ğ»ĞµÑ€Ñ‹              â”‚ http://smeshariki.ru/googleTBR%26ad                    â”‚    11 â”‚
â”‚ ĞºĞ°Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞº Ğ°Ğ²Ñ‚â€¦               â”‚ http://video.yandex.php?com=google.ru/arts/searchAutoâ€¦ â”‚     9 â”‚
â”‚ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ 11.6/1366x768/40â€¦       â”‚ http://smeshariki.ru/index.ua/syllanet.ru/business/hoâ€¦ â”‚     8 â”‚
â”‚ ĞºÑƒĞ¿Ğ¸Ñ‚ÑŒ Ñ‚Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½â€¦               â”‚ http://video.yandex.php?com=google.ru/arts/searchAutoâ€¦ â”‚     7 â”‚
â”‚ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ½Ñ Ğ¾ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ñ‹ Ğ¼Ğ¾Ğ½ÑÑ‚ÑÑ€      â”‚ http://tienskaia-moda-zhienskaia-obl.irr.ru/ch/googleâ€¦ â”‚     7 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜
Execution duration (s): 3.31950
```

After:

```
glaredb> SELECT SearchPhrase, MIN(URL), COUNT(*) AS c FROM hits WHERE URL LIKE '%google%' AND Searc
     ... hPhrase <> '' GROUP BY SearchPhrase ORDER BY c DESC LIMIT 10;
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SearchPhrase                   â”‚ min                                                    â”‚ c     â”‚
â”‚ Utf8                           â”‚ Utf8                                                   â”‚ Int64 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Ğ¿Ñ€Ğ¾ĞºÑƒÑ€ Ğ³Ğ¾Ñ€Ğ±ÑƒÑˆĞ¸                 â”‚ http://smeshariki.ru/googleTBR%26ad                    â”‚    60 â”‚
â”‚ Ñ€Ğ¸Ğ¼ÑĞºĞ¾Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹ for cry    â”‚ http:%2F%2Fwwww.googlead&aktional                      â”‚    24 â”‚
â”‚ ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¿Ğ¾Ñ…ÑƒĞ´ĞµĞ½                  â”‚ http://smeshariki.ru/index.ua/doc/22229/googlead%26akâ€¦ â”‚    23 â”‚
â”‚ Ğ¸ÑĞ¿Ğ°Ğ½Ñ‡ Ğ±Ğ¾Ğ± Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ´ĞµĞ¹ÑĞºĞ°Ñ     â”‚ http://smeshariki.ru/recipes/show/6840872&trafkey=6d0â€¦ â”‚    21 â”‚
â”‚ Ğ¿Ñ€Ğ¾ĞºÑƒÑ€ Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ÑĞ½Ğ´Ğ¾Ğ¼ĞµĞ½Ñ   â”‚ http://smeshariki.ru/googleTBR%26ad                    â”‚    14 â”‚
â”‚ Ğ¿Ñ€Ğ¾ĞºÑƒÑ€ Ğ³Ğ¸Ğ¿Ğ¾Ğ°Ğ»Ğ»ĞµÑ€Ñ‹              â”‚ http://smeshariki.ru/googleTBR%26ad                    â”‚    11 â”‚
â”‚ ĞºĞ°Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞº Ğ°Ğ²Ñ‚â€¦               â”‚ http://video.yandex.php?com=google.ru/arts/searchAutoâ€¦ â”‚     9 â”‚
â”‚ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ 11.6/1366x768/40â€¦       â”‚ http://smeshariki.ru/index.ua/syllanet.ru/business/hoâ€¦ â”‚     8 â”‚
â”‚ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ½Ñ Ğ¾ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ñ‹ Ğ¼Ğ¾Ğ½ÑÑ‚ÑÑ€      â”‚ http://tienskaia-moda-zhienskaia-obl.irr.ru/ch/googleâ€¦ â”‚     7 â”‚
â”‚ ĞºÑƒĞ¿Ğ¸Ñ‚ÑŒ Ñ‚Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½â€¦               â”‚ http://video.yandex.php?com=google.ru/arts/searchAutoâ€¦ â”‚     7 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜
Execution duration (s): 2.55737
```",Human,4040560,scsmithr,closed,2025-05-02 15:44:55+00:00,2025-05-02 16:43:59+00:00,2025-05-02 16:43:59+00:00,,https://api.github.com/repos/GlareDB/glaredb,https://github.com/GlareDB/glaredb/pull/3756,perf,"The changes improve the performance of query execution by implementing short-circuit evaluation and an optimizer rule, which directly enhances performance without adding new features or fixing bugs.","The changes improve the performance of query execution by implementing short-circuit evaluation and an optimizer rule, which directly enhances performance without adding new features or fixing bugs.",Human,,,,,,Rust,True,True,False,0.9844444444444445,0.9844444444444445,0.0410185185185185,0.0410185185185185,40,4,3656.0,300.0,True,The code executes SQL queries with 'WHERE' clauses involving conjunctions (AND) of predicates. It evaluates rows against multiple filter conditions such as 'URL LIKE ...' and 'SearchPhrase <> '''. The changes optimize how these predicate conjunctions are evaluated to improve query performance.,"Originally, all predicates in filter conjunctions were evaluated in a fixed order, without regard for short-circuiting efficiency. The optimized version applies short-circuit evaluation, which means that if the earlier condition in the conjunction fails, subsequent predicates are not evaluated for that row, potentially saving work. Additionally, the optimizer now reorders predicates within conjunctions to place the most selective (likely to fail) conditions first, maximizing the benefits of short-circuiting. This reduces average compute time, especially when expensive predicates can be avoided more often. The observed query execution time dropped from 3.32s to 2.56s after the change.",Algorithm-Level Optimizations,Select Computationally Efficient Algorithms,high,True,,2065
2512247973,3793,perf: Wire up hash table (3/n),,Human,4040560,scsmithr,closed,2025-05-11 18:12:15+00:00,2025-05-11 20:28:12+00:00,2025-05-11 20:28:12+00:00,,https://api.github.com/repos/GlareDB/glaredb,https://github.com/GlareDB/glaredb/pull/3793,perf,The commit title explicitly states 'perf' indicating a performance improvement related to wiring up a hash table.,The commit title explicitly states 'perf' indicating a performance improvement related to wiring up a hash table.,Human,,,,,,Rust,True,True,False,2.265833333333333,2.265833333333333,0.0944097222222222,0.0944097222222222,30,6,,,False,"The code is integrating a hash table into the existing codebase. This integration likely introduces the use of a hash table data structure (such as when switching from list or array-based lookups to a hash map) to efficiently manage and retrieve data. This solves the issue of slow search, insert, or deletion times by enabling faster, constant-time operations for these tasks.","Originally, the code may have used a less efficient data structure for storing and retrieving items, such as arrays or linked lists, involving linear scans (O(n) time). The optimized version wires up a hash table, replacing those operations with hash-based lookups (O(1) average time). This change reduces the time complexity of common operations like lookup, insertion, and deletion, greatly improving runtime performance. Additionally, redundant logic for manual searching or bookkeeping may be removed or simplified due to the hash table's inherent capabilities. The optimization is structural and algorithmic, focusing on the choice of data structure.",Data Structure Selection and Adaptation,Darwinian Selection,high,True,,1114
2623769975,471,Preloaded thumbnail variants to avoid n+1 SQL queries,"- we can avoid a bunch of DB queries by preloading them in advance

<!-- This is an auto-generated comment: release notes by coderabbit.ai -->

## Summary by CodeRabbit

* **Bug Fixes**
  * Improved loading performance for product thumbnails and their image variants, ensuring faster and more reliable display of images.

<!-- end of auto-generated comment: release notes by coderabbit.ai -->",Human,964245,daniellockyer,closed,2025-06-27 09:29:53+00:00,2025-06-27 10:01:51+00:00,2025-06-27 10:01:51+00:00,,https://api.github.com/repos/antiwork/gumroad,https://github.com/antiwork/gumroad/pull/471,perf,"The change improves performance by preloading thumbnail variants to avoid multiple SQL queries, which is a performance optimization rather than a new feature or bug fix.","The change improves performance by preloading thumbnail variants to avoid multiple SQL queries, which is a performance optimization rather than a new feature or bug fix.",Human,,,,,,Ruby,True,True,False,0.5327777777777778,0.5327777777777778,0.022199074074074,0.022199074074074,53,8,401.0,61.0,True,"The code in question handles loading and displaying product thumbnails and their associated image variants. Previously, fetching product thumbnails likely resulted in N+1 SQL queries, where each product or thumbnail required separate database calls to retrieve its image variants. The optimization preloads all required image variants in advance via eager loading, thus reducing the number of SQL queries and improving overall page or API response times.","Originally, the code triggered an additional database query for each thumbnail or product to fetch its image variants (N+1 style). The optimized version preloads all necessary variants in a single query (using ORM features like includes/preload), turning many queries into just a couple regardless of the number of products. No complex algorithmic changes were made, but data access is far more efficient. This reduces load on the database, lowers response times, and decreases resource usage. The redundancy comes from the repeated querying in the original; the new version eliminates this.",Memory and Data Locality Optimizations,Increase Cache Efficiency via Locality,high,True,,1177
2604162624,361,Improved Sidekiq scheduling efficiency for `LargeSellersUpdateUserBalâ€¦,"â€¦anceStatsCacheWorker`

- instead of scheduling each sub-job individually, we can use `push_bulk` to do it much more efficiently

<!-- This is an auto-generated comment: release notes by coderabbit.ai -->

## Summary by CodeRabbit

- **Refactor**
  - Improved job processing efficiency by switching to bulk enqueuing for user balance stats updates. End users may experience faster and more reliable updates.

<!-- end of auto-generated comment: release notes by coderabbit.ai -->",Human,964245,daniellockyer,closed,2025-06-19 09:17:02+00:00,2025-06-20 14:49:46+00:00,2025-06-20 14:49:46+00:00,,https://api.github.com/repos/antiwork/gumroad,https://github.com/antiwork/gumroad/pull/361,perf,"The change improves the efficiency of job scheduling by using bulk enqueuing, which is a performance enhancement rather than a bug fix or new feature.","The change improves the efficiency of job scheduling by using bulk enqueuing, which is a performance enhancement rather than a bug fix or new feature.",Human,,,,,,Ruby,True,True,False,29.545555555555556,29.545555555555556,1.231064814814815,1.231064814814815,70,6,490.0,69.0,True,"The code schedules jobs for updating user balance statistics for large sellers in a background processing framework (Sidekiq). Previously, it enqueued each user-specific job individually; the optimized code uses Sidekiq's bulk API (push_bulk), submitting all jobs in a single, batched command.","Originally, each sub-job (user update) was enqueued one-by-one, resulting in potentially high overhead due to many individual network/database writes or context switches. The optimized version collects all jobs and submits them together with push_bulk, reducing overhead, grouping Sidekiq schedule operations, minimizing latency, and making job scheduling much more efficient. There is no change in job semantics or data; only the scheduling mechanism is improved. No algorithmic complexity change is present, but performance is improved through optimized I/O and batch operations. The code also becomes simpler and more maintainable.",I/O and Synchronization,Selection of I/O Size,high,True,,1185
2608906245,397,Cached repetitive data lookups for creator analytics,"- because of how the code is structured, we create a separate CreatorAnalytics::Web instance for every missing date range
- this then calls the `products_for_creator_analytics` method on a user, which returns a different relation each time, so query caching doesn't work
- instead, we can just calculate this once in the caching proxy and then pass it to the web instance
- I'll refactor this properly in the future once the fix is confirmed good

<!-- This is an auto-generated comment: release notes by coderabbit.ai -->

## Summary by CodeRabbit

- **Performance Improvements**
  - Enhanced analytics performance by caching userâ€™s first sale date and product data, resulting in faster loading times for analytics features.

<!-- end of auto-generated comment: release notes by coderabbit.ai -->",Human,964245,daniellockyer,closed,2025-06-21 09:21:17+00:00,2025-06-21 09:38:34+00:00,,,https://api.github.com/repos/antiwork/gumroad,https://github.com/antiwork/gumroad/pull/397,perf,"The changes improve performance by caching data to avoid repetitive lookups, resulting in faster analytics loading times without adding new features or fixing bugs.","The changes improve performance by caching data to avoid repetitive lookups, resulting in faster analytics loading times without adding new features or fixing bugs.",Human,,,,,,Ruby,False,True,False,0.2880555555555555,,0.0120023148148148,,52,7,809.0,124.0,True,"The code manages analytics calculations for creators by retrieving relevant user data (such as products and first sale date) and aggregating it for specified date ranges. Previously, multiple instances of the analytics class for different date ranges each redundantly queried the database for the same userâ€™s product data. The optimization introduces a caching mechanism that fetches this repetitive data once and shares it across analytics calculations, avoiding redundant queries.","In the original implementation, each analytics operation for a different date range created a new Web instance, which triggered a separate call to fetch products for the creator, causing multiple identical database queries that bypassed query caching. The optimized code modifies this to prefetch and cache the userâ€™s products and first sale date once, passing the cached data into subsequent analytics operations, thereby reducing repetitive data retrieval and database load. No changes are made to the algorithm itself; rather, the optimization improves data retrieval efficiency and reduces redundant work. No algorithmic changes or redundant logic removals are made besides this aspect.",Memory and Data Locality Optimizations,"Caching, Buffering",high,True,,1280
2577421996,307,Fixed duplicate context lookups across app,"refs https://github.com/antiwork/gumroad/issues/234

- because we were calling the custom_context function twice, we were doing all the DB queries twice
- this saves ~4 DB queries per page load when logged in, resulting in a ~2% win

<!-- This is an auto-generated comment: release notes by coderabbit.ai -->

## Summary by CodeRabbit

- **Refactor**
  - Improved performance and maintainability by optimizing how custom context data is accessed when embedding design settings and user agent information in the layout. No visible changes to users.

<!-- end of auto-generated comment: release notes by coderabbit.ai -->",Human,964245,daniellockyer,closed,2025-06-09 05:40:16+00:00,2025-06-09 06:14:39+00:00,2025-06-09 06:14:39+00:00,,https://api.github.com/repos/antiwork/gumroad,https://github.com/antiwork/gumroad/pull/307,perf,"The change optimizes database queries by reducing duplicate lookups, improving performance without adding new features or fixing bugs explicitly, which aligns with a performance improvement.","The change optimizes database queries by reducing duplicate lookups, improving performance without adding new features or fixing bugs explicitly, which aligns with a performance improvement.",Human,,,,,,Ruby,True,True,False,0.5730555555555555,0.5730555555555555,0.0238773148148148,0.0238773148148148,42,6,631.0,92.0,True,"The code manages application context retrieval, specifically gathering user and page-specific data (like design settings and user agent) to be injected into a page layout. It initially called the 'custom_context' function twice, causing redundant database queries and unnecessary data fetching. The commit changes the implementation so 'custom_context' is called only once per page render, avoiding duplicate lookups and redundant queries.","The original version called the 'custom_context' function twice for one page render, unnecessarily repeating all the logic (and thus all associated database queries) inside that function. The optimized code refactors the logic so 'custom_context' is called just once and its result is reused wherever needed. This reduces the number of database queries by roughly 4 per logged-in page load, yielding a ~2% end-to-end performance gain. No algorithmic redesign was performed, but there was a clear removal of redundant method calls and a structural simplification improving maintainability as well.",Code Smells and Structural Simplification,Remove Redundant Method Calls,high,True,,1228
2560305820,289,Added Typhoeus client for HTTP connection pooling + re-use,"refs https://github.com/elastic/elasticsearch-ruby#usage refs https://github.com/antiwork/gumroad/issues/234

- as suggested by the Elasticsearch Ruby docs, we should be using a HTTP library that supports persistent HTTP connections
- by adding the `typhoeus` gem, it'll automatically be used by Faraday
- this should improve performance by reusing HTTP connections instead of invoking the overhead of setting up and closing

<!-- This is an auto-generated comment: release notes by coderabbit.ai -->

## Summary by CodeRabbit

- **Chores**
  - Added a new dependency to improve application capabilities. No user-facing changes.

<!-- end of auto-generated comment: release notes by coderabbit.ai -->",Human,964245,daniellockyer,closed,2025-06-02 11:32:52+00:00,2025-06-02 12:32:02+00:00,2025-06-02 12:32:02+00:00,,https://api.github.com/repos/antiwork/gumroad,https://github.com/antiwork/gumroad/pull/289,perf,"The change introduces a new HTTP client library to improve connection pooling and reuse, which enhances performance without adding new features or fixing bugs directly.","The change introduces a new HTTP client library to improve connection pooling and reuse, which enhances performance without adding new features or fixing bugs directly.",Human,,,,,,Ruby,True,True,False,0.9861111111111112,0.9861111111111112,0.0410879629629629,0.0410879629629629,58,9,713.0,94.0,True,"The code modifies the way HTTP requests are performed within the application, specifically to interfaces such as Elasticsearch. It adds the 'typhoeus' gem as a dependency, thereby enabling persistent HTTP connection pooling and reuse through Faraday, which is the HTTP client adapter. This addresses the performance issue of repeatedly setting up and tearing down connections for multiple HTTP requests. The problem being solved is inefficient HTTP connection handling, and the solution is to leverage Typhoeus's connection pooling to improve throughput and resource usage.","Originally, the code relied on the default Faraday HTTP adapter, which may not support persistent connections, leading to repeated connection handshakes and increased latency for each HTTP request. The optimized version introduces the Typhoeus HTTP client, which implements connection pooling so that HTTP connections can be reused across multiple requests. This reduces connection overhead, minimizes latency, and can lead to better performance and throughput, especially under heavy load. There is no algorithmic change in terms of business logic, but there is a significant change in infrastructure, specifically how external HTTP communication is managed. This also reduces any redundant setup/teardown code for establishing HTTP connections.",Memory and Data Locality Optimizations,"Caching, Buffering",high,True,,1290
2441809617,56,Index `followers` to better support our query patterns,"We frequently query followers filtered by followed_id and ordered by
confirmed_at. (e.g. on the /followers page)

This could take 80+ seconds for sellers with a large amount of
following.

I am hoping that this composite index on (followed_id, confirmed_at) can
help more efficiently perform range scans without a filesort, improving
query performances.

I left these out of the composite index:

- `deleted_at`: vast majority of the records should have `deleted_at is
  NULL`
- `id`: I think this is only going to help if we switch to cursor-based
  pagination (unlikely to be prioritized)

I had tested this with a local table of 1M records. Will further
bench this once it's rolled out and make adjustments if needed.",Human,6376558,xrav3nz,closed,2025-04-06 19:16:30+00:00,2025-04-06 19:36:36+00:00,2025-04-06 19:36:36+00:00,,https://api.github.com/repos/antiwork/gumroad,https://github.com/antiwork/gumroad/pull/56,perf,"The change introduces a new composite index to improve query performance, which is a performance optimization rather than a bug fix or new feature.","The change introduces a new composite index to improve query performance, which is a performance optimization rather than a bug fix or new feature.",Human,,,,,,Ruby,True,True,False,0.335,0.335,0.0139583333333333,0.0139583333333333,54,8,738.0,116.0,True,"The code introduces a new composite database index on the 'followers' table, specifically indexing the ('followed_id', 'confirmed_at') columns. This is in response to slow query performance filtering by 'followed_id' and sorting by 'confirmed_at', particularly on the page showing followers. The goal is to speed up these frequent range queries, eliminating filesorts and reducing response times for large data sets.","Originally, queries filtering 'followers' by 'followed_id' and ordering by 'confirmed_at' would require the database to scan a large portion of the table and perform an explicit filesort, causing significant latency (up to 80+ seconds for some users). By adding a composite index on ('followed_id', 'confirmed_at'), the database can now use the index to quickly locate relevant records and return them in the desired order without a separate sort step. This change specifically avoids including columns like 'deleted_at' and 'id' in the index, since including them would add unnecessary overhead, given current query patterns. The new index dramatically improves time complexity for these queries, upgrading from a full table scan/sort to an efficient range scan over a sorted index.",Memory and Data Locality Optimizations,Increase Cache Efficiency via Locality,high,True,,1309
2297969098,309,feat(pack): Performance Optimization for Large Repositories,"## Performance Improvement

### yamadashy/repomix
- Before: `868.73 millis (usr: 1.11 secs, sys: 0.14 secs)`
- After: `671.26 millis (usr: 1.42 secs, sys: 0.22 secs)`

No significant change

### facebook/react

- Before: `123.31 secs (usr: 118.64 secs, sys: 1.60 secs)`
- After: `4.19 secs (usr: 22.66 secs, sys: 2.49 secs)`

29x faster

### vercel/next.js
- Before: `17.85 mins (usr: 16.66 mins, sys: 0.18 mins)`
- After: `17.27 secs (usr: 52.93 secs, sys: 7.11 secs)`

58x faster

## Changes
- Replace `p-map` with Piscina worker threads for parallel processing
- Implement dedicated workers for file collection, processing, and metrics
- Optimize file search with single worker implementation
- Add proper resource cleanup for worker pools
- Move file manipulation logic to worker threads

## Checklist

- [x] Run `npm run test`
- [x] Run `npm run lint`
",Human,5019072,yamadashy,closed,2025-01-25 04:56:39+00:00,2025-01-25 05:32:13+00:00,2025-01-25 05:32:13+00:00,,https://api.github.com/repos/yamadashy/repomix,https://github.com/yamadashy/repomix/pull/309,perf,"The PR introduces performance improvements by optimizing processing with worker threads and parallelism, resulting in significant speedups. This is a code change that improves performance without adding new features or fixing bugs.","The PR introduces performance improvements by optimizing processing with worker threads and parallelism, resulting in significant speedups. This is a code change that improves performance without adding new features or fixing bugs.",Human,,,,,,TypeScript,True,True,False,0.5927777777777777,0.5927777777777777,0.024699074074074,0.024699074074074,59,6,889.0,136.0,True,"The code manages repository analysis tasks, including file collection, processing, and generating metrics for large codebases. The key functionality is efficiently traversing and processing repository files, likely for tasks such as dependency analysis or package generation, aiming to handle very large repositories (such as facebook/react and vercel/next.js) quickly and efficiently.","Originally, the code used 'p-map' for parallel processing, which is limited to the main thread and Node.js event loop. The optimized version replaces this with Piscina, a library for the Node.js worker threads API, enabling true multi-threaded execution by offloading CPU-intensive tasks (file collection, processing, metrics) to worker threads. This allows better utilization of multi-core CPUs and avoids bottlenecks of the main thread. The optimization also includes specifically assigning certain jobs (file collection, etc) to dedicated workers, improving task specialization and further reducing contention. Resource cleanup (proper worker pool management) is added, leading to improved stability and lower memory/handle leaks. File logic moved into threads means the main thread remains responsive, and heavy operations do not block the event loop. These changes together create massive runtime improvements (e.g., 29xâ€“58x for big repos). No redundant code elimination is notedâ€”changes are mostly architectural and parallelization focused.",Algorithm-Level Optimizations,Select Computationally Efficient Algorithms,high,True,,1443
2531991252,4109,fix(boxai-sidebar): refactor sidebar content to reduce re-render ,"Use` React.useCallback `to reduce component  re-rendering
<!--
Please add the `ready-to-merge` label when the pull request has received the appropriate approvals.
Using the `ready-to-merge` label adds your approved pull request to the merge queue where it waits to be merged.
Mergify will merge your pull request based on the queue assuming your pull request is still in a green state after the previous merge.

What to do when the `ready-to-merge` label is not working:

- Do you have two approvals?
  - At least two approvals are required in order to merge to the master branch.
- Are there any reviewers that are still requested for review?
  - If the pull request has received the necessary approvals, remove any additional reviewer requests that are pending.
    - e.g.
      - Three reviewers added comments but you already have two necessary approvals and the third reviewer's comments are no longer applicable. You can remove the third person as a reviewer or have them approve the pull request.
      - A team was added as a reviewer because of a change to a file but the file change has been undone. At this point, it should be safe to remove the team as a reviewer.
- Are there other pull requests at the front of the merge queue?
  - Mergify handles the queueing, your pull request will eventually get merged.

When to contact someone for assistance when trying to merge via `ready-to-merge` label:

- There are no other pull requests in the merge queue and your pull request has been sitting there with the `ready-to-merge` label for longer than a couple of hours.
- If you are unable to remove unnecessary reviewers from the pull request.
- If you are unable to add the `ready-to-merge` label.
  -->


<!-- This is an auto-generated comment: release notes by coderabbit.ai -->
## Summary by CodeRabbit

## Summary by CodeRabbit

- **Performance Improvements**
  - Enhanced sidebar responsiveness by optimizing content update checks to reduce unnecessary refreshes.
<!-- end of auto-generated comment: release notes by coderabbit.ai -->",Human,97700841,fpan225,closed,2025-05-20 15:56:03+00:00,2025-05-21 22:27:04+00:00,,,https://api.github.com/repos/box/box-ui-elements,https://github.com/box/box-ui-elements/pull/4109,perf,"The change uses React.useCallback to reduce unnecessary re-rendering, which is a performance optimization rather than a bug fix or new feature.","The change uses React.useCallback to reduce unnecessary re-rendering, which is a performance optimization rather than a bug fix or new feature.",Human,,,,,,JavaScript,False,True,False,30.516944444444444,,1.271539351851852,,65,7,2082.0,336.0,True,"The code in question is a React sidebar component. The purpose of the sidebar is to display content related to the BoxAI feature. The change focuses on reducing unnecessary rerenders of the sidebar content, which can improve application performance and responsiveness.","Originally, the sidebar component likely passed inline or non-memoized functions/handlers (e.g., event handlers, callbacks) as props or used them within the component, causing React to see those functions as new objects on each render. This causes child components that depend on those handlers to rerender whenever the parent rerenders. The optimized version uses 'React.useCallback' to memoize key handler functions, so they retain their identity unless specified dependencies change. This optimization reduces the number of unnecessary rerenders in child components, thus improving UI performance, especially for complex or frequently updating components. There are no major algorithmic changes, but there is a structural refactor to remove redundant rerender triggers.",Code Smells and Structural Simplification,Remove Redundant Method Calls,high,True,,1530
2311607019,8412,ML-KEM/Kyber: small memory usage,"# Description

Options to compile ML-KEM/Kyber to use less dynamic memory.
Only available with C code and has small performance trade-off.

# Testing

Regression tested ML-KEM/Kyber.

# Checklist

 - [ ] added tests
 - [ ] updated/added doxygen
 - [ ] updated appropriate READMEs
 - [ ] Updated manual and documentation
",Human,17982558,SparkiDev,closed,2025-02-03 02:36:12+00:00,2025-02-04 19:45:02+00:00,2025-02-04 19:45:02+00:00,,https://api.github.com/repos/wolfSSL/wolfssl,https://github.com/wolfSSL/wolfssl/pull/8412,perf,"The change introduces an option to reduce memory usage in ML-KEM/Kyber, which is an improvement in resource efficiency but not a new feature or bug fix. It optimizes memory usage with a small performance trade-off, fitting the definition of a performance improvement.","The change introduces an option to reduce memory usage in ML-KEM/Kyber, which is an improvement in resource efficiency but not a new feature or bug fix. It optimizes memory usage with a small performance trade-off, fitting the definition of a performance improvement.",Human,,,,,,C,True,True,False,41.147222222222226,41.147222222222226,1.7144675925925927,1.7144675925925927,32,4,335.0,51.0,True,"This code deals with ML-KEM/Kyber, a lattice-based cryptographic scheme. The change introduces a way to compile the implementation to use less dynamic memory, likely by modifying memory allocation or usage patterns in the C code. The option seems to slightly impact performance but reduces memory footprint, making the implementation more space-efficient, potentially important for embedded or constrained environments.","The original version presumably used a larger amount of dynamic memory, perhaps allocating more memory than necessary per invocation (such as for intermediate buffers, temporary storage, or data structures). The optimized version introduces a way (likely preprocessor flags or alternative code paths) to reduce dynamic memory usage, possibly by reusing buffers, reducing buffer sizes, using stack instead of heap, or allocating on-demand. This change improves space efficiency, which could potentially lead to reduced memory bandwidth usage and fewer cache misses. There may be a small performance trade-off indicated, implying that some memory-saving techniques (like smaller buffers or more frequent allocations) could lead to less efficient processing. No mention of redundant logic, algorithmic overhaul, or stylistic refactor is made.",Memory and Data Locality Optimizations,Select Space Efficient Algorithm,high,True,,1225
2319710038,32,minimize the docker image size after building,Shrinking the image size from 1G+ to 200M+,Human,24894259,ArnoChenFx,closed,2025-02-06 11:43:33+00:00,2025-02-06 13:39:37+00:00,2025-02-06 13:39:37+00:00,,https://api.github.com/repos/jina-ai/node-DeepResearch,https://github.com/jina-ai/node-DeepResearch/pull/32,perf,"The change focuses on reducing the Docker image size, which is an improvement in performance and efficiency of the build output, thus it is classified as a performance improvement.","The change focuses on reducing the Docker image size, which is an improvement in performance and efficiency of the build output, thus it is classified as a performance improvement.",Human,,,,,,TypeScript,True,True,False,1.9344444444444444,1.9344444444444444,0.0806018518518518,0.0806018518518518,45,7,42.0,8.0,True,"The code alters the Dockerfile or build process to reduce the final Docker image size from over 1GB to around 200MB. This typically involves removing unnecessary layers, files, dependencies, build artifacts, or employing more efficient base images and multi-stage builds.","The original version resulted in a Docker image larger than 1GB, likely due to inclusion of redundant build dependencies, intermediate files, or an inefficient base image. The optimized version streamlines the Docker build: removing extraneous packages after build, leveraging multi-stage builds to separate build-time and runtime dependencies, minimizing image layers, and excluding unnecessary files or caches. These changes lead to a significantly smaller image size, improving deployment speed and resource usage. No major algorithmic changes are made; the focus is on artifact and environment optimization.",Memory and Data Locality Optimizations,Optimize Object Use,high,True,,1095
2398994327,6182,Reduce transaction aborted warning logs,,Human,749094,codyebberson,closed,2025-03-17 20:33:25+00:00,2025-03-17 21:21:41+00:00,2025-03-17 21:21:41+00:00,,https://api.github.com/repos/medplum/medplum,https://github.com/medplum/medplum/pull/6182,perf,"The change aims to reduce warning logs related to transaction aborts, which is an improvement in the logging behavior and potentially the performance of the system by reducing unnecessary log noise.","The change aims to reduce warning logs related to transaction aborts, which is an improvement in the logging behavior and potentially the performance of the system by reducing unnecessary log noise.",Human,,,,,,TypeScript,True,True,False,0.8044444444444444,0.8044444444444444,0.0335185185185185,0.0335185185185185,39,5,,,False,"The code manages transaction lifecycles and, in certain cases when a transaction is aborted, emits warning log messages. This mechanism is intended to alert system maintainers about aborted transactions for monitoring or debugging purposes.","In the previous version, every aborted transaction triggered a warning log message, which could result in excessive logging if aborts are frequent. The optimized version introduces a rate limiter or some filtering logic that reduces the frequency of these warning messages, for example by logging only every Nth occurrence or by suppressing repeated warnings within a time window. This modification does not change the core logic handling transactions, but substantially reduces redundant outputs and system noise. It improves overall performance by reducing unnecessary disk and I/O operations from excessive logging and makes logs more actionable. No significant algorithmic change is introduced concerning transaction handling itself; the main difference is in log emission logic.",Code Smells and Structural Simplification,Remove Redundant Method Calls,high,True,,1088
2503287360,89,perf(copool): remove lock in queue,,Human,46670806,noneback,closed,2025-05-07 05:26:42+00:00,2025-05-07 05:31:00+00:00,2025-05-07 05:31:00+00:00,,https://api.github.com/repos/noneback/go-taskflow,https://github.com/noneback/go-taskflow/pull/89,perf,"The commit title indicates a performance improvement by removing a lock in the queue, which is a change aimed at improving performance.","The commit title indicates a performance improvement by removing a lock in the queue, which is a change aimed at improving performance.",Human,,,,,,Go,True,True,False,0.0716666666666666,0.0716666666666666,0.0029861111111111,0.0029861111111111,34,5,,,False,"The code manages a queue within a coroutine pool (copool), likely responsible for scheduling or dispatching coroutines or tasks. Previously, access or modifications to the queue were protected by a lock to ensure thread safety. The commit removes this locking mechanism to improve performance.","Originally, the code used a lock to serialize access to the queue, which incurs overhead due to mutex contention and possible thread descheduling. The optimized version eliminates this locking, either by leveraging a lock-free data structure or by constraining queue access to a single thread or context where locking is unnecessary. This reduces synchronization overhead and improves throughput. There could also be subtle algorithmic changes if the queue's thread safety is ensured differently. No indication of additional method calls, control structures, or redundant code outside of lock management appears present.",I/O and Synchronization,Polling,high,True,,1075
